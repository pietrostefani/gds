[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geographic Data Science in R and Python",
    "section": "",
    "text": "Welcome\nThis is the website for the “Geographic Data Science” module ENVS363/563 at the University of Liverpool. This is course designed and delivered by Dr. Elisabetta Pietrostefani and Dr. Carmen Cabrera-Arnau from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. Much of the course material is inspired by Dani Arribas-Bel’s course on Geographic Data Science.\nThis module will introduce students to the field of Geographic Data Science (GDS), a discipline established at the intersection between Geographic Information Science (GIS) and Data Science. The course covers how the modern GIS toolkit can be integrated with Data Science tools to solve practical real-world problems.\nCore to the set of employable skills to be taught in this course is an introduction to programming tools. Students will be able to whether to develop their skills in either R or Python in Lab sessions.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Geographic Data Science in R and Python",
    "section": "Contact",
    "text": "Contact\n\nElisabetta Pietrostefani - e.pietrostefani [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 6xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.\n\n\nCarmen Cabrera-Arnau - c.cabrera-arnau [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 6xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Further readings\nWatch: Solving Life’s Everyday Problems with Data"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "openscience.html",
    "href": "openscience.html",
    "title": "1 - Open Science",
    "section": "",
    "text": "Concepts\nThe ideas behind this block are better communicated through narrative than video or lectures. Hence, the concepts section are delivered through a few references you are expected to read. These will total up about one and a half hours of your focused time.\n\n\nOpen Science\nThe first part of this block is about setting the philosophical background. Why do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\nRead the chapter here. Estimated time: 15min.\n\nFirst half of Chapter 1 in \"Geographic Data Science with PySAL and the PyData stack\" reyABwolf.\n\nRead the piece here. Estimated time: 35min.\n\nThe 2018 Atlantic piece \"The scientific paper is obsolete\" on computational notebooks, by James Somers somers2018scientific.\n\n\n\nModern Scientific Tools\nOnce we know a bit more about why we should care about the tools we use, let's dig into those that will underpin much of this course. This part is interesting in itself, but will also valuable to better understand the practical aspects of the course. Again, we have two reads here to set the tone and complement the practical introduction we saw in the Hands-on and DIY parts of the previous block. We are closing the circle here:\nRead the chapter here.\n\nSecond half of Chapter 1 in \"Geographic Data Science with PySAL and the PyData stack\" reyABwolf.\nThe chapter in the GIS&T Book of Knowledge on computational notebooks, by Geoff Boeing and Dani Arribas-Bel."
  },
  {
    "objectID": "overview.html#aims",
    "href": "overview.html#aims",
    "title": "Overview",
    "section": "Aims",
    "text": "Aims\nThe module has three main aims.\n\nProvide students with core competences in Geographic Data Science (GDS). This includes advancing their statistical and numerical literacy and introducing basic principles of programming and state-of-the-art computational tools for GDS;\nPresent a comprehensive overview of the main methodologies available to the Geographic Data Scientist, as well as their intuition as to how and when they can be applied;\nFocus on real world applications of these techniques in a geographical and applied context."
  },
  {
    "objectID": "overview.html#learning-outcomes",
    "href": "overview.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the module, students should be able to:\nFor all\n\nDemonstrate advanced GIS/GDS concepts and be able to use the tools programmatically to import, manipulate and analyse data in different formats.\nUnderstand the motivation and inner workings of the main methodological approaches of GDS, both analytical and visual.\nEvaluate the suitability of a specific technique, what it can offer and how it can help answer questions of interest.\nApply a number of spatial analysis techniques and how to interpret the results, in the process of turning data into information.\nWhen faced with a new data-set, work independently using GIS/GDS tools programmatically.\n\nOnly for MSc students\n\nDemonstrate a sound understanding of how real-world (geo)data are produced, their potential insights and biases, as well as opportunities and limitations."
  },
  {
    "objectID": "overview.html#feedback",
    "href": "overview.html#feedback",
    "title": "Overview",
    "section": "Feedback",
    "text": "Feedback\nFormal assessment of one map, one MCQ test and one computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during computer, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times. Response time will vary depending on the complexity of the question and staff availability."
  },
  {
    "objectID": "overview.html#computational-environment",
    "href": "overview.html#computational-environment",
    "title": "Overview",
    "section": "Computational Environment",
    "text": "Computational Environment\nADD SOMETHING ABOUR R or Python\nEDIT the below\nThis course can be followed by anyone with access to a bit of technical infrastructure. This section details the set of local and online requirements you will need to be able to follow along, as well as instructions or pointers to get set up on your own. This is a centralized section that lists everything you will require, but keep in mind that different blocks do not always require everything all the time.\nTo reproduce the code in the book, you need the most recent version of R and packages. These can be installed following the instructions provided in our R installation guide.\n\nSoftware\nEDIT\nTo run the analysis and reproduce the code, you need the following software:\n\nQGIS- the stable version (3.22 LTR at the time of writing) is OK, any more recent version will also work.\nR-4.2.2\nRStudio 2022.12.0-353\nQuarto 1.2.280\nthe list of libraries in the next section\n\nTo install and update:\n\nQGIS, download the appropriate version from QGIS.org\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\n\n\nR List of libraries\nThe list of libraries used in this book is provided below:\n\n\nPython set-up\n\n\nOnline accounts"
  },
  {
    "objectID": "assess.html#assignment-i",
    "href": "assess.html#assignment-i",
    "title": "Assessments",
    "section": "Assignment I",
    "text": "Assignment I\n\nTitle: Programmed Map\nType: Coursework\nDue date: TBC\n25% of the final mark\nChance to be reassessed\nElectronic submission only\n\nThis assignment will be evaluated on technical data processing, map design abilities, and overall narrative.\nFirst, the data.\nSecond, the assemblage.\nThird, the design and overall narrative\nOnce you have created your map, you will need to present it. Write 250 about the choices you made to create the map.\n\nSubmit\nOnce completed, you will need to submit the following:\nAn html version of an .qmd document with R or Python integrated code.\nThe assignment will be evaluated based on four main pillars, on which you will have to be successful to achieve a good mark:\n\nData processing\nMap assemblage This includes your ability to master technologies that allow you to create a compelling map.\nDesign and narrative"
  },
  {
    "objectID": "assess.html#assignment-ii",
    "href": "assess.html#assignment-ii",
    "title": "Assessments",
    "section": "Assignment II",
    "text": "Assignment II\n\nTitle: MCQ test\nType: Test\nDue date: TBC\n25% of the final mark\nChance to be reassessed\nElectronic submission only\n\n\nTo ensure students are engaging with the course content as it progresses and\nTo provide core learning in advance of the third assessment."
  },
  {
    "objectID": "assess.html#marking-criteria",
    "href": "assess.html#marking-criteria",
    "title": "Assessments",
    "section": "Marking Criteria",
    "text": "Marking Criteria\nThis course follows the standard marking criteria (the general ones and those relating to GIS assignments in particular) set by the School of Environmental Sciences. Please make sure to check the student handbook and familiarise with them. In addition to these generic criteria, the following specific criteria will be used in cases where computer code is part of the work being assessed:\n\n0-15: the code does not run and there is no documentation to follow it.\n16-39: the code does not run, or runs but it does not produce the expected outcome. There is some documentation explaining its logic.\n40-49: the code runs and produces the expected output. There is some documentation explaining its logic.\n50-59: the code runs and produces the expected output. There is extensive documentation explaining its logic.\n60-69: the code runs and produces the expected output. There is extensive documentation, properly formatted, explaining its logic.\n70-79: all as above, plus the code design includes clear evidence of skills presented in advanced sections of the course (e.g. custom methods, list comprehensions, etc.).\n80-100: all as above, plus the code contains novel contributions that extend/improve the functionality the student was provided with (e.g. algorithm optimizations, novel methods to perform the task, etc.)."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Introduction and Open Science\n\nLecture: Introduction to the module & Open Science\nLab: Setting up your Computational Environment & Data Wrangling\n\nSpatial Data\n\nLecture:\nLab:\n\nMapping Vector Data\n\nLecture:\nLab:\n\nMapping Raster Data\n\nLecture:\nLab:\n\nAssignment I: Programmed Map\nSpatial Weights\n\nLecture:\nLab:\n\nESDA\n\nLecture:\nLab:\n\nAssignment II: MCQ test\nClustering\n\nLecture:\nLab:\n\nInterpolation, heatmaps and point patterns\n\nLecture:\nLab:\n\nSpatial Network Analysis\n\nLecture:\nLab:\n\nAssignment III: A computational essay"
  },
  {
    "objectID": "concepts_openscience.html",
    "href": "concepts_openscience.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_spatialdata.html",
    "href": "concepts_spatialdata.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_mapvector.html",
    "href": "concepts_mapvector.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_mapraster.html",
    "href": "concepts_mapraster.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "environR.html#r-list-of-libraries",
    "href": "environR.html#r-list-of-libraries",
    "title": "R",
    "section": "R List of libraries",
    "text": "R List of libraries\nThe list of libraries used in this book is provided below:\n\nsf\ngeojsonsf\nmapview"
  },
  {
    "objectID": "intro.html#what-is-geographic-data-science",
    "href": "intro.html#what-is-geographic-data-science",
    "title": "Introduction",
    "section": "What is Geographic Data Science?",
    "text": "What is Geographic Data Science?\nThe following clip is taken from a keynote response by Dani Arribas-Bel at the first Spatial Data Science Conference, organised by CARTO and held in Brooklyn in 2017. The talk provides a bit of background and context, which will hopefully help you understand a bit better what Geographic Data Science is.\nTOP UP with slide content"
  },
  {
    "objectID": "intro.html#get-ready",
    "href": "intro.html#get-ready",
    "title": "Introduction",
    "section": "Get ready!",
    "text": "Get ready!\nGo the the Computation Environment section"
  },
  {
    "objectID": "intro.html#further-readings",
    "href": "intro.html#further-readings",
    "title": "1  Introduction",
    "section": "2.1 Further readings",
    "text": "2.1 Further readings\nTo get a better picture, the following readings complement the overview provided above very well:\nBonus\nWatch ! All Maps are wrong https://www.youtube.com/watch?v=kIID5FDi2JQ Watch: Solving Life’s Everyday Problems with Data https://www.sciencefriday.com/segments/solving-lifes-everyday-problems-with-data/\nThe chapter is available free online HTML | PDF\n\nThe introductory chapter to “Doing Data Science” schutt2013doing, by Cathy O’Neil and Rachel Schutt is general overview of why we needed Data Science and where if came from.\nA slightly more technical historical perspective on where Data Science came from and where it might go can be found in David Donoho’s recent overview donoho201750.\nA geographic take on Data Science, proposing more interaction between Geography and Data Science singleton2019geographic."
  },
  {
    "objectID": "environ.html#software",
    "href": "environ.html#software",
    "title": "Environment",
    "section": "Software",
    "text": "Software\nTo run the analysis and reproduce the code, you need the following software:\n\nQGIS- the stable version (3.22 LTR at the time of writing) is OK, any more recent version will also work.\nQGIS, download the appropriate version from QGIS.org\nQuarto 1.2.280\nQuarto, download the appropriate version from the Quarto website"
  },
  {
    "objectID": "assess.html#assignment-iii",
    "href": "assess.html#assignment-iii",
    "title": "Assessments",
    "section": "Assignment III",
    "text": "Assignment III\n\nTitle: Computational Essay\nType: Coursework\nDue date: TBC\n50% of the final mark\nChance to be reassessed\nElectronic submission only\n\nA 2500 word computational essay on a geographic data set which they have explored and analysed using the skills and techniques developed during the course. Students will complete an essay which combines both code, data visualisation and prose supported by references in order to demonstrate sound understanding of all learning outcomes.\nOverview\nHere’s the premise. You will take the role of a real-world geographic sata scientist tasked to explore datasets on New York City and find useful insights for a variety of city decision-makers. It does not matter if you have never been to New York City. In fact, this will help you focus on what you can learn about the city through the data, without the influence of prior knowledge. Furthermore, the assessment will not be marked based on how much you know about New York City but instead about how much you can show you have learned through analysing data. You will need contextualise your project by highlighting the opportunities and limitations of ‘old’ and ‘new’ forms of spatial data and reference relevant literature.\nWhat is a Computational Essay?\nA computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. This piece of assessment is equivalent to 2,500 word. However, this is the overall weight. Since you will need to create not only narrative but also code and figures, here are the requirements:\n\nMaximum of 1,000 words (ordinary text) (references do not contribute to the word count). You should answer the specified questions within the narrative. The questions should be included within a wider analysis.\nUp to four maps or figures (a figure may include more than one map and will only count as one but needs to be integrated in the same overall output)\nUp to one table\n\nThere are three kinds of elements in a computational essay:\n1. Ordinary text (in English)\n2. Computer input (R-markdown or Python code)\n3. Computer output These three elements all work together to express what’s being communicated.\nSubmission (Coming Soon)\nData (Coming Soon)\nSpecifics (Coming Soon)"
  },
  {
    "objectID": "environ.html#book-software",
    "href": "environ.html#book-software",
    "title": "Environment",
    "section": "Book Software",
    "text": "Book Software\nTo reproduce the code in the book, you need the most recent version of Quarto, R and relevant packages. These can be installed following the instructions provided in our R installation guide. Quarto (1.2.280) can be downloaded from the Quarto website, it may already be installed when you download R and R Studio."
  },
  {
    "objectID": "intro.html#from-geographic-data-science-to-geographic-data-science",
    "href": "intro.html#from-geographic-data-science-to-geographic-data-science",
    "title": "1 Introduction",
    "section": "From Geographic Data Science to Geographic Data Science",
    "text": "From Geographic Data Science to Geographic Data Science\nGeographic Information holds a pivotal position within our modern societies, permeating various aspects of our daily lives. It underpins essential sectors such as housing, transportation, insurance, banking, telecommunications, logistics, energy, retail, agriculture, healthcare, and urban planning. Its significance lies in the capacity to analyze and derive invaluable insights from geo-spatial data, enabling us to make informed decisions and address complex challenges. Proficiency in this field equips individuals with the ability to work with real-world data across multiple domains and tackle diverse problems. Furthermore, it provides the opportunity to acquire essential data science skills and utilize important tools for answering spatial questions. Given its wide-ranging applications and the increasing reliance on location-based information, there is a substantial demand for experts in the geographic information industry, making it a highly sought-after skill set in today’s workforce.\nWhat information does GIS use?\n\nData that defines geographical features like roads, rivers\nSoil types, land use, elevation\nDemographics, socioeconomic attributes\nEnvironmental, climate, air-quality\nAnnotations that label features and places\n\nGeographic Data Science\nA GIS person typically produces cartographic and analytical products using desktop software. A geospatial data scientist creates code and runs pipelines that produce analytical products and cartographic representations.\nThis entails working with real-world data from various domains and tackling a wide range of complex problems. Through this process geospatial data science includes both data science and GIS tools that lead to the analysos of intricate spatial questions effectively. The synergy between CyberGIS and Geographic Data Science is unmistakable, with coding playing a pivotal role in enabling the seamless development of interactive data analysis. By leveraging cutting-edge technologies and innovative methodologies, this symbiotic relationship enhances the accessibility, scalability, and interactivity of geospatial data analysis. Consequently, it opens up new vistas for collaborative research and decision-making processes.\nThis multifaceted approach equips them with the knowledge and expertise to navigate the intricate world of spatial data analysis and contribute meaningfully to diverse fields where location-based insights are invaluable."
  },
  {
    "objectID": "intro.html#a-useful-clip-cannot-find-it",
    "href": "intro.html#a-useful-clip-cannot-find-it",
    "title": "Introduction",
    "section": "A useful clip (cannot find it)",
    "text": "A useful clip (cannot find it)\nThe following clip is taken from a keynote response by Dani Arribas-Bel at the first Spatial Data Science Conference, organised by CARTO and held in Brooklyn in 2017. The talk provides a bit of background and context, which will hopefully help you understand a bit better what Geographic Data Science is."
  },
  {
    "objectID": "intro.html#open-science-1",
    "href": "intro.html#open-science-1",
    "title": "1 Introduction",
    "section": "Open Science",
    "text": "Open Science\nWhy do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\n\nFirst half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.\nThe 2018 Atlantic piece “The scientific paper is obsolete” on computational notebooks, by James Somers."
  },
  {
    "objectID": "intro.html#modern-scientific-tools",
    "href": "intro.html#modern-scientific-tools",
    "title": "1 Introduction",
    "section": "Modern Scientific Tools",
    "text": "Modern Scientific Tools\nOnce we know a bit more about why we should care about the tools we use, let’s dig into those that will underpin much of this course. This part is interesting in itself, but will also valuable to better understand the practical aspects of the course. Again, we have two reads here to set the tone and complement the practical introduction we saw in the Hands-on and DIY parts of the previous block. We are closing the circle here:\n\nSecond half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists."
  },
  {
    "objectID": "spatialdata.html",
    "href": "spatialdata.html",
    "title": "Spatial Data",
    "section": "",
    "text": "Watch ! All Maps are wrong https://www.youtube.com/watch?v=kIID5FDi2JQ"
  },
  {
    "objectID": "environPy.html",
    "href": "environPy.html",
    "title": "Python",
    "section": "",
    "text": "Resources\nSome help along the way with:\n\nGeographic Data Science with Python by Sergio J. Rey, Dani Arribas-Bel, Levi J. Wolf"
  },
  {
    "objectID": "environR.html",
    "href": "environR.html",
    "title": "R",
    "section": "",
    "text": "Resources\nSome help along the way with:"
  },
  {
    "objectID": "openscienceR.html",
    "href": "openscienceR.html",
    "title": "1  | include: false",
    "section": "",
    "text": "OpenScience in R\nOnce we know a bit about what computational notebooks are and why we should care about them, let’s jump to using them! This section introduces you to using R or Python for manipulating tabular data. Please read through it carefully and pay attention to how ideas about manipulating data are translated into code that “does stuff”. For this part, you can read directly from the course website, although it is recommended you follow the section interactively by running code on your own.\nOnce you have read through and have a bit of a sense of how things work, jump on the Do-It-Yourself section, which will provide you with a challenge to complete it on your own, and will allow you to put what you have already learnt to good use."
  },
  {
    "objectID": "openscienceR.html#data-wrangling",
    "href": "openscienceR.html#data-wrangling",
    "title": "OpenScience in R",
    "section": "Data wrangling",
    "text": "Data wrangling\nReal world datasets are messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to munge them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but has to do with much more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nFor how labor intensive and relevant this aspect is, there is surprisingly very little published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation. In this session, you will use a few real world datasets and learn how to process them into Python so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the bread and butter of data analysis and scientific computing in Python. These are fundamental tools that are constantly used in almost any task relating to data analysis.\nThis notebook covers the basic and the content that is expected to be learnt by every student. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at. As a companion to this introduction, there is an additional notebook (see link on the website page for Lab 01) that covers how the dataset used here was prepared from raw data downloaded from the internet, and includes some additional exercises you can do if you want dig deeper into the content of this lab.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need, so we can get that out of the way and focus on the task at hand:"
  },
  {
    "objectID": "openscienceR.html#loading-packages",
    "href": "openscienceR.html#loading-packages",
    "title": "OpenScience in R",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\nPythonR\n\n\n\n\n\n\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tmap)"
  },
  {
    "objectID": "syllabus.html#part-1",
    "href": "syllabus.html#part-1",
    "title": "Syllabus",
    "section": "Part 1",
    "text": "Part 1\nIntroduction and Open Science\n\nLecture: Introduction to the module & Open Science\nLab: Setting up your Computational Environment & Data Wrangling\n\nSpatial Data\n\nLecture:\nLab:\n\nMapping Vector Data\n\nLecture:\nLab:\n\nMapping Raster Data\n\nLecture:\nLab:\n\nAssignment I: Programmed Map"
  },
  {
    "objectID": "syllabus.html#part-2",
    "href": "syllabus.html#part-2",
    "title": "Syllabus",
    "section": "Part 2",
    "text": "Part 2\nSpatial Weights\n\nLecture:\nLab:\n\nESDA\n\nLecture:\nLab:\n\nAssignment II: MCQ test\nClustering\n\nLecture:\nLab:\n\nInterpolation, heatmaps and point patterns\n\nLecture:\nLab:\n\nSpatial Network Analysis\n\nLecture:\nLab:\n\nAssignment III: A computational essay"
  },
  {
    "objectID": "openscienceDIY.html#import-libraries",
    "href": "openscienceDIY.html#import-libraries",
    "title": "Do-It-Yourself",
    "section": "Import libraries",
    "text": "Import libraries\n\nPythonR\n\n\n\n\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "openscienceDIY.html#tasks",
    "href": "openscienceDIY.html#tasks",
    "title": "Do-It-Yourself",
    "section": "Tasks",
    "text": "Tasks\nNow, the challenge is to put to work what we have learnt in this block. For that, the suggestion is that you carry out an analysis of the Afghan Logs in a similar way as how we looked at population composition in Liverpool. These are of course very different datasets reflecting immensely different realities. Their structure, however, is relatively parallel: both capture counts aggregated by a spatial (neighbourhood) or temporal unit (month), and each count is split by a few categories.\nTry to answer the following questions:\n\nObtain the minimum number of civilian casualties (in what month was that?)\nHow many NATO casualties were registered in August 2008?\nWhat is the month with the most total number of casualties?\n\nTip: You will need to first create a column with total counts"
  },
  {
    "objectID": "openscience.html#data-wrangling",
    "href": "openscience.html#data-wrangling",
    "title": "OpenScience",
    "section": "Data wrangling",
    "text": "Data wrangling\nReal world datasets are messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to wrangle (manipulating, transforming & structuring) them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but has to do with much more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nIn this session, you will use a few real world datasets and learn how to process them in R or Python so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the bread and butter of data analysis and scientific computing. These are fundamental tools that are constantly used in almost any task relating to data analysis.\nThis notebook covers the basic and the content that is expected to be learnt by every student. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need, so we can get that out of the way and focus on the task at hand:"
  },
  {
    "objectID": "openscience.html#loading-packages",
    "href": "openscience.html#loading-packages",
    "title": "OpenScience",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\nRPython\n\n\n\nlibrary(tidyverse) # a structure of data manipulation including several packages \nlibrary(data.table)"
  },
  {
    "objectID": "openscience.html#datasets",
    "href": "openscience.html#datasets",
    "title": "OpenScience",
    "section": "Datasets",
    "text": "Datasets\nWe will be exploring some demographic characteristics in Liverpool. To do that, we will use a dataset that contains population counts, split by ethnic origin. These counts are aggregated at the Lower Layer Super Output Area (LSOA from now on). LSOAs are an official Census geography defined by the Office of National Statistics. You can think of them, more or less, as neighbourhoods. Many data products (Census, deprivation indices, etc.) use LSOAs as one of their main geographies.\nTo make things easier, we will read data from a file posted online so, for now, you do not need to download any dataset:\nImport housesales data from csv\n\nRPython\n\n\n\ncensus2021 <- read.csv(\"data/census2021_ethn/liv_pop.csv\", row.names = \"GeographyCode\")\n\nLet us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n\nWe are using the method read.csv from base R, you could also use read_csv from library(\"readr\")\nHere the csv is based in the file data but it could also be a web address or sometimes you find data in packages\nThe argument row.names is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\nWe are using read.csv because the file we want to read is in the csv format. However, many more formats can be read into an R environment. A full list of formats supported may be found here.\n\n\n\n\n\n\nLet us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n\nWe are using the method read_csv from the pandas library, which we have imported with the alias pd.\nIn this form, all that is required is to pass the path to the file we want to read, which in this case is a web address.\nThe argument index_col is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\nWe are using read_csv because the file we want to read is in the csv format. However, pandas allows for many more formats to be read and write. A full list of formats supported may be found here.\n\nTo ensure we can access the data we have read, we store it in an object that we call db. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read_csv.\n\n\n\nTo ensure we can access the data we have read, we store it in an object that we call census2021. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read.csv.\nImportant\nYou need to store the data file on your computer, and read it locally. To do that, you can follow these steps: 1. Download the file by right-clicking on this link and saving the file 2. Place the file on the same folder as the notebook where you intend to read it"
  },
  {
    "objectID": "openscience.html#data-sliced-and-diced",
    "href": "openscience.html#data-sliced-and-diced",
    "title": "OpenScience",
    "section": "Data, sliced and diced",
    "text": "Data, sliced and diced\nNow we are ready to start playing and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the LSOAs in Liverpool, how many people live in each, by the region of the world where they were born. We call these tables DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain. Let’s explore a few of those cool tricks!\nStructure\nThe first aspect worth spending a bit of time is the structure of a DataFrame. We can print it by simply typing its name:\n\nRPython\n\n\n\nview(census2021)\n\n\n\n\n\n\n\nSince they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what we will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. In the example above, we can see how the column index is automatically picked up from the .csv file’s column names. For rows, we have specified when reading the file we wanted the column GeographyCode, so that is used. If we hadn’t specified any, pandas in Python or tidyverse in R will automatically generate a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so we will come to it over and over. Importantly, even when we move to spatial data, our datasets will have a similar structure.\nOne final feature that is worth mentioning about these tables is that they can hold columns with different types of data. In our example, this is not used as we have counts (or int, for integer, types) for each column. But it is useful to keep in mind we can combine this with columns that hold other type of data such as categories, text (str, for string), dates or, as we will see later in the course, geographic features.\nInspecting what it looks like. We can check the top (bottom) X lines of the table by passing X to the method head (tail). For example, for the top/bottom five lines:\n\nRPython\n\n\n\nhead(census2021) # read first 5 rows\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania\nE01006512                      0\nE01006513                      7\nE01006514                      5\nE01006515                      2\nE01006518                      4\nE01006519                      3\n\ntail(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033763   1302     68                  142                             11\nE01033764   2106     32                   49                             15\nE01033765   1277     21                   33                             17\nE01033766   1028     12                   20                              8\nE01033767   1003     29                   29                              5\nE01033768   1016     69                  111                             21\n          Antarctica.and.Oceania\nE01033763                      4\nE01033764                      0\nE01033765                      3\nE01033766                      7\nE01033767                      1\nE01033768                      6"
  },
  {
    "objectID": "openscience.html#summarise",
    "href": "openscience.html#summarise",
    "title": "OpenScience",
    "section": "Summarise",
    "text": "Summarise\nOr of the values of the table:\n\nRPython\n\n\n\nsummary(census2021)\n\n     Europe         Africa       Middle.East.and.Asia\n Min.   : 731   Min.   :  0.00   Min.   :  1.00      \n 1st Qu.:1331   1st Qu.:  7.00   1st Qu.: 16.00      \n Median :1446   Median : 14.00   Median : 33.50      \n Mean   :1462   Mean   : 29.82   Mean   : 62.91      \n 3rd Qu.:1580   3rd Qu.: 30.00   3rd Qu.: 62.75      \n Max.   :2551   Max.   :484.00   Max.   :840.00      \n The.Americas.and.the.Caribbean Antarctica.and.Oceania\n Min.   : 0.000                 Min.   : 0.00         \n 1st Qu.: 2.000                 1st Qu.: 0.00         \n Median : 5.000                 Median : 1.00         \n Mean   : 8.087                 Mean   : 1.95         \n 3rd Qu.:10.000                 3rd Qu.: 3.00         \n Max.   :61.000                 Max.   :11.00         \n\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nRPython\n\n\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00"
  },
  {
    "objectID": "openscience.html#queries",
    "href": "openscience.html#queries",
    "title": "OpenScience",
    "section": "Queries",
    "text": "Queries\nIndex-based queries\nHere we explore how we can subset parts of a DataFrame if we know exactly which bits we want. For example, if we want to extract the total and European population of the first four areas in the table:\n\nRPython\n\n\n\neu_tot_first4 <- census2021[c('E01006512', 'E01006513', 'E01006514', 'E01006515'), c('Total_Population', 'Europe')]\n\neu_tot_first4\n\n          Total_Population Europe\nE01006512             1880    910\nE01006513             2941   2225\nE01006514             2108   1786\nE01006515             1208    974\n\n\n\n\nwe use loc with lists:\n\n\n\nCondition-based queries\nHowever, sometimes, we do not know exactly which observations we want, but we do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose we want to select…\n\nRPython\n\n\nAreas with more than 900 people in Total:\n\npop900 <- census2021 %>%\n  filter(Total_Population > 900)\n\nAreas where there are no more than 750 Europeans:\n\neuro750 <- census2021 %>%\n  filter(Europe < 750)\n\nAreas with exactly ten person from Antarctica and Oceania:\n\noneOA <- census2021 %>%\n  filter(`Antarctica.and.Oceania` == 10)\n\n\n\n… areas with more than 2,500 people in Total:\n… areas where there are no more than 750 Europeans:\n… areas with exactly ten person from Antarctica and Oceania:\n\n\n\nPro-tip: These queries can grow in sophistication with almost no limits.\nCombining queries\nNow all of these queries can be combined with each other, for further flexibility. For example, imagine we want areas with more than 25 people from the Americas and Caribbean, but less than 1,500 in total:\n\nRPython\n\n\n\nac25_l500 <- census2021 %>%\n  filter(The.Americas.and.the.Caribbean > 25, Total_Population < 1500)\nac25_l500\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033750   1235     53                  129                             26\nE01033752   1024     19                  114                             33\nE01033754   1262     37                  112                             32\nE01033756    886     31                  221                             42\nE01033757    731     39                  223                             29\nE01033761   1138     52                  138                             33\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01033750                      5             1448      1448          1\nE01033752                      6             1196      1196          1\nE01033754                      9             1452      1452          1\nE01033756                      5             1185      1185          1\nE01033757                      3             1025      1025          1\nE01033761                     11             1372      1372          1"
  },
  {
    "objectID": "openscience.html#sorting",
    "href": "openscience.html#sorting",
    "title": "OpenScience",
    "section": "Sorting",
    "text": "Sorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine we want to sort the table by total population:\n\nRPython\n\n\n\ndb_pop_sorted <- census2021 %>%\n  arrange(desc(Total_Pop)) #sorts the dataframe by the \"Total_Pop\" column in descending order \n\nhead(db_pop_sorted)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006747   2551    163                  812                             24\nE01006513   2225     61                  595                             53\nE01006751   1843    139                  568                             21\nE01006524   2235     36                  125                             24\nE01006787   2187     53                   75                             13\nE01006537   2180     23                   46                              6\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01006747                      2             3552      3552          1\nE01006513                      7             2941      2941          1\nE01006751                      1             2572      2572          1\nE01006524                     11             2431      2431          1\nE01006787                      2             2330      2330          1\nE01006537                      2             2257      2257          1"
  },
  {
    "objectID": "openscience.html#visual-exploration",
    "href": "openscience.html#visual-exploration",
    "title": "OpenScience",
    "section": "Visual Exploration",
    "text": "Visual Exploration"
  },
  {
    "objectID": "openscience.html#python-4",
    "href": "openscience.html#python-4",
    "title": "OpenScience",
    "section": "Python",
    "text": "Python\n:::\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nRPython\n\n\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00   \n\n\n\n\n\n\n\n\nCreate new columns Delete columns"
  },
  {
    "objectID": "openscienceDIY.html#data-preparation",
    "href": "openscienceDIY.html#data-preparation",
    "title": "Do-It-Yourself",
    "section": "Data preparation",
    "text": "Data preparation\nBefore you can set off on your data journey, the dataset needs to be read, and there’s a couple of details we will get out of the way so it is then easier for you to start working.\nThe data are published on a Google Sheet.\nAs you will see, each row includes casualties recorded month by month, split by Taliban, Civilians, Afghan forces, and NATO.\nLet’s read it into an R or Python session:\n\nRPython\n\n\n\n# Specify the URL of the CSV file\nurl <- \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRa7OIBiz7-yqmgwUEn4V5Wm1TO8rGow_wQVS1PWp--UTCAKqNUhtifECO5ZR9XrMd6Ddq9NxQwf1ll/pub?gid=0&single=true&output=csv\"\n\n# Read the data from the URL into a DataFrame\ndata <- read.csv(url)\n\n# see the data\nhead(data)\n\n  Year    Month Taliban Civilians Afghan.forces Nato..detailed.in.spreadsheet.\n1 2004  January      15        51            23                               \n2 2004 February                 7             4                              5\n3 2004    March      19         2                                            2\n4 2004    April       5         3            19                               \n5 2004      May      18        29            56                              6\n6 2004     June     163        32            14                              2\n  Nato...official.figures\n1                      11\n2                       2\n3                       3\n4                       3\n5                       9\n6                       5\n\n\n\n\n\n\n\n\nThis allows us to read the data straight into a DataFrame, as we have done in the previous session.\nNow we are good to go!"
  },
  {
    "objectID": "environ.html#website-software",
    "href": "environ.html#website-software",
    "title": "Environment",
    "section": "Website Software",
    "text": "Website Software\nTo reproduce the code in the book, you need the most recent version of Quarto, R and relevant packages. These can be installed following the instructions provided in our R installation guide. Quarto (1.2.280) can be downloaded from the Quarto website, it may already be installed when you download R and R Studio."
  },
  {
    "objectID": "environR.html#r-basics-and-making-a-simple-map-of-london",
    "href": "environR.html#r-basics-and-making-a-simple-map-of-london",
    "title": "R",
    "section": "R Basics and Making a simple map of London",
    "text": "R Basics and Making a simple map of London\n\nStarting a session\nUpon startup, RStudio will look something like this. Note: the Pane Layout and Appearance settings can be altered e.g. on Mac OS by clicking RStudio>Preferences>Appearance and RStudio>Preferences>Pane Layout. I personally like to have my Console in the top right corner and Environment in the bottom left and keep the Source and Environment panes wider than Console and Files for easier readability. Default settings will probably have the Console in the bottom left and Environment in the top right. You will also have a standard white background; I personally use the Cobalt theme.\n\n\n\n\n\nAt the start of a session, it’s good practice clearing your R environment:\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set. You should have set this following the pre-recorded video.\n\ngetwd() \n\nIf the directory is not set yet, type in setwd(\"~/pathtodirectory\") to set it. It is crucial to perform this step at the beginning of your R script, so that relative paths can be used in the subsequent parts.\n\nsetwd(\"~/Dropbox/Github/gds\")\n\nIf you have set your directory correctly, it will show up at the top of the console pane:\n\n\n\n\n\n\n\nUsing the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\nR Objects\nEverything in R is an object. R possesses a simple generic function mechanism which can be used for an object-oriented style of programming. Indeed, everything that happens in R is the result of a function call (John M. Chambers). Method dispatch takes place based on the class of the first argument to the generic function.\nAll R statements where you create objects – “assignments” – have this form: object_name <- value. Assignment can also be performed using = instead of <-, but the standard advice is to use the latter syntax (see e.g. The R Inferno, ch. 8.2.26). In RStudio, the standard shortcut for the assignment operator <- is Alt + - (in Windows) or option + - (in Mac OS).\nA mock assignment of the value 30 to the name age is reported below. In order to inspect the content of the newly created variable, it is sufficient to type the name into the console. Within R, the hash symbol # is used to write comments and create collapsible code sections.\n\nage <- 30 # Assign the number 30 to the name \"age\"\nage # print the variable \"age\" to the console\n\n[1] 30\n\n\n\n\nA small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing <- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA\n\n\n\n\nLogical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n> greater than\n>= greater or equal to\n<= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not\n\n\n\nExamples\nLet’s create some random R objects:\n\n## Entering random \nLondon  <- 8982000 # population\nBristol <- 467099 # population\nLondon_area <-1572 # area km2\nBristol_area <-110 # area km2\n\nLondon\n\n[1] 8982000\n\n\nCalculate Population Density in London:\n\nLondon_pop_dens <- London/London_area\nBristol_pop_dens <- Bristol/Bristol_area\n\nLondon_pop_dens\n\n[1] 5713.74\n\n\nThe function c(), which you will use extensively if you keep coding in R, means “concatenate”. In this case, we use it to create a vector of population densities for London and Bristol:\n\nc(London_pop_dens, Bristol_pop_dens)\n\n[1] 5713.740 4246.355\n\npop_density <- c(London_pop_dens, Bristol_pop_dens) # In order to create a vector in R we make use of c() (which stands for concatenate)\n\nCreate a character variable:\n\nx <- \"a city\"\nclass(x)\n\n[1] \"character\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1\n\n\n\n\nData Structures\nObjects in R are typically stored in data structures. There are multiple types of data structures:\n\n\nVectors\nIn R, a vector is a sequence of elements which share the same data type. A vector supports logical, integer, double, character, complex, or raw data types.\n\n# first vector y\ny <- 1:10\nas.numeric(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\n# another vector z\nz <- c(2, 4, 56, 4)\nz\n\n[1]  2  4 56  4\n\n# and another one called cities\ncities <- c(\"London\", \"Bristol\", \"Bath\")\ncities\n\n[1] \"London\"  \"Bristol\" \"Bath\"   \n\n\n\n\nMatrices\nTwo-dimensional, rectangular, and homogeneous data structures. They are similar to vectors, with the additional attribute of having two dimensions: the number of rows and columns.\n\nm <- matrix(nrow = 2, ncol = 2)\nm\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\nn <- matrix(c(4, 5, 78, 56), nrow = 2, ncol = 2 )\nn\n\n     [,1] [,2]\n[1,]    4   78\n[2,]    5   56\n\n\n\n\nLists\nLists are containers which can store elements of different types and sizes. A list can contain vectors, matrices, dataframes, another list, functions which can be accessed, unlisted, and assigned to other objects.\n\nlist_data <- list(\"Red\", \"Green\", c(21,32,11), TRUE, 51.23, 119.1)\nprint(list_data)\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] 51.23\n\n[[6]]\n[1] 119.1\n\n\n\n\nData frames\nThey are the most common way of storing data in R and are the most used data structure for statistical analysis. Data frames are “rectangular lists”, i.e. tabular structures in which every element has the same length, and can also be thought of as lists of equal length vectors.\n\n## Here is a data frame of 3 columns named id, x, y and 10 rows\ndat <- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nhead(dat) # read first 5 rows\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ntail(dat)\n\n   id  x  y\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n\nDataframes in R are indexed by rows and columns numbers using the [rows,cols] syntax. The $ operator allows you to access columns in the dataframe, or to create new columns in the dataframe.\n\ndat[1,] # read first row and all colum ns\n\n  id x  y\n1  a 1 11\n\ndat[,1] # read all rows and the first column\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\ndat[6,3] # read 6th row, third column\n\n[1] 16\n\ndat[c(2:4),] # read rows 2 to 4 and all columns\n\n  id x  y\n2  b 2 12\n3  c 3 13\n4  d 4 14\n\ndat$y # read column y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat[dat$x<7,] # read rows that have a x value less than 7\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndat$new_column <- runif(10, 0, 1) # create a new variable called \"new_column\"\n\ndat\n\n   id  x  y new_column\n1   a  1 11  0.3230397\n2   b  2 12  0.2004565\n3   c  3 13  0.3654617\n4   d  4 14  0.6679464\n5   e  5 15  0.4820938\n6   f  6 16  0.8266886\n7   g  7 17  0.1367763\n8   h  8 18  0.1307404\n9   i  9 19  0.1557868\n10  j 10 20  0.8541773\n\n\n\n\nExercises 1\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector <- c(\"Elisabetta\", \"Capucine\", \"Lorenzo\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform <- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 2.17794816 0.05844188 1.55794470 0.55761688 0.20836326 2.56954717\n [7] 2.44263921 2.99906779 0.33792777 1.35694690\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector <- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix <-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df <-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\nlibrary(tidyverse)\n\n\n\nShow the code\nnew_df <-  new_df %>%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %>%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714"
  },
  {
    "objectID": "intro.html#open-science",
    "href": "intro.html#open-science",
    "title": "1 Introduction",
    "section": "Open Science",
    "text": "Open Science\nWhy do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\n\nFirst half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.\nThe 2018 Atlantic piece “The scientific paper is obsolete” on computational notebooks, by James Somers."
  },
  {
    "objectID": "openscience.html#columns",
    "href": "openscience.html#columns",
    "title": "OpenScience",
    "section": "Columns",
    "text": "Columns\nCreate new columns\nWe can generate new variables by applying operations on existing ones. For example, we can calculate the total population by area. Here is a couple of ways to do it:\n\nRPython\n\n\nBase R\n\ncensus2021$Total_Population <- rowSums(census2021[, c(\"Africa\", \"Middle.East.and.Asia\", \"Europe\", \"The.Americas.and.the.Caribbean\", \"Antarctica.and.Oceania\")])\n\ndplyr\n\ncensus2021 <- census2021 %>%\n  mutate(Total_Pop = rowSums(select(., Africa, Middle.East.and.Asia, Europe, The.Americas.and.the.Caribbean, Antarctica.and.Oceania)))\n\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Population Total_Pop\nE01006512                      0             1880      1880\nE01006513                      7             2941      2941\nE01006514                      5             2108      2108\nE01006515                      2             1208      1208\nE01006518                      4             1696      1696\nE01006519                      3             1286      1286\n\n\nA different spin on this is assigning new values: we can generate new variables with scalars, and modify those:\n\ncensus2021$new_column <- 1\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01006512                      0             1880      1880          1\nE01006513                      7             2941      2941          1\nE01006514                      5             2108      2108          1\nE01006515                      2             1208      1208          1\nE01006518                      4             1696      1696          1\nE01006519                      3             1286      1286          1\n\n\ndplyr is an immensely useful package in R because it streamlines and simplifies the process of data manipulation and transformation. With its intuitive and consistent syntax, dplyr provides a set of powerful and efficient functions that make tasks like filtering, summarizing, grouping, and joining datasets much more straightforward. Whether you’re working with small or large datasets, dplyr’s optimized code execution ensures fast and efficient operations. Its ability to chain functions together using the pipe operator (%>%) allows for a clean and readable code structure, enhancing code reproducibility and collaboration. Overall, dplyr is an indispensable tool for data analysts and scientists working in R, enabling them to focus on their data insights rather than wrestling with complex data manipulation code.\n\n\n\n\n\n\nDelete columns\nPermanently deleting variables is also within reach of one command:\n\nRPython\n\n\nBase R\n\ncensus2021 <- subset(census2021, select = -new_column)\n\ndplyr\n\ncensus2021 <- census2021 %>%\n  mutate(new_column = 1)"
  },
  {
    "objectID": "environR.html#installing-packages",
    "href": "environR.html#installing-packages",
    "title": "R",
    "section": "Installing packages",
    "text": "Installing packages\nIn R, packages are collections of functions, compiled code and sample data. They functionally act as “extensions” to the base R language, and can help you accomplish all operations you might want to perform in R (if no package serves your purpose, you may want to write an entirely new one!). Now, we will install the R package tidyverse. Look at the link to see what tidyverse includes, and directly load a .csv file (comma-separated values) into R from your computer.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse) \n\nImport data from csv:\n\nDensities_UK_cities <- read_csv(\"data/London/Tables/Densities_UK_cities.csv\")\n\nRows: 76 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): city, pop\ndbl (1): n\nnum (2): area, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nDensities_UK_cities\n\n# A tibble: 76 × 5\n       n city               pop        area density\n   <dbl> <chr>              <chr>     <dbl>   <dbl>\n 1     1 Greater London     9,787,426 1738.    5630\n 2     2 Greater Manchester 2,553,379  630.    4051\n 3     3 West Midlands      2,440,986  599.    4076\n 4     4 West Yorkshire     1,777,934  488.    3645\n 5     5 Greater Glasgow    957,620    368.    3390\n 6     6 Liverpool          864,122    200.    4329\n 7     7 South Hampshire    855,569    192     4455\n 8     8 Tyneside           774,891    180.    4292\n 9     9 Nottingham         729,977    176.    4139\n10    10 Sheffield          685,368    168.    4092\n# … with 66 more rows\n\n\nYou can also view the data set with:\n\nglimpse(Densities_UK_cities)\n\nRows: 76\nColumns: 5\n$ n       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ city    <chr> \"Greater London\", \"Greater Manchester\", \"West Midlands\", \"West…\n$ pop     <chr> \"9,787,426\", \"2,553,379\", \"2,440,986\", \"1,777,934\", \"957,620\",…\n$ area    <dbl> 1737.9, 630.3, 598.9, 487.8, 368.5, 199.6, 192.0, 180.5, 176.4…\n$ density <dbl> 5630, 4051, 4076, 3645, 3390, 4329, 4455, 4292, 4139, 4092, 42…\n\ntable(Densities_UK_cities$city)\n\n\n               Aberdeen  Accrington/ Rossendale Barnsley/ Dearne Valley \n                      1                       1                       1 \n               Basildon             Basingstoke                 Bedford \n                      1                       1                       1 \n                Belfast              Birkenhead               Blackburn \n                      1                       1                       1 \n              Blackpool      Bournemouth/ Poole       Brighton and Hove \n                      1                       1                       1 \n                Bristol                 Burnley       Burton-upon-Trent \n                      1                       1                       1 \n              Cambridge                 Cardiff              Chelmsford \n                      1                       1                       1 \n             Cheltenham            Chesterfield              Colchester \n                      1                       1                       1 \n               Coventry                 Crawley                   Derby \n                      1                       1                       1 \n              Doncaster                  Dundee              Eastbourne \n                      1                       1                       1 \n              Edinburgh                  Exeter  Farnborough/ Aldershot \n                      1                       1                       1 \n             Gloucester         Greater Glasgow          Greater London \n                      1                       1                       1 \n     Greater Manchester                 Grimsby                Hastings \n                      1                       1                       1 \n           High Wycombe                 Ipswich      Kingston upon Hull \n                      1                       1                       1 \n              Leicester                 Lincoln               Liverpool \n                      1                       1                       1 \n                  Luton               Maidstone               Mansfield \n                      1                       1                       1 \n           Medway Towns           Milton Keynes              Motherwell \n                      1                       1                       1 \n                Newport             Northampton                 Norwich \n                      1                       1                       1 \n             Nottingham                  Oxford       Paignton/ Torquay \n                      1                       1                       1 \n           Peterborough                Plymouth                 Preston \n                      1                       1                       1 \n                Reading               Sheffield                  Slough \n                      1                       1                       1 \n        South Hampshire         Southend-on-Sea          Stoke-on-Trent \n                      1                       1                       1 \n             Sunderland                 Swansea                 Swindon \n                      1                       1                       1 \n               Teesside                 Telford                  Thanet \n                      1                       1                       1 \n               Tyneside              Warrington           West Midlands \n                      1                       1                       1 \n         West Yorkshire                   Wigan               Worcester \n                      1                       1                       1 \n                   York \n                      1"
  },
  {
    "objectID": "environR.html#resources",
    "href": "environR.html#resources",
    "title": "R",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nR for Data Science. R4DS teaches you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it.\nSpatial Data Science by Edzer Pebesma and Roger Bivand introduces and explains the concepts underlying spatial data.\nGeo-computation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow."
  },
  {
    "objectID": "environR.html#r-basics",
    "href": "environR.html#r-basics",
    "title": "R",
    "section": "R Basics",
    "text": "R Basics\n\nStarting a session\nUpon startup, RStudio will look something like this. Note: the Pane Layout and Appearance settings can be altered e.g. on Mac OS by clicking RStudio>Preferences>Appearance and RStudio>Preferences>Pane Layout. I personally like to have my Console in the top right corner and Environment in the bottom left and keep the Source and Environment panes wider than Console and Files for easier readability. Default settings will probably have the Console in the bottom left and Environment in the top right. You will also have a standard white background; I personally use the Cobalt theme.\n\n\n\n\n\nAt the start of a session, it’s good practice clearing your R environment:\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set. You should have set this following the pre-recorded video.\n\ngetwd() \n\nIf the directory is not set yet, type in setwd(\"~/pathtodirectory\") to set it. It is crucial to perform this step at the beginning of your R script, so that relative paths can be used in the subsequent parts.\n\nsetwd(\"~/Dropbox/Github/gds\")\n\nIf you have set your directory correctly, it will show up at the top of the console pane:\n\n\n\n\n\n\n\nUsing the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\nR Objects\nEverything in R is an object. R possesses a simple generic function mechanism which can be used for an object-oriented style of programming. Indeed, everything that happens in R is the result of a function call (John M. Chambers). Method dispatch takes place based on the class of the first argument to the generic function.\nAll R statements where you create objects – “assignments” – have this form: object_name <- value. Assignment can also be performed using = instead of <-, but the standard advice is to use the latter syntax (see e.g. The R Inferno, ch. 8.2.26). In RStudio, the standard shortcut for the assignment operator <- is Alt + - (in Windows) or option + - (in Mac OS).\nA mock assignment of the value 30 to the name age is reported below. In order to inspect the content of the newly created variable, it is sufficient to type the name into the console. Within R, the hash symbol # is used to write comments and create collapsible code sections.\n\nage <- 30 # Assign the number 30 to the name \"age\"\nage # print the variable \"age\" to the console\n\n[1] 30"
  },
  {
    "objectID": "environR.html#a-small-note-on-variable-types",
    "href": "environR.html#a-small-note-on-variable-types",
    "title": "R",
    "section": "A small note on variable types",
    "text": "A small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing <- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA"
  },
  {
    "objectID": "environR.html#logical-operators-and-expressions",
    "href": "environR.html#logical-operators-and-expressions",
    "title": "R",
    "section": "Logical operators and expressions",
    "text": "Logical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n> greater than\n>= greater or equal to\n<= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not"
  },
  {
    "objectID": "environR.html#examples",
    "href": "environR.html#examples",
    "title": "R",
    "section": "Examples",
    "text": "Examples\nLet’s create some random R objects:\n\n## Entering random \nLondon  <- 8982000 # population\nBristol <- 467099 # population\nLondon_area <-1572 # area km2\nBristol_area <-110 # area km2\n\nLondon\n\n[1] 8982000\n\n\nCalculate Population Density in London:\n\nLondon_pop_dens <- London/London_area\nBristol_pop_dens <- Bristol/Bristol_area\n\nLondon_pop_dens\n\n[1] 5713.74\n\n\nThe function c(), which you will use extensively if you keep coding in R, means “concatenate”. In this case, we use it to create a vector of population densities for London and Bristol:\n\nc(London_pop_dens, Bristol_pop_dens)\n\n[1] 5713.740 4246.355\n\npop_density <- c(London_pop_dens, Bristol_pop_dens) # In order to create a vector in R we make use of c() (which stands for concatenate)\n\nCreate a character variable:\n\nx <- \"a city\"\nclass(x)\n\n[1] \"character\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1"
  },
  {
    "objectID": "environR.html#data-structures",
    "href": "environR.html#data-structures",
    "title": "R",
    "section": "Data Structures",
    "text": "Data Structures\nObjects in R are typically stored in data structures. There are multiple types of data structures:\n\nVectors\nIn R, a vector is a sequence of elements which share the same data type. A vector supports logical, integer, double, character, complex, or raw data types.\n\n# first vector y\ny <- 1:10\nas.numeric(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\n# another vector z\nz <- c(2, 4, 56, 4)\nz\n\n[1]  2  4 56  4\n\n# and another one called cities\ncities <- c(\"London\", \"Bristol\", \"Bath\")\ncities\n\n[1] \"London\"  \"Bristol\" \"Bath\"   \n\n\n\n\nMatrices\nTwo-dimensional, rectangular, and homogeneous data structures. They are similar to vectors, with the additional attribute of having two dimensions: the number of rows and columns.\n\nm <- matrix(nrow = 2, ncol = 2)\nm\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\nn <- matrix(c(4, 5, 78, 56), nrow = 2, ncol = 2 )\nn\n\n     [,1] [,2]\n[1,]    4   78\n[2,]    5   56\n\n\n\n\nLists\nLists are containers which can store elements of different types and sizes. A list can contain vectors, matrices, dataframes, another list, functions which can be accessed, unlisted, and assigned to other objects.\n\nlist_data <- list(\"Red\", \"Green\", c(21,32,11), TRUE, 51.23, 119.1)\nprint(list_data)\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] 51.23\n\n[[6]]\n[1] 119.1\n\n\n\n\nData frames\nThey are the most common way of storing data in R and are the most used data structure for statistical analysis. Data frames are “rectangular lists”, i.e. tabular structures in which every element has the same length, and can also be thought of as lists of equal length vectors.\n\n## Here is a data frame of 3 columns named id, x, y and 10 rows\ndat <- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nhead(dat) # read first 5 rows\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ntail(dat)\n\n   id  x  y\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n\nDataframes in R are indexed by rows and columns numbers using the [rows,cols] syntax. The $ operator allows you to access columns in the dataframe, or to create new columns in the dataframe.\n\ndat[1,] # read first row and all colum ns\n\n  id x  y\n1  a 1 11\n\ndat[,1] # read all rows and the first column\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\ndat[6,3] # read 6th row, third column\n\n[1] 16\n\ndat[c(2:4),] # read rows 2 to 4 and all columns\n\n  id x  y\n2  b 2 12\n3  c 3 13\n4  d 4 14\n\ndat$y # read column y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat[dat$x<7,] # read rows that have a x value less than 7\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndat$new_column <- runif(10, 0, 1) # create a new variable called \"new_column\"\n\ndat\n\n   id  x  y new_column\n1   a  1 11 0.31130612\n2   b  2 12 0.51108502\n3   c  3 13 0.05251427\n4   d  4 14 0.98527547\n5   e  5 15 0.59702848\n6   f  6 16 0.70949907\n7   g  7 17 0.54361005\n8   h  8 18 0.29220352\n9   i  9 19 0.04759733\n10  j 10 20 0.86567181"
  },
  {
    "objectID": "environR.html#exercises-1",
    "href": "environR.html#exercises-1",
    "title": "R",
    "section": "Exercises 1",
    "text": "Exercises 1\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector <- c(\"Elisabetta\", \"Capucine\", \"Lorenzo\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform <- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 1.36448262 0.94936261 1.13249667 0.01386841 1.14539223 1.34002270\n [7] 2.38620325 0.13660425 0.56899676 0.37095379\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector <- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix <-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df <-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\nlibrary(tidyverse)\n\n\n\nShow the code\nnew_df <-  new_df %>%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %>%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714"
  },
  {
    "objectID": "spatialdata_code.html#visual-exploration",
    "href": "spatialdata_code.html#visual-exploration",
    "title": "Spatial Data",
    "section": "Visual exploration",
    "text": "Visual exploration"
  },
  {
    "objectID": "openscience.html#additional-lab-materials",
    "href": "openscience.html#additional-lab-materials",
    "title": "OpenScience",
    "section": "Additional lab materials",
    "text": "Additional lab materials\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis”\nA good introduction to data manipulation in R is the “Data wrangling” chapter in R for Data Science.\nA good extension is Hadley Wickham’ “Tidy data” paper which presents a very popular way of organising tabular data for efficient manipulation."
  }
]