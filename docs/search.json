[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A course in Geographic Data Science",
    "section": "",
    "text": "This is the website for the “Geographic Data Science” module ENVS363/563 at the University of Liverpool. This is course designed and delivered by Dr. Elisabetta Pietrostefani and Dr. Carmen Cabrera-Arnau from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. Much of the course material is inspired by Dani Arribas-Bel’s course on Geographic Data Science.\nThis module will introduce students to the field of Geographic Data Science (GDS), a discipline established at the intersection between Geographic Information Science (GIS) and Data Science. The course covers how the modern GIS toolkit can be integrated with Data Science tools to solve practical real-world problems.\nCore to the set of employable skills to be taught in this course is an introduction to programming tools. Students will be able to whether to develop their skills in either R (or Python) in Lab sessions.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "A course in Geographic Data Science",
    "section": "Contact",
    "text": "Contact\n\nElisabetta Pietrostefani - e.pietrostefani [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 6xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.\n\n\nCarmen Cabrera-Arnau - c.cabrera-arnau [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 6xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Further readings\nWatch: Solving Life’s Everyday Problems with Data"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "openscience.html",
    "href": "openscience.html",
    "title": "1 - Open Science",
    "section": "",
    "text": "Concepts\nThe ideas behind this block are better communicated through narrative than video or lectures. Hence, the concepts section are delivered through a few references you are expected to read. These will total up about one and a half hours of your focused time.\n\n\nOpen Science\nThe first part of this block is about setting the philosophical background. Why do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\nRead the chapter here. Estimated time: 15min.\n\nFirst half of Chapter 1 in \"Geographic Data Science with PySAL and the PyData stack\" reyABwolf.\n\nRead the piece here. Estimated time: 35min.\n\nThe 2018 Atlantic piece \"The scientific paper is obsolete\" on computational notebooks, by James Somers somers2018scientific.\n\n\n\nModern Scientific Tools\nOnce we know a bit more about why we should care about the tools we use, let's dig into those that will underpin much of this course. This part is interesting in itself, but will also valuable to better understand the practical aspects of the course. Again, we have two reads here to set the tone and complement the practical introduction we saw in the Hands-on and DIY parts of the previous block. We are closing the circle here:\nRead the chapter here.\n\nSecond half of Chapter 1 in \"Geographic Data Science with PySAL and the PyData stack\" reyABwolf.\nThe chapter in the GIS&T Book of Knowledge on computational notebooks, by Geoff Boeing and Dani Arribas-Bel."
  },
  {
    "objectID": "overview.html#aims",
    "href": "overview.html#aims",
    "title": "Overview",
    "section": "Aims",
    "text": "Aims\nThe module has three main aims.\n\nProvide students with core competences in Geographic Data Science (GDS). This includes advancing their statistical and numerical literacy and introducing basic principles of programming and state-of-the-art computational tools for GDS;\nPresent a comprehensive overview of the main methodologies available to the Geographic Data Scientist, as well as their intuition as to how and when they can be applied;\nFocus on real world applications of these techniques in a geographical and applied context."
  },
  {
    "objectID": "overview.html#learning-outcomes",
    "href": "overview.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the module, students should be able to:\nFor all\n\nDemonstrate advanced GIS/GDS concepts and be able to use the tools programmatically to import, manipulate and analyse data in different formats.\nUnderstand the motivation and inner workings of the main methodological approaches of GDS, both analytical and visual.\nEvaluate the suitability of a specific technique, what it can offer and how it can help answer questions of interest.\nApply a number of spatial analysis techniques and how to interpret the results, in the process of turning data into information.\nWhen faced with a new data-set, work independently using GIS/GDS tools programmatically.\n\nOnly for MSc students\n\nDemonstrate a sound understanding of how real-world (geo)data are produced, their potential insights and biases, as well as opportunities and limitations."
  },
  {
    "objectID": "overview.html#feedback",
    "href": "overview.html#feedback",
    "title": "Overview",
    "section": "Feedback",
    "text": "Feedback\nFormal assessment of one map, one MCQ test and one computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during computer, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times. Response time will vary depending on the complexity of the question and staff availability."
  },
  {
    "objectID": "overview.html#computational-environment",
    "href": "overview.html#computational-environment",
    "title": "Overview",
    "section": "Computational Environment",
    "text": "Computational Environment\nADD SOMETHING ABOUR R or Python\nEDIT the below\nThis course can be followed by anyone with access to a bit of technical infrastructure. This section details the set of local and online requirements you will need to be able to follow along, as well as instructions or pointers to get set up on your own. This is a centralized section that lists everything you will require, but keep in mind that different blocks do not always require everything all the time.\nTo reproduce the code in the book, you need the most recent version of R and packages. These can be installed following the instructions provided in our R installation guide.\n\nSoftware\nEDIT\nTo run the analysis and reproduce the code, you need the following software:\n\nQGIS- the stable version (3.22 LTR at the time of writing) is OK, any more recent version will also work.\nR-4.2.2\nRStudio 2022.12.0-353\nQuarto 1.2.280\nthe list of libraries in the next section\n\nTo install and update:\n\nQGIS, download the appropriate version from QGIS.org\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\n\n\nR List of libraries\nThe list of libraries used in this book is provided below:\n\n\nPython set-up\n\n\nOnline accounts"
  },
  {
    "objectID": "assess.html#assignment-i",
    "href": "assess.html#assignment-i",
    "title": "Assessments",
    "section": "Assignment I",
    "text": "Assignment I\n\nTitle: Programmed Map\nType: Coursework\nDue date: 30th October\n25% of the final mark\nChance to be reassessed\nElectronic submission only\n\nThis assignment will be evaluated on technical data processing, map design abilities (assemblage), and design overall narrative.\nOnce you have created your map, you will need to present it. Write up to 500 about the choices you made to create the map.\n\nFull Assignment details\n\n\n.qmd file\n\n\nSubmit\nOnce completed, you will need to submit the following:\nAn html version of an .qmd document with R integrated code.\nThe assignment will be evaluated based on three main pillars, on which you will have to be successful to achieve a good mark:\n\nData Processing: Your proficiency in handling and manipulating data will be a fundamental aspect of the assessment.\nMap assemblage This includes your ability to master technologies that allow you to create a compelling map.\nDesign and narrative: Your success in designing an appealing map with a compelling narrative will play a pivotal role in your overall evaluation."
  },
  {
    "objectID": "assess.html#assignment-ii",
    "href": "assess.html#assignment-ii",
    "title": "Assessments",
    "section": "Assignment II",
    "text": "Assignment II\n\nTitle: Multiple Choice Questions - test\nType: 16th November - during the Lab. 90 mins\nDue date: TBC\n25% of the final mark\nChance to be reassessed\nElectronic submission only\n\n\nTo ensure students are engaging with the course content as it progresses and\nTo provide core learning in advance of the third assessment."
  },
  {
    "objectID": "assess.html#marking-criteria",
    "href": "assess.html#marking-criteria",
    "title": "Assessments",
    "section": "Marking Criteria",
    "text": "Marking Criteria\nThis course follows the standard marking criteria (the general ones and those relating to GIS assignments in particular) set by the School of Environmental Sciences. Please make sure to check the student handbook and familiarise with them. In addition to these generic criteria, the following specific criteria will be used in cases where computer code is part of the work being assessed:\n\n0-15: the code does not run and there is no documentation to follow it.\n16-39: the code does not run, or runs but it does not produce the expected outcome. There is some documentation explaining its logic.\n40-49: the code runs and produces the expected output. There is some documentation explaining its logic.\n50-59: the code runs and produces the expected output. There is extensive documentation explaining its logic.\n60-69: the code runs and produces the expected output. There is extensive documentation, properly formatted, explaining its logic.\n70-79: all as above, plus the code design includes clear evidence of skills presented in advanced sections of the course (e.g. custom methods, list comprehensions, etc.).\n80-100: all as above, plus the code contains novel contributions that extend/improve the functionality the student was provided with (e.g. algorithm optimizations, novel methods to perform the task, etc.)."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Introduction and Open Science\n\nLecture: Introduction to the module & Open Science\nLab: Setting up your Computational Environment & Data Wrangling\n\nSpatial Data\n\nLecture:\nLab:\n\nMapping Vector Data\n\nLecture:\nLab:\n\nMapping Raster Data\n\nLecture:\nLab:\n\nAssignment I: Programmed Map\nSpatial Weights\n\nLecture:\nLab:\n\nESDA\n\nLecture:\nLab:\n\nAssignment II: MCQ test\nClustering\n\nLecture:\nLab:\n\nInterpolation, heatmaps and point patterns\n\nLecture:\nLab:\n\nSpatial Network Analysis\n\nLecture:\nLab:\n\nAssignment III: A computational essay"
  },
  {
    "objectID": "concepts_openscience.html",
    "href": "concepts_openscience.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_spatialdata.html",
    "href": "concepts_spatialdata.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_mapvector.html",
    "href": "concepts_mapvector.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_mapraster.html",
    "href": "concepts_mapraster.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "environR.html#r-list-of-libraries",
    "href": "environR.html#r-list-of-libraries",
    "title": "R",
    "section": "R List of libraries",
    "text": "R List of libraries\nThe list of libraries used in this book is provided below:\n\nsf\ngeojsonsf\nmapview"
  },
  {
    "objectID": "intro.html#what-is-geographic-data-science",
    "href": "intro.html#what-is-geographic-data-science",
    "title": "Introduction",
    "section": "What is Geographic Data Science?",
    "text": "What is Geographic Data Science?\nThe following clip is taken from a keynote response by Dani Arribas-Bel at the first Spatial Data Science Conference, organised by CARTO and held in Brooklyn in 2017. The talk provides a bit of background and context, which will hopefully help you understand a bit better what Geographic Data Science is.\nTOP UP with slide content"
  },
  {
    "objectID": "intro.html#get-ready",
    "href": "intro.html#get-ready",
    "title": "Introduction",
    "section": "Get ready!",
    "text": "Get ready!\nGo the the Computation Environment section"
  },
  {
    "objectID": "intro.html#further-readings",
    "href": "intro.html#further-readings",
    "title": "1  Introduction",
    "section": "2.1 Further readings",
    "text": "2.1 Further readings\nTo get a better picture, the following readings complement the overview provided above very well:\nBonus\nWatch ! All Maps are wrong https://www.youtube.com/watch?v=kIID5FDi2JQ Watch: Solving Life’s Everyday Problems with Data https://www.sciencefriday.com/segments/solving-lifes-everyday-problems-with-data/\nThe chapter is available free online HTML | PDF\n\nThe introductory chapter to “Doing Data Science” schutt2013doing, by Cathy O’Neil and Rachel Schutt is general overview of why we needed Data Science and where if came from.\nA slightly more technical historical perspective on where Data Science came from and where it might go can be found in David Donoho’s recent overview donoho201750.\nA geographic take on Data Science, proposing more interaction between Geography and Data Science singleton2019geographic."
  },
  {
    "objectID": "environ.html#software",
    "href": "environ.html#software",
    "title": "Environment",
    "section": "Software",
    "text": "Software\nTo run the analysis and reproduce the code, you need the following software:\n\nQGIS- the stable version (3.22 LTR at the time of writing) is OK, any more recent version will also work.\nQGIS, download the appropriate version from QGIS.org\nQuarto 1.2.280\nQuarto, download the appropriate version from the Quarto website"
  },
  {
    "objectID": "assess.html#assignment-iii",
    "href": "assess.html#assignment-iii",
    "title": "Assessments",
    "section": "Assignment III",
    "text": "Assignment III\n\nTitle: Computational Essay\nType: Coursework\nDue date: 8th January\n50% of the final mark\nChance to be reassessed\nElectronic submission only\n\nA 2500 word computational essay on a geographic data set which they have explored and analysed using the skills and techniques developed during the course. Students will complete an essay which combines both code, data visualisation and prose supported by references in order to demonstrate sound understanding of all learning outcomes.\nOverview\nHere’s the premise. You will take the role of a real-world geographic sata scientist tasked to explore datasets on New York City and find useful insights for a variety of city decision-makers. It does not matter if you have never been to New York City. In fact, this will help you focus on what you can learn about the city through the data, without the influence of prior knowledge. Furthermore, the assessment will not be marked based on how much you know about New York City but instead about how much you can show you have learned through analysing data. You will need contextualise your project by highlighting the opportunities and limitations of ‘old’ and ‘new’ forms of spatial data and reference relevant literature.\nWhat is a Computational Essay?\nA computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. This piece of assessment is equivalent to 2,500 word. However, this is the overall weight. Since you will need to create not only narrative but also code and figures, here are the requirements:\n\nMaximum of 1,000 words (ordinary text) (references do not contribute to the word count). You should answer the specified questions within the narrative. The questions should be included within a wider analysis.\nUp to four maps or figures (a figure may include more than one map and will only count as one but needs to be integrated in the same overall output)\nUp to one table\n\nThere are three kinds of elements in a computational essay:\n\nOrdinary text (in English)\nComputer input (R)\nComputer output These three elements all work together to express what’s being communicated."
  },
  {
    "objectID": "environ.html#book-software",
    "href": "environ.html#book-software",
    "title": "Environment",
    "section": "Book Software",
    "text": "Book Software\nTo reproduce the code in the book, you need the most recent version of Quarto, R and relevant packages. These can be installed following the instructions provided in our R installation guide. Quarto (1.2.280) can be downloaded from the Quarto website, it may already be installed when you download R and R Studio."
  },
  {
    "objectID": "intro.html#from-geographic-data-science-to-geographic-data-science",
    "href": "intro.html#from-geographic-data-science-to-geographic-data-science",
    "title": "1 Introduction",
    "section": "From Geographic Data Science to Geographic Data Science",
    "text": "From Geographic Data Science to Geographic Data Science\nGeographic Information holds a pivotal position within our modern societies, permeating various aspects of our daily lives. It underpins essential sectors such as housing, transportation, insurance, banking, telecommunications, logistics, energy, retail, agriculture, healthcare, and urban planning. Its significance lies in the capacity to analyze and derive invaluable insights from geo-spatial data, enabling us to make informed decisions and address complex challenges. Proficiency in this field equips individuals with the ability to work with real-world data across multiple domains and tackle diverse problems. Furthermore, it provides the opportunity to acquire essential data science skills and utilize important tools for answering spatial questions. Given its wide-ranging applications and the increasing reliance on location-based information, there is a substantial demand for experts in the geographic information industry, making it a highly sought-after skill set in today’s workforce.\nWhat information does GIS use?\n\nData that defines geographical features like roads, rivers\nSoil types, land use, elevation\nDemographics, socioeconomic attributes\nEnvironmental, climate, air-quality\nAnnotations that label features and places\n\nGeographic Data Science\nA GIS person typically produces cartographic and analytical products using desktop software. A geospatial data scientist creates code and runs pipelines that produce analytical products and cartographic representations.\nThis entails working with real-world data from various domains and tackling a wide range of complex problems. Through this process geospatial data science includes both data science and GIS tools that lead to the analysos of intricate spatial questions effectively. The synergy between CyberGIS and Geographic Data Science is unmistakable, with coding playing a pivotal role in enabling the seamless development of interactive data analysis. By leveraging cutting-edge technologies and innovative methodologies, this symbiotic relationship enhances the accessibility, scalability, and interactivity of geospatial data analysis. Consequently, it opens up new vistas for collaborative research and decision-making processes.\nThis multifaceted approach equips them with the knowledge and expertise to navigate the intricate world of spatial data analysis and contribute meaningfully to diverse fields where location-based insights are invaluable."
  },
  {
    "objectID": "intro.html#a-useful-clip-cannot-find-it",
    "href": "intro.html#a-useful-clip-cannot-find-it",
    "title": "Introduction",
    "section": "A useful clip (cannot find it)",
    "text": "A useful clip (cannot find it)\nThe following clip is taken from a keynote response by Dani Arribas-Bel at the first Spatial Data Science Conference, organised by CARTO and held in Brooklyn in 2017. The talk provides a bit of background and context, which will hopefully help you understand a bit better what Geographic Data Science is."
  },
  {
    "objectID": "intro.html#open-science-1",
    "href": "intro.html#open-science-1",
    "title": "1 Introduction",
    "section": "Open Science",
    "text": "Open Science\nWhy do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\n\nFirst half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.\nThe 2018 Atlantic piece “The scientific paper is obsolete” on computational notebooks, by James Somers."
  },
  {
    "objectID": "intro.html#modern-scientific-tools",
    "href": "intro.html#modern-scientific-tools",
    "title": "1 Introduction",
    "section": "Modern Scientific Tools",
    "text": "Modern Scientific Tools\nOnce we know a bit more about why we should care about the tools we use, let’s dig into those that will underpin much of this course. This part is interesting in itself, but will also valuable to better understand the practical aspects of the course. Again, we have two reads here to set the tone and complement the practical introduction we saw in the Hands-on and DIY parts of the previous block. We are closing the circle here:\n\nSecond half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists."
  },
  {
    "objectID": "spatialdata.html",
    "href": "spatialdata.html",
    "title": "2 Spatial Data",
    "section": "",
    "text": "Special about spatial data\nAll data is spatial - data comes from observation, and observation needs to happen somewhere and at some time. This makes all data spatial. For a lot of data, the location expressed in spatial, earth-bound coordinates of observation is not of prime importance:\nThese core spatial geometries are all supported in R package sf.\nWatch: Nathan Yau’s https://flowingdata.com/\nInspired by material from https://r-spatial.org/book/ and https://geocompr.robinlovelace.net/index.html\nhttps://www.visualcapitalist.com/problem-with-our-maps/ https://jcheshire.com/atlas-of-the-invisible-education/do-maps-lie/"
  },
  {
    "objectID": "environPy.html",
    "href": "environPy.html",
    "title": "Python",
    "section": "",
    "text": "Resources\nSome help along the way with:\n\nGeographic Data Science with Python by Sergio J. Rey, Dani Arribas-Bel, Levi J. Wolf"
  },
  {
    "objectID": "environR.html",
    "href": "environR.html",
    "title": "R",
    "section": "",
    "text": "Resources\nSome help along the way with:"
  },
  {
    "objectID": "openscienceR.html",
    "href": "openscienceR.html",
    "title": "1  | include: false",
    "section": "",
    "text": "OpenScience in R\nOnce we know a bit about what computational notebooks are and why we should care about them, let’s jump to using them! This section introduces you to using R or Python for manipulating tabular data. Please read through it carefully and pay attention to how ideas about manipulating data are translated into code that “does stuff”. For this part, you can read directly from the course website, although it is recommended you follow the section interactively by running code on your own.\nOnce you have read through and have a bit of a sense of how things work, jump on the Do-It-Yourself section, which will provide you with a challenge to complete it on your own, and will allow you to put what you have already learnt to good use."
  },
  {
    "objectID": "openscienceR.html#data-wrangling",
    "href": "openscienceR.html#data-wrangling",
    "title": "OpenScience in R",
    "section": "Data wrangling",
    "text": "Data wrangling\nReal world datasets are messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to munge them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but has to do with much more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nFor how labor intensive and relevant this aspect is, there is surprisingly very little published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation. In this session, you will use a few real world datasets and learn how to process them into Python so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the bread and butter of data analysis and scientific computing in Python. These are fundamental tools that are constantly used in almost any task relating to data analysis.\nThis notebook covers the basic and the content that is expected to be learnt by every student. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at. As a companion to this introduction, there is an additional notebook (see link on the website page for Lab 01) that covers how the dataset used here was prepared from raw data downloaded from the internet, and includes some additional exercises you can do if you want dig deeper into the content of this lab.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need, so we can get that out of the way and focus on the task at hand:"
  },
  {
    "objectID": "openscienceR.html#loading-packages",
    "href": "openscienceR.html#loading-packages",
    "title": "OpenScience in R",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\nPythonR\n\n\n\n\n\n\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tmap)"
  },
  {
    "objectID": "syllabus.html#part-1",
    "href": "syllabus.html#part-1",
    "title": "Syllabus",
    "section": "Part 1",
    "text": "Part 1\nWeek 1 - Introduction and Open Science\n\nLecture: Introduction to the module & Open Science\nLab: Setting up your Computational Environment & Data Wrangling\n\nWeek 2 - Spatial Data\nWeek 3 - Mapping Vector Data\nWeek 4 - Mapping Raster Data\nWeek 5 - No lecture\nAssignment I: Programmed Map"
  },
  {
    "objectID": "syllabus.html#part-2",
    "href": "syllabus.html#part-2",
    "title": "Syllabus",
    "section": "Part 2",
    "text": "Part 2\nWeek 6 - Spatial Weights\nWeek 7 - ESDA\nWeek 8\nAssignment II: MCQ test\nWeek 9 - Clustering\nWeek 10 - Interpolation, heatmaps and point patterns\nWeek 11 - Spatial Network Analysis\nWeek 12 - No lecture\nAssignment III: A computational essay"
  },
  {
    "objectID": "openscienceDIY.html#import-libraries",
    "href": "openscienceDIY.html#import-libraries",
    "title": "Do-It-Yourself",
    "section": "Import libraries",
    "text": "Import libraries\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "openscienceDIY.html#tasks",
    "href": "openscienceDIY.html#tasks",
    "title": "Do-It-Yourself",
    "section": "Tasks",
    "text": "Tasks\nNow, the challenge is to put to work what we have learnt in this block. For that, the suggestion is that you carry out an analysis of the Afghan Logs in a similar way as how we looked at population composition in Liverpool. These are of course very different datasets reflecting immensely different realities. Their structure, however, is relatively parallel: both capture counts aggregated by a spatial (neighbourhood) or temporal unit (month), and each count is split by a few categories.\nTry to answer the following questions:\n\nObtain the minimum number of civilian casualties (in what month was that?)\nHow many NATO casualties were registered in August 2008?\nWhat is the month with the most total number of casualties?\n\nTip: You will need to first create a column with total counts"
  },
  {
    "objectID": "openscience.html#data-wrangling",
    "href": "openscience.html#data-wrangling",
    "title": "Lab",
    "section": "Data wrangling",
    "text": "Data wrangling\nReal world datasets tend to be messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to wrangle (manipulate, transform and structure) them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but to more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nIn this session, you will use a few real world datasets and learn how to process them in R so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the fundamental tools of data analysis and scientific computing. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need to run the code:"
  },
  {
    "objectID": "openscience.html#loading-packages",
    "href": "openscience.html#loading-packages",
    "title": "Lab",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\nlibrary(tidyverse) # a structure of data manipulation including several packages \nlibrary(data.table)"
  },
  {
    "objectID": "openscience.html#datasets",
    "href": "openscience.html#datasets",
    "title": "Lab",
    "section": "Datasets",
    "text": "Datasets\nWe will be exploring some demographic characteristics in Liverpool. To do that, we will use a dataset that contains population counts, split by ethnic origin. These counts are aggregated at the Lower Layer Super Output Area (LSOA from now on). LSOAs are an official Census geography defined by the Office of National Statistics. You can think of them, more or less, as neighbourhoods. Many data products (Census, deprivation indices, etc.) use LSOAs as one of their main geographies.\nTo do this, we will download a data folder from github called census2021_ethn. You should place this in a data folder you will use throughout the course.\nImport housesales data from csv\n\ncensus2021 &lt;- read.csv(\"data/census2021_ethn/liv_pop.csv\", row.names = \"GeographyCode\")\n\nLet us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n\nWe are using the method read.csv from base R, you could also use read_csv from library(\"readr\").\nHere the csv is based on a data file but it could also be a web address or sometimes you find data in packages.\nThe argument row.names is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\nWe are using read.csv because the file we want to read is in the csv format. However, many more formats can be read into an R environment. A full list of formats supported may be found here.\nTo ensure we can access the data we have read, we store it in an object that we call census2021. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read.csv.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to store the data file on your computer, and read it locally. To do that, you can follow these steps: 1. Download the census2021_ethn file by right-clicking on this link and saving the file 2. Place the file in a data folder you have created where you intend to read it. 3. Your folder should have the following structure a. a gds folder (where you will save your quarto documents) b. a data folder c. the census2021_ethn folder inside your data folder."
  },
  {
    "objectID": "openscience.html#data-sliced-and-diced",
    "href": "openscience.html#data-sliced-and-diced",
    "title": "Lab",
    "section": "Data, sliced and diced",
    "text": "Data, sliced and diced\nNow we are ready to start playing with and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the LSOAs in Liverpool, how many people live in each, by the region of the world where they were born. We call these tables DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain.\nStructure\nLet’s start by exploring the structure of a DataFrame. We can print it by simply typing its name:\n\nview(census2021)\n\nSince they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what we will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. In the example above, we can see how the column index is automatically picked up from the .csv file’s column names. For rows, we have specified when reading the file we wanted the column GeographyCode, so that is used. If we hadn’t specified any, tidyverse in R will automatically generate a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so we will come to it over and over. Importantly, even when we move to spatial data, our datasets will have a similar structure.\nOne further feature of these tables is that they can hold columns with different types of data. In our example, this is not used as we have counts (or int, for integer, types) for each column. But it is useful to keep in mind we can combine this with columns that hold other type of data such as categories, text (str, for string), dates or, as we will see later in the course, geographic features.\nInspecting\nWe can check the top (bottom) X lines of the table by passing X to the method head (tail). For example, for the top/bottom five lines:\n\nhead(census2021) # read first 5 rows\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania\nE01006512                      0\nE01006513                      7\nE01006514                      5\nE01006515                      2\nE01006518                      4\nE01006519                      3\n\ntail(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033763   1302     68                  142                             11\nE01033764   2106     32                   49                             15\nE01033765   1277     21                   33                             17\nE01033766   1028     12                   20                              8\nE01033767   1003     29                   29                              5\nE01033768   1016     69                  111                             21\n          Antarctica.and.Oceania\nE01033763                      4\nE01033764                      0\nE01033765                      3\nE01033766                      7\nE01033767                      1\nE01033768                      6\n\n\nSummarise\nWe can get an overview of the values of the table:\n\nsummary(census2021)\n\n     Europe         Africa       Middle.East.and.Asia\n Min.   : 731   Min.   :  0.00   Min.   :  1.00      \n 1st Qu.:1331   1st Qu.:  7.00   1st Qu.: 16.00      \n Median :1446   Median : 14.00   Median : 33.50      \n Mean   :1462   Mean   : 29.82   Mean   : 62.91      \n 3rd Qu.:1580   3rd Qu.: 30.00   3rd Qu.: 62.75      \n Max.   :2551   Max.   :484.00   Max.   :840.00      \n The.Americas.and.the.Caribbean Antarctica.and.Oceania\n Min.   : 0.000                 Min.   : 0.00         \n 1st Qu.: 2.000                 1st Qu.: 0.00         \n Median : 5.000                 Median : 1.00         \n Mean   : 8.087                 Mean   : 1.95         \n 3rd Qu.:10.000                 3rd Qu.: 3.00         \n Max.   :61.000                 Max.   :11.00         \n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00"
  },
  {
    "objectID": "openscience.html#summarise",
    "href": "openscience.html#summarise",
    "title": "OpenScience",
    "section": "Summarise",
    "text": "Summarise\nOr of the values of the table:\n\nRPython\n\n\n\nsummary(census2021)\n\n     Europe         Africa       Middle.East.and.Asia\n Min.   : 731   Min.   :  0.00   Min.   :  1.00      \n 1st Qu.:1331   1st Qu.:  7.00   1st Qu.: 16.00      \n Median :1446   Median : 14.00   Median : 33.50      \n Mean   :1462   Mean   : 29.82   Mean   : 62.91      \n 3rd Qu.:1580   3rd Qu.: 30.00   3rd Qu.: 62.75      \n Max.   :2551   Max.   :484.00   Max.   :840.00      \n The.Americas.and.the.Caribbean Antarctica.and.Oceania\n Min.   : 0.000                 Min.   : 0.00         \n 1st Qu.: 2.000                 1st Qu.: 0.00         \n Median : 5.000                 Median : 1.00         \n Mean   : 8.087                 Mean   : 1.95         \n 3rd Qu.:10.000                 3rd Qu.: 3.00         \n Max.   :61.000                 Max.   :11.00         \n\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nRPython\n\n\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00"
  },
  {
    "objectID": "openscience.html#queries",
    "href": "openscience.html#queries",
    "title": "Lab",
    "section": "Queries",
    "text": "Queries\nIndex-based queries\nHere we explore how we can subset parts of a DataFrame if we know exactly which bits we want. For example, if we want to extract the total and European population of the first four areas in the table:\nWe can select with c(). If this structure is new to you have a look here.\n\neu_tot_first4 &lt;- census2021[c('E01006512', 'E01006513', 'E01006514', 'E01006515'), c('Total_Population', 'Europe')]\n\neu_tot_first4\n\n          Total_Population Europe\nE01006512             1880    910\nE01006513             2941   2225\nE01006514             2108   1786\nE01006515             1208    974\n\n\nCondition-based queries\nHowever, sometimes, we do not know exactly which observations we want, but we do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose we want to select…\nAreas with more than 900 people in Total:\n\npop900 &lt;- census2021 %&gt;%\n  filter(Total_Population &gt; 900)\n\nAreas where there are no more than 750 Europeans:\n\neuro750 &lt;- census2021 %&gt;%\n  filter(Europe &lt; 750)\n\nAreas with exactly ten person from Antarctica and Oceania:\n\noneOA &lt;- census2021 %&gt;%\n  filter(`Antarctica.and.Oceania` == 10)\n\nPro-tip: These queries can grow in sophistication with almost no limits.\nCombining queries\nNow all of these queries can be combined with each other, for further flexibility. For example, imagine we want areas with more than 25 people from the Americas and Caribbean, but less than 1,500 in total:\n\nac25_l500 &lt;- census2021 %&gt;%\n  filter(The.Americas.and.the.Caribbean &gt; 25, Total_Population &lt; 1500)\nac25_l500\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033750   1235     53                  129                             26\nE01033752   1024     19                  114                             33\nE01033754   1262     37                  112                             32\nE01033756    886     31                  221                             42\nE01033757    731     39                  223                             29\nE01033761   1138     52                  138                             33\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01033750                      5             1448      1448          1\nE01033752                      6             1196      1196          1\nE01033754                      9             1452      1452          1\nE01033756                      5             1185      1185          1\nE01033757                      3             1025      1025          1\nE01033761                     11             1372      1372          1"
  },
  {
    "objectID": "openscience.html#sorting",
    "href": "openscience.html#sorting",
    "title": "Lab",
    "section": "Sorting",
    "text": "Sorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine we want to sort the table by total population:\n\ndb_pop_sorted &lt;- census2021 %&gt;%\n  arrange(desc(Total_Pop)) #sorts the dataframe by the \"Total_Pop\" column in descending order \n\nhead(db_pop_sorted)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006747   2551    163                  812                             24\nE01006513   2225     61                  595                             53\nE01006751   1843    139                  568                             21\nE01006524   2235     36                  125                             24\nE01006787   2187     53                   75                             13\nE01006537   2180     23                   46                              6\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01006747                      2             3552      3552          1\nE01006513                      7             2941      2941          1\nE01006751                      1             2572      2572          1\nE01006524                     11             2431      2431          1\nE01006787                      2             2330      2330          1\nE01006537                      2             2257      2257          1"
  },
  {
    "objectID": "openscience.html#visual-exploration",
    "href": "openscience.html#visual-exploration",
    "title": "OpenScience",
    "section": "Visual Exploration",
    "text": "Visual Exploration"
  },
  {
    "objectID": "openscience.html#python-4",
    "href": "openscience.html#python-4",
    "title": "OpenScience",
    "section": "Python",
    "text": "Python\n:::\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nRPython\n\n\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00   \n\n\n\n\n\n\n\n\nCreate new columns Delete columns"
  },
  {
    "objectID": "openscienceDIY.html#data-preparation",
    "href": "openscienceDIY.html#data-preparation",
    "title": "Do-It-Yourself",
    "section": "Data preparation",
    "text": "Data preparation\nBefore you can set off on your data journey, the dataset needs to be read, and there’s a couple of details we will get out of the way so it is then easier for you to start working.\nThe data are published on a Google Sheet.\nAs you will see, each row includes casualties recorded month by month, split by Taliban, Civilians, Afghan forces, and NATO.\nLet’s read it into an R session:\n\n# Specify the URL of the CSV file\nurl &lt;- \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRa7OIBiz7-yqmgwUEn4V5Wm1TO8rGow_wQVS1PWp--UTCAKqNUhtifECO5ZR9XrMd6Ddq9NxQwf1ll/pub?gid=0&single=true&output=csv\"\n\n# Read the data from the URL into a DataFrame\ndata &lt;- read.csv(url)\n\n# see the data\nhead(data)\n\n  Year    Month Taliban Civilians Afghan.forces Nato..detailed.in.spreadsheet.\n1 2004  January      15        51            23                               \n2 2004 February                 7             4                              5\n3 2004    March      19         2                                            2\n4 2004    April       5         3            19                               \n5 2004      May      18        29            56                              6\n6 2004     June     163        32            14                              2\n  Nato...official.figures\n1                      11\n2                       2\n3                       3\n4                       3\n5                       9\n6                       5\n\n\nThis allows us to read the data straight into a data frame, as we have done in the previous session.\nNow we are good to go!"
  },
  {
    "objectID": "environ.html#website-software",
    "href": "environ.html#website-software",
    "title": "Environment",
    "section": "Website Software",
    "text": "Website Software\nTo reproduce the code in the book, you need the most recent version of Quarto, R and relevant packages. These can be installed following the instructions provided in our R installation guide. Quarto (1.2.280) can be downloaded from the Quarto website, it may already be installed when you download R and R Studio."
  },
  {
    "objectID": "environR.html#r-basics-and-making-a-simple-map-of-london",
    "href": "environR.html#r-basics-and-making-a-simple-map-of-london",
    "title": "R",
    "section": "R Basics and Making a simple map of London",
    "text": "R Basics and Making a simple map of London\n\nStarting a session\nUpon startup, RStudio will look something like this. Note: the Pane Layout and Appearance settings can be altered e.g. on Mac OS by clicking RStudio>Preferences>Appearance and RStudio>Preferences>Pane Layout. I personally like to have my Console in the top right corner and Environment in the bottom left and keep the Source and Environment panes wider than Console and Files for easier readability. Default settings will probably have the Console in the bottom left and Environment in the top right. You will also have a standard white background; I personally use the Cobalt theme.\n\n\n\n\n\nAt the start of a session, it’s good practice clearing your R environment:\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set. You should have set this following the pre-recorded video.\n\ngetwd() \n\nIf the directory is not set yet, type in setwd(\"~/pathtodirectory\") to set it. It is crucial to perform this step at the beginning of your R script, so that relative paths can be used in the subsequent parts.\n\nsetwd(\"~/Dropbox/Github/gds\")\n\nIf you have set your directory correctly, it will show up at the top of the console pane:\n\n\n\n\n\n\n\nUsing the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\nR Objects\nEverything in R is an object. R possesses a simple generic function mechanism which can be used for an object-oriented style of programming. Indeed, everything that happens in R is the result of a function call (John M. Chambers). Method dispatch takes place based on the class of the first argument to the generic function.\nAll R statements where you create objects – “assignments” – have this form: object_name <- value. Assignment can also be performed using = instead of <-, but the standard advice is to use the latter syntax (see e.g. The R Inferno, ch. 8.2.26). In RStudio, the standard shortcut for the assignment operator <- is Alt + - (in Windows) or option + - (in Mac OS).\nA mock assignment of the value 30 to the name age is reported below. In order to inspect the content of the newly created variable, it is sufficient to type the name into the console. Within R, the hash symbol # is used to write comments and create collapsible code sections.\n\nage <- 30 # Assign the number 30 to the name \"age\"\nage # print the variable \"age\" to the console\n\n[1] 30\n\n\n\n\nA small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing <- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA\n\n\n\n\nLogical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n> greater than\n>= greater or equal to\n<= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not\n\n\n\nExamples\nLet’s create some random R objects:\n\n## Entering random \nLondon  <- 8982000 # population\nBristol <- 467099 # population\nLondon_area <-1572 # area km2\nBristol_area <-110 # area km2\n\nLondon\n\n[1] 8982000\n\n\nCalculate Population Density in London:\n\nLondon_pop_dens <- London/London_area\nBristol_pop_dens <- Bristol/Bristol_area\n\nLondon_pop_dens\n\n[1] 5713.74\n\n\nThe function c(), which you will use extensively if you keep coding in R, means “concatenate”. In this case, we use it to create a vector of population densities for London and Bristol:\n\nc(London_pop_dens, Bristol_pop_dens)\n\n[1] 5713.740 4246.355\n\npop_density <- c(London_pop_dens, Bristol_pop_dens) # In order to create a vector in R we make use of c() (which stands for concatenate)\n\nCreate a character variable:\n\nx <- \"a city\"\nclass(x)\n\n[1] \"character\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1\n\n\n\n\nData Structures\nObjects in R are typically stored in data structures. There are multiple types of data structures:\n\n\nVectors\nIn R, a vector is a sequence of elements which share the same data type. A vector supports logical, integer, double, character, complex, or raw data types.\n\n# first vector y\ny <- 1:10\nas.numeric(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\n# another vector z\nz <- c(2, 4, 56, 4)\nz\n\n[1]  2  4 56  4\n\n# and another one called cities\ncities <- c(\"London\", \"Bristol\", \"Bath\")\ncities\n\n[1] \"London\"  \"Bristol\" \"Bath\"   \n\n\n\n\nMatrices\nTwo-dimensional, rectangular, and homogeneous data structures. They are similar to vectors, with the additional attribute of having two dimensions: the number of rows and columns.\n\nm <- matrix(nrow = 2, ncol = 2)\nm\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\nn <- matrix(c(4, 5, 78, 56), nrow = 2, ncol = 2 )\nn\n\n     [,1] [,2]\n[1,]    4   78\n[2,]    5   56\n\n\n\n\nLists\nLists are containers which can store elements of different types and sizes. A list can contain vectors, matrices, dataframes, another list, functions which can be accessed, unlisted, and assigned to other objects.\n\nlist_data <- list(\"Red\", \"Green\", c(21,32,11), TRUE, 51.23, 119.1)\nprint(list_data)\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] 51.23\n\n[[6]]\n[1] 119.1\n\n\n\n\nData frames\nThey are the most common way of storing data in R and are the most used data structure for statistical analysis. Data frames are “rectangular lists”, i.e. tabular structures in which every element has the same length, and can also be thought of as lists of equal length vectors.\n\n## Here is a data frame of 3 columns named id, x, y and 10 rows\ndat <- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nhead(dat) # read first 5 rows\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ntail(dat)\n\n   id  x  y\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n\nDataframes in R are indexed by rows and columns numbers using the [rows,cols] syntax. The $ operator allows you to access columns in the dataframe, or to create new columns in the dataframe.\n\ndat[1,] # read first row and all colum ns\n\n  id x  y\n1  a 1 11\n\ndat[,1] # read all rows and the first column\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\ndat[6,3] # read 6th row, third column\n\n[1] 16\n\ndat[c(2:4),] # read rows 2 to 4 and all columns\n\n  id x  y\n2  b 2 12\n3  c 3 13\n4  d 4 14\n\ndat$y # read column y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat[dat$x<7,] # read rows that have a x value less than 7\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndat$new_column <- runif(10, 0, 1) # create a new variable called \"new_column\"\n\ndat\n\n   id  x  y new_column\n1   a  1 11  0.3230397\n2   b  2 12  0.2004565\n3   c  3 13  0.3654617\n4   d  4 14  0.6679464\n5   e  5 15  0.4820938\n6   f  6 16  0.8266886\n7   g  7 17  0.1367763\n8   h  8 18  0.1307404\n9   i  9 19  0.1557868\n10  j 10 20  0.8541773\n\n\n\n\nExercises 1\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector <- c(\"Elisabetta\", \"Capucine\", \"Lorenzo\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform <- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 2.17794816 0.05844188 1.55794470 0.55761688 0.20836326 2.56954717\n [7] 2.44263921 2.99906779 0.33792777 1.35694690\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector <- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix <-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df <-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\nlibrary(tidyverse)\n\n\n\nShow the code\nnew_df <-  new_df %>%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %>%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714"
  },
  {
    "objectID": "intro.html#open-science",
    "href": "intro.html#open-science",
    "title": "1 Introduction",
    "section": "Open Science",
    "text": "Open Science\nWhy do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\n\nFirst half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.\nThe 2018 Atlantic piece “The scientific paper is obsolete” on computational notebooks, by James Somers."
  },
  {
    "objectID": "openscience.html#columns",
    "href": "openscience.html#columns",
    "title": "Lab",
    "section": "Columns",
    "text": "Columns\nCreate new columns\nWe can generate new variables by applying operations on existing ones. For example, we can calculate the total population by area. Here is a couple of ways to do it:\nOn base R\n\ncensus2021$Total_Population &lt;- rowSums(census2021[, c(\"Africa\", \"Middle.East.and.Asia\", \"Europe\", \"The.Americas.and.the.Caribbean\", \"Antarctica.and.Oceania\")])\n\nUsing the package dplyr\n\ncensus2021 &lt;- census2021 %&gt;%\n  mutate(Total_Pop = rowSums(select(., Africa, Middle.East.and.Asia, Europe, The.Americas.and.the.Caribbean, Antarctica.and.Oceania)))\n\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Population Total_Pop\nE01006512                      0             1880      1880\nE01006513                      7             2941      2941\nE01006514                      5             2108      2108\nE01006515                      2             1208      1208\nE01006518                      4             1696      1696\nE01006519                      3             1286      1286\n\n\nA different spin on this is assigning new values: we can generate new variables with scalars, and modify those:\n\ncensus2021$new_column &lt;- 1\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01006512                      0             1880      1880          1\nE01006513                      7             2941      2941          1\nE01006514                      5             2108      2108          1\nE01006515                      2             1208      1208          1\nE01006518                      4             1696      1696          1\nE01006519                      3             1286      1286          1\n\n\ndplyr is an immensely useful package in R because it streamlines and simplifies the process of data manipulation and transformation. With its intuitive and consistent syntax, dplyr provides a set of powerful and efficient functions that make tasks like filtering, summarizing, grouping, and joining datasets much more straightforward. Whether you’re working with small or large datasets, dplyr’s optimized code execution ensures fast and efficient operations. Its ability to chain functions together using the pipe operator (%&gt;%) allows for a clean and readable code structure, enhancing code reproducibility and collaboration. Overall, dplyr is an indispensable tool for data analysts and scientists working in R, enabling them to focus on their data insights rather than wrestling with complex data manipulation code.\nDelete columns\nPermanently deleting variables is also within reach of one command:\nBase R\n\ncensus2021 &lt;- subset(census2021, select = -new_column)\n\ndplyr\n\ncensus2021 &lt;- census2021 %&gt;%\n  mutate(new_column = 1)"
  },
  {
    "objectID": "environR.html#installing-packages",
    "href": "environR.html#installing-packages",
    "title": "R",
    "section": "Installing packages",
    "text": "Installing packages\nIn R, packages are collections of functions, compiled code and sample data. They functionally act as “extensions” to the base R language, and can help you accomplish all operations you might want to perform in R (if no package serves your purpose, you may want to write an entirely new one!). Now, we will install the R package tidyverse. Look at the link to see what tidyverse includes, and directly load a .csv file (comma-separated values) into R from your computer.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "environR.html#resources",
    "href": "environR.html#resources",
    "title": "R",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nR for Data Science. R4DS teaches you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it.\nSpatial Data Science by Edzer Pebesma and Roger Bivand introduces and explains the concepts underlying spatial data.\nGeo-computation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow."
  },
  {
    "objectID": "environR.html#r-basics",
    "href": "environR.html#r-basics",
    "title": "R",
    "section": "R Basics",
    "text": "R Basics\n\nStarting a session\nUpon startup, RStudio will look something like this. Note: the Pane Layout and Appearance settings can be altered e.g. on Mac OS by clicking RStudio>Preferences>Appearance and RStudio>Preferences>Pane Layout. I personally like to have my Console in the top right corner and Environment in the bottom left and keep the Source and Environment panes wider than Console and Files for easier readability. Default settings will probably have the Console in the bottom left and Environment in the top right. You will also have a standard white background; but you can chose specific themes.\n\n\n\n\n\nAt the start of a session, it’s good practice clearing your R environment:\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set. You should have set this following the pre-recorded video.\n\ngetwd() \n\nIf the directory is not set yet, type in setwd(\"~/pathtodirectory\") to set it. It is crucial to perform this step at the beginning of your R script, so that relative paths can be used in the subsequent parts.\n\nsetwd(\"~/Dropbox/Github/gds\")\n\nIf you have set your directory correctly, it will show up at the top of the console pane:\n\n\n\n\n\nImportant: You do not need to set your working directory if you are using an R-markdown or Quarto document and you have it saved in the right location. The pathway will start from where your document is saved.\n\n\nUsing the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\nR Objects\nEverything in R is an object. R possesses a simple generic function mechanism which can be used for an object-oriented style of programming. Indeed, everything that happens in R is the result of a function call (John M. Chambers). Method dispatch takes place based on the class of the first argument to the generic function.\nAll R statements where you create objects – “assignments” – have this form: object_name <- value. Assignment can also be performed using = instead of <-, but the standard advice is to use the latter syntax (see e.g. The R Inferno, ch. 8.2.26). In RStudio, the standard shortcut for the assignment operator <- is Alt + - (in Windows) or option + - (in Mac OS).\nA mock assignment of the value 30 to the name age is reported below. In order to inspect the content of the newly created variable, it is sufficient to type the name into the console. Within R, the hash symbol # is used to write comments and create collapsible code sections.\n\nage <- 30 # Assign the number 30 to the name \"age\"\nage # print the variable \"age\" to the console\n\n[1] 30\n\n\n\n\nA small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing <- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA\n\n\n\n\nLogical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n> greater than\n>= greater or equal to\n<= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not"
  },
  {
    "objectID": "environR.html#a-small-note-on-variable-types",
    "href": "environR.html#a-small-note-on-variable-types",
    "title": "R",
    "section": "A small note on variable types",
    "text": "A small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing <- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA"
  },
  {
    "objectID": "environR.html#logical-operators-and-expressions",
    "href": "environR.html#logical-operators-and-expressions",
    "title": "R",
    "section": "Logical operators and expressions",
    "text": "Logical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n> greater than\n>= greater or equal to\n<= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not"
  },
  {
    "objectID": "environR.html#examples",
    "href": "environR.html#examples",
    "title": "R",
    "section": "Examples",
    "text": "Examples\nLet’s create some random R objects:\n\n## Entering random \nLondon  <- 8982000 # population\nBristol <- 467099 # population\nLondon_area <-1572 # area km2\nBristol_area <-110 # area km2\n\nLondon\n\n[1] 8982000\n\n\nCalculate Population Density in London:\n\nLondon_pop_dens <- London/London_area\nBristol_pop_dens <- Bristol/Bristol_area\n\nLondon_pop_dens\n\n[1] 5713.74\n\n\nThe function c(), which you will use extensively if you keep coding in R, means “concatenate”. In this case, we use it to create a vector of population densities for London and Bristol:\n\nc(London_pop_dens, Bristol_pop_dens)\n\n[1] 5713.740 4246.355\n\npop_density <- c(London_pop_dens, Bristol_pop_dens) # In order to create a vector in R we make use of c() (which stands for concatenate)\n\nCreate a character variable:\n\nx <- \"a city\"\nclass(x)\n\n[1] \"character\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1"
  },
  {
    "objectID": "environR.html#data-structures",
    "href": "environR.html#data-structures",
    "title": "R",
    "section": "Data Structures",
    "text": "Data Structures\nObjects in R are typically stored in data structures. There are multiple types of data structures:\n\nVectors\nIn R, a vector is a sequence of elements which share the same data type. A vector supports logical, integer, double, character, complex, or raw data types.\n\n# first vector y\ny <- 1:10\nas.numeric(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\n# another vector z\nz <- c(2, 4, 56, 4)\nz\n\n[1]  2  4 56  4\n\n# and another one called cities\ncities <- c(\"London\", \"Bristol\", \"Bath\")\ncities\n\n[1] \"London\"  \"Bristol\" \"Bath\"   \n\n\n\n\nMatrices\nTwo-dimensional, rectangular, and homogeneous data structures. They are similar to vectors, with the additional attribute of having two dimensions: the number of rows and columns.\n\nm <- matrix(nrow = 2, ncol = 2)\nm\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\nn <- matrix(c(4, 5, 78, 56), nrow = 2, ncol = 2 )\nn\n\n     [,1] [,2]\n[1,]    4   78\n[2,]    5   56\n\n\n\n\nLists\nLists are containers which can store elements of different types and sizes. A list can contain vectors, matrices, dataframes, another list, functions which can be accessed, unlisted, and assigned to other objects.\n\nlist_data <- list(\"Red\", \"Green\", c(21,32,11), TRUE, 51.23, 119.1)\nprint(list_data)\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] 51.23\n\n[[6]]\n[1] 119.1\n\n\n\n\nData frames\nThey are the most common way of storing data in R and are the most used data structure for statistical analysis. Data frames are “rectangular lists”, i.e. tabular structures in which every element has the same length, and can also be thought of as lists of equal length vectors.\n\n## Here is a data frame of 3 columns named id, x, y and 10 rows\ndat <- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nhead(dat) # read first 5 rows\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ntail(dat)\n\n   id  x  y\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n\nDataframes in R are indexed by rows and columns numbers using the [rows,cols] syntax. The $ operator allows you to access columns in the dataframe, or to create new columns in the dataframe.\n\ndat[1,] # read first row and all colum ns\n\n  id x  y\n1  a 1 11\n\ndat[,1] # read all rows and the first column\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\ndat[6,3] # read 6th row, third column\n\n[1] 16\n\ndat[c(2:4),] # read rows 2 to 4 and all columns\n\n  id x  y\n2  b 2 12\n3  c 3 13\n4  d 4 14\n\ndat$y # read column y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat[dat$x<7,] # read rows that have a x value less than 7\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndat$new_column <- runif(10, 0, 1) # create a new variable called \"new_column\"\n\ndat\n\n   id  x  y new_column\n1   a  1 11 0.24409650\n2   b  2 12 0.03560325\n3   c  3 13 0.82042254\n4   d  4 14 0.80079558\n5   e  5 15 0.25702070\n6   f  6 16 0.94972479\n7   g  7 17 0.15214774\n8   h  8 18 0.98108283\n9   i  9 19 0.48167385\n10  j 10 20 0.22331529"
  },
  {
    "objectID": "environR.html#exercises-1",
    "href": "environR.html#exercises-1",
    "title": "R",
    "section": "Exercises 1",
    "text": "Exercises 1\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector <- c(\"Elisabetta\", \"Capucine\", \"Lorenzo\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform <- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 1.36448262 0.94936261 1.13249667 0.01386841 1.14539223 1.34002270\n [7] 2.38620325 0.13660425 0.56899676 0.37095379\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector <- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix <-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df <-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\nlibrary(tidyverse)\n\n\n\nShow the code\nnew_df <-  new_df %>%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %>%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714"
  },
  {
    "objectID": "spatialdata_code.html#visual-exploration",
    "href": "spatialdata_code.html#visual-exploration",
    "title": "Spatial Data",
    "section": "Visual exploration",
    "text": "Visual exploration"
  },
  {
    "objectID": "openscience.html#additional-lab-materials",
    "href": "openscience.html#additional-lab-materials",
    "title": "OpenScience",
    "section": "Additional lab materials",
    "text": "Additional lab materials\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis”\nA good introduction to data manipulation in R is the “Data wrangling” chapter in R for Data Science.\nA good extension is Hadley Wickham’ “Tidy data” paper which presents a very popular way of organising tabular data for efficient manipulation."
  },
  {
    "objectID": "openscience.html#additional-resources",
    "href": "openscience.html#additional-resources",
    "title": "Lab",
    "section": "Additional resources",
    "text": "Additional resources\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis”\nA good introduction to data manipulation in R is the “Data wrangling” chapter in R for Data Science.\nA good extension is Hadley Wickham’ “Tidy data” paper which presents a very popular way of organising tabular data for efficient manipulation."
  },
  {
    "objectID": "environR.html#exercises",
    "href": "environR.html#exercises",
    "title": "R",
    "section": "Exercises",
    "text": "Exercises\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector <- c(\"Elisabetta\", \"Carmen\", \"Habib\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform <- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 1.1175108 2.4480687 2.4927297 2.7243597 2.1288363 0.8949736 1.7117790\n [8] 1.9318872 2.1184755 2.8638416\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector <- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix <-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df <-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\n\nShow the code\nnew_df <-  new_df %>%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %>%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714"
  },
  {
    "objectID": "environR.html#import-data-from-csv",
    "href": "environR.html#import-data-from-csv",
    "title": "R",
    "section": "Import data from csv",
    "text": "Import data from csv\n\nDensities_UK_cities <- read_csv(\"data/London/Tables/Densities_UK_cities.csv\")\n\nRows: 76 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): city, pop\ndbl (1): n\nnum (2): area, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nDensities_UK_cities\n\n# A tibble: 76 × 5\n       n city               pop        area density\n   <dbl> <chr>              <chr>     <dbl>   <dbl>\n 1     1 Greater London     9,787,426 1738.    5630\n 2     2 Greater Manchester 2,553,379  630.    4051\n 3     3 West Midlands      2,440,986  599.    4076\n 4     4 West Yorkshire     1,777,934  488.    3645\n 5     5 Greater Glasgow    957,620    368.    3390\n 6     6 Liverpool          864,122    200.    4329\n 7     7 South Hampshire    855,569    192     4455\n 8     8 Tyneside           774,891    180.    4292\n 9     9 Nottingham         729,977    176.    4139\n10    10 Sheffield          685,368    168.    4092\n# … with 66 more rows\n\n\nYou can also view the data set with:\n\nglimpse(Densities_UK_cities)\n\nRows: 76\nColumns: 5\n$ n       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ city    <chr> \"Greater London\", \"Greater Manchester\", \"West Midlands\", \"West…\n$ pop     <chr> \"9,787,426\", \"2,553,379\", \"2,440,986\", \"1,777,934\", \"957,620\",…\n$ area    <dbl> 1737.9, 630.3, 598.9, 487.8, 368.5, 199.6, 192.0, 180.5, 176.4…\n$ density <dbl> 5630, 4051, 4076, 3645, 3390, 4329, 4455, 4292, 4139, 4092, 42…\n\ntable(Densities_UK_cities$city)\n\n\n               Aberdeen  Accrington/ Rossendale Barnsley/ Dearne Valley \n                      1                       1                       1 \n               Basildon             Basingstoke                 Bedford \n                      1                       1                       1 \n                Belfast              Birkenhead               Blackburn \n                      1                       1                       1 \n              Blackpool      Bournemouth/ Poole       Brighton and Hove \n                      1                       1                       1 \n                Bristol                 Burnley       Burton-upon-Trent \n                      1                       1                       1 \n              Cambridge                 Cardiff              Chelmsford \n                      1                       1                       1 \n             Cheltenham            Chesterfield              Colchester \n                      1                       1                       1 \n               Coventry                 Crawley                   Derby \n                      1                       1                       1 \n              Doncaster                  Dundee              Eastbourne \n                      1                       1                       1 \n              Edinburgh                  Exeter  Farnborough/ Aldershot \n                      1                       1                       1 \n             Gloucester         Greater Glasgow          Greater London \n                      1                       1                       1 \n     Greater Manchester                 Grimsby                Hastings \n                      1                       1                       1 \n           High Wycombe                 Ipswich      Kingston upon Hull \n                      1                       1                       1 \n              Leicester                 Lincoln               Liverpool \n                      1                       1                       1 \n                  Luton               Maidstone               Mansfield \n                      1                       1                       1 \n           Medway Towns           Milton Keynes              Motherwell \n                      1                       1                       1 \n                Newport             Northampton                 Norwich \n                      1                       1                       1 \n             Nottingham                  Oxford       Paignton/ Torquay \n                      1                       1                       1 \n           Peterborough                Plymouth                 Preston \n                      1                       1                       1 \n                Reading               Sheffield                  Slough \n                      1                       1                       1 \n        South Hampshire         Southend-on-Sea          Stoke-on-Trent \n                      1                       1                       1 \n             Sunderland                 Swansea                 Swindon \n                      1                       1                       1 \n               Teesside                 Telford                  Thanet \n                      1                       1                       1 \n               Tyneside              Warrington           West Midlands \n                      1                       1                       1 \n         West Yorkshire                   Wigan               Worcester \n                      1                       1                       1 \n                   York \n                      1"
  },
  {
    "objectID": "mapvector.html#visual-exploration",
    "href": "mapvector.html#visual-exploration",
    "title": "3 Mapping Vector Data",
    "section": "Visual exploration",
    "text": "Visual exploration"
  },
  {
    "objectID": "spatialdata_code.html",
    "href": "spatialdata_code.html",
    "title": "Lab",
    "section": "",
    "text": "Python\nAlthough not too complicated, the way to access borders in geopandas is not as straightforward as it is the case for other aspects of the map, such as size or frame."
  },
  {
    "objectID": "spatialdata_code.html#installing-packages",
    "href": "spatialdata_code.html#installing-packages",
    "title": "Lab",
    "section": "Installing packages",
    "text": "Installing packages\nWe will start by loading core packages for working with spatial data. See detailed description of R.\n\n# Load the 'sf' library, which stands for Simple Features, used for working with spatial data.\nlibrary(sf)\n# Load the 'tidyverse' library, a collection of packages for data manipulation and visualization.\nlibrary(tidyverse)\n# Load the 'tmap' library, which is used for creating thematic maps and visualizing spatial data.\nlibrary(tmap)\n# The 'readr' library provides a fast and user-friendly way to read data from common formats like CSV.\nlibrary(readr)\n# Converts Between GeoJSON and simple feature objects\nlibrary(geojsonsf) \n# Using data from OpenStreetMap (OSM)\nlibrary(osmdata)\n# Static maps\nlibrary(basemapR)\n\nTo install basemapR you will need to do run\n\nlibrary(devtools)\ninstall_github('Chrisjb/basemapR')"
  },
  {
    "objectID": "spatialdata_code.html#datasets",
    "href": "spatialdata_code.html#datasets",
    "title": "Lab",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\npoi_df = tribble(\n  ~name, ~lon, ~lat,\n  \"The British Museum\",        -0.1459604, 51.5045975,\n  \"Big Ben\",    -0.1272057, 51.5007325,\n  \"King's Cross\", -0.1319481, 51.5301701,\n  \"The Natural History Museum\",     -0.173734, 51.4938451\n)\npoi_sf = sf::st_as_sf(poi_df, coords = c(\"lon\", \"lat\"), crs = \"EPSG:4326\")\n\n\n\nTypes of Data\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPolygonsLinesPoints\n\n\nWe first import the district shapefile use read_sf, we then plot it to make sure we are seeing it ‘correctly’. We us $geometry to plot just the geometry, if we don’t include $geometry R will plot the first 9 columns and if the dataset is large this is not advisable.\n\ndistricts <- read_sf(\"data/London/Polygons/districts.shp\")\n\nplot(districts$geometry) # Create a simple plot\n\n\n\n\n\n\nWe them import a file of roads in London and plot it.\n\na_roads <- read_sf(\"data/London/Lines/a_roads.shp\")\n\n# If you needed to import a `geojson` this would be the function.\n#a_roads <- geojson_sf(\"data/London/Lines/a_roads.geojson\")\n\nplot(a_roads$geometry)\n\n\n\n\n\n\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science DIY session or from other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\n\nosm_q_sf <- opq(\"Greater London, U.K.\") %>% # searching only in Greater London\n    add_osm_feature(key = \"building\", value = \"museum\") %>% #adding osm data that is tagged as a museum\n  osmdata_sf () # transforming to sf object\n\nThe structure of osmdata objects are clear from their default print method, illustrated using the museum example. We will use them shortly.\n\nosm_q_sf  \n\nObject of class 'osmdata' with:\n                 $bbox : 51.2867601,-0.5103751,51.6918741,0.3340155\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 206 points\n            $osm_lines : NULL\n         $osm_polygons : 'sf' Simple Features Collection with 8 polygons\n       $osm_multilines : NULL\n    $osm_multipolygons : 'sf' Simple Features Collection with 1 multipolygons\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “museums in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote: the code cell above requires internet connectivity.\nImportant: Be careful, if you query too much data, your environment is likely to get stuck."
  },
  {
    "objectID": "spatialdata_code.html#inspecting-spatial-data",
    "href": "spatialdata_code.html#inspecting-spatial-data",
    "title": "Lab",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting\nJust like a dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within a spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non spatial dataframes.\nWe can see our data is very similar to a traditional, non-spatial dataFrame, but with an additional column called geometry.\n\nhead(districts) # the command \"head\" reads the first 5 rows of the data\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 515484.9 ymin: 156480.8 xmax: 554503.8 ymax: 198355.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  <chr>     <chr>                                                  <POLYGON [m]>\n1 00AA      City of London       ((531028.5 181611.2, 531036.1 181611.5, 531074…\n2 00AB      Barking and Dagenham ((550817 184196, 550814 184189.1, 550799 18416…\n3 00AC      Barnet               ((526830.3 187535.5, 526830.3 187535.4, 526829…\n4 00AD      Bexley               ((552373.5 174606.9, 552372.9 174603.9, 552371…\n5 00AE      Brent                ((524661.7 184631, 524665.3 184626.4, 524667.9…\n6 00AF      Bromley              ((533852.2 170129, 533850.4 170128.5, 533844.9…\n\n\nWe can inspect the object in different ways :\n\ndistricts[1,] # read first row\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  <chr>     <chr>                                                  <POLYGON [m]>\n1 00AA      City of London ((531028.5 181611.2, 531036.1 181611.5, 531074 18161…\n\ndistricts[,1] # read first column\n\nSimple feature collection with 33 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.1 ymin: 155850.8 xmax: 561957.4 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 33 × 2\n   DIST_CODE                                                            geometry\n   <chr>                                                           <POLYGON [m]>\n 1 00AA      ((531028.5 181611.2, 531036.1 181611.5, 531074 181610.3, 531107 18…\n 2 00AB      ((550817 184196, 550814 184189.1, 550799 184162.6, 550797.2 184159…\n 3 00AC      ((526830.3 187535.5, 526830.3 187535.4, 526829 187534.7, 526825.3 …\n 4 00AD      ((552373.5 174606.9, 552372.9 174603.9, 552371 174595.3, 552367.9 …\n 5 00AE      ((524661.7 184631, 524665.3 184626.4, 524667.9 184623.1, 524673.5 …\n 6 00AF      ((533852.2 170129, 533850.4 170128.5, 533844.9 170127.9, 533842.6 …\n 7 00AG      ((531410.7 181576.1, 531409.4 181573.1, 531409.4 181573.1, 531405 …\n 8 00AH      ((532745.1 157404.6, 532756.3 157394.6, 532768.1 157384, 532777.3 …\n 9 00AJ      ((512740.6 182181.6, 512740.1 182185.9, 512739.5 182190.5, 512739.…\n10 00AK      ((530417 191627.4, 530416.8 191627.6, 530410 191631.2, 530396.3 19…\n# ℹ 23 more rows\n\ndistricts[1,1] #read first row, first column: 00AA\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 2\n  DIST_CODE                                                             geometry\n  <chr>                                                            <POLYGON [m]>\n1 00AA      ((531028.5 181611.2, 531036.1 181611.5, 531074 181610.3, 531107 181…\n\n# variable can be called using the operator $\ndistricts$DIST_NAME #read the column \"DIST_NAME\"\n\n [1] \"City of London\"         \"Barking and Dagenham\"   \"Barnet\"                \n [4] \"Bexley\"                 \"Brent\"                  \"Bromley\"               \n [7] \"Camden\"                 \"Croydon\"                \"Ealing\"                \n[10] \"Enfield\"                \"Greenwich\"              \"Hackney\"               \n[13] \"Hammersmith and Fulham\" \"Haringey\"               \"Harrow\"                \n[16] \"Havering\"               \"Hillingdon\"             \"Hounslow\"              \n[19] \"Islington\"              \"Kensington and Chelsea\" \"Kingston upon Thames\"  \n[22] \"Lambeth\"                \"Lewisham\"               \"Merton\"                \n[25] \"Newham\"                 \"Redbridge\"              \"Richmond upon Thames\"  \n[28] \"Southwark\"              \"Sutton\"                 \"Tower Hamlets\"         \n[31] \"Waltham Forest\"         \"Wandsworth\"             \"Westminster\"           \n\n\nWe can read or create subsets:\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\ndistricts[districts$DIST_NAME== \"City of London\",] \n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  <chr>     <chr>                                                  <POLYGON [m]>\n1 00AA      City of London ((531028.5 181611.2, 531036.1 181611.5, 531074 18161…\n\n\nNote Go back to open science for subsetting with dplyr.\n\n\nQuick visualisation\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# plot london in grey\nplot(districts$geometry, col = \"lightgrey\")\n\n# Add city of London in turquoise to the map\nplot(districts[districts$DIST_NAME == \"Hackney\", ]$geometry, # select city of london\n     col = \"turquoise\",\n     add = T) # add to the existing map\n\n\n\n\nSome guidance on colours in R can be found here.\nHow to reset a plot:\n\nplot(districts$geometry, reset = T) # reset"
  },
  {
    "objectID": "spatialdata_code.html#styling-plots",
    "href": "spatialdata_code.html#styling-plots",
    "title": "Lab",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\nplot(districts$geometry)\nplot(a_roads$geometry, add=T) # note the `add=T` is adding the second layer.\n\n\n\n\nOr use the ggplot package for something a bit fancier\n\nggplot() +\n geom_sf(data = districts, color = \"black\") +  # Plot districts with black outline\n  geom_sf(data = a_roads, color = \"brown\") +  # Plot roads with brown color and 50% transparency\n  theme_minimal() \n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme_minimal()\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme(line = element_blank(), # remove tick marks\n        rect = element_blank(), # remove background\n        axis.text=element_blank()) # remove x and y axis\n\n\n\n  # theme_void() # could also be used instead of the 3 above lines \n\nFor more on themes in ggplot see here\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme_void() + # \n  ggtitle(\"Some London roads\") #add ggtitle\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  \n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) + \n  geom_sf(data = poi_sf, color = \"blue\", size = 3) + # size adjusts size of visualization\n  theme_void() +\n  ggtitle(\"Some London Roads\") #add ggtitle\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\nHere we are using geom_sf_text to add data, specifically the distrct name, to the centre of each District in a specific size.\n\nggplot() +\n  geom_sf(data = districts,\n          fill = \"gray95\") +\n  geom_sf_text(data = districts,\n               aes(label = DIST_NAME),\n               fun.geometry = sf::st_centroid, size=2) +\n  theme_void()\n\n\n\n\ngeom_sf_text() and geom_sf_label() can also be used to achieve similar effects."
  },
  {
    "objectID": "spatialdata_code.html#coordinate-reference-systems",
    "href": "spatialdata_code.html#coordinate-reference-systems",
    "title": "Lab",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\n\nCRSs in R\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\nst_crs(districts) # retrieve coordinate reference system from object\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nThe st_crs function also has one helpful feature - we can retrieve some additional information about the used CRS. For example, try to run:\n\nst_crs(districts)$IsGeographic # to check is the CRS is geographic or not\n\n[1] FALSE\n\nst_crs(districts)$units_gdal # to find out the CRS units\n\n[1] \"metre\"\n\nst_crs(districts)$srid # extracts its SRID (when available)\n\n[1] \"EPSG:27700\"\n\nst_crs(districts)$proj4string # extracts the proj4string representation\n\n[1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the st_set_crs() function can be used:\n\ndistricts_4326 = st_transform(districts, \"EPSG:4326\") # set CRS\n# districts_4326 <- st_transform(districts_4326, crs = 4326)\n\n\n\nFrom coordinates to spatial objects\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a spatial dataframe. For example we have some London housing transactions we want to import and use.\nWe want to transform the .csv in a sf object with the st_as_sf function using the coordinates stored in columns 17 and 18, and then we set the dataframe CRS to the British National Grid (EPSG:27700) using the st_set_crs function.\n\nhousesales <- read.csv(\"data/London/Tables/housesales.csv\") # import housesales data from csv\n\n# 3 commands: \nhousesales_filtered = filter(housesales,price < 500000)\nhousesales_sf <- st_as_sf(housesales_filtered, coords = c(17,18)) # denote columns which have the coordinates\nhousesales_clean <- st_set_crs(housesales_sf, 27700)# set crs to British National Grid \n\nAs we’ve seen in open science, we can do consecutive operations using dplyr pipes %>%, they are used to simplify syntax. Pipes allow to perform successive operations on dataframes in one command! More info here.\n\n# all one in go and one output\nhousesales_clean = housesales %>% # select the main object\n  filter(price < 500000) %>% # remove values above 500,000\n  st_as_sf(coords = c(17,18)) %>% # # denote columns which have the coordinates\n  st_set_crs(27700) # set crs to British National Grid\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\nggplot() + \n geom_sf(data = districts_4326$geometry) + \n  geom_sf(data = poi_sf$geometry, fill = 'blue', size = 3) + \n  coord_sf(xlim = c(-0.180723,-0.014212), ylim = c(51.476668,51.532337)) +\n   theme_void()"
  },
  {
    "objectID": "spatialdata_code.html#manipulating-spatial-tables",
    "href": "spatialdata_code.html#manipulating-spatial-tables",
    "title": "Lab",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a geographical dataframes contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial dataframes (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, geo dataframes also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeo dataframes come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nAreaLengthCentroidsBuffers and selecting by location\n\n\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the dataframe you are working with is projected. If that is the case, you can calculate areas as follows:\nWe had already checked that district was projected to the British National Grid\n\ndistricts <- districts %>%\n  mutate(area = st_area(.)/1000000) # calculate area and make it km2\n\n\n\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\na_roads <- a_roads %>%\n  mutate(street_length = st_length(geometry)) # calculate street length in metres\n\nIf you check the dataframe you will see the lengths.\n\n\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\n# Create a dataframe with centroids\ncentroids_df <- districts %>%\n  st_centroid()\n\nPlot the centroids\n\nggplot() +\n  geom_sf(data = districts) +  # Plot the districts segments\n  geom_sf(data = centroids_df, color = \"red\", size = 2) +  # Plot the centroids in red\n  theme_minimal()\n\n\n\n\n\n\nHere, we first select by expression the Hackney district and then we create a 1km buffer around it with the st_buffer() function from the sf package.\n\n# buffer\ncentroid_buffers <- st_buffer(centroids_df, 1000)\n\nggplot() +\n  geom_sf(data = districts) +  # Plot the districts segments\n  geom_sf(data = centroids_df, color = \"red\", size = 2) +  # Plot the centroids in red\n  geom_sf(data = centroid_buffers, color = \"darkred\", size = 2) +  # Plot the buffers of the centroids\n  theme_minimal()"
  },
  {
    "objectID": "spatialdata_code.html#saving-maps-to-figures",
    "href": "spatialdata_code.html#saving-maps-to-figures",
    "title": "Lab",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\nCreate a file to put your maps:\n\ndir.create(\"maps\") \n\nWarning in dir.create(\"maps\"): 'maps' already exists\n\n\nIf you were creating a map with teh plot function you could save it like this:\n\npdf(\"maps/london_test.pdf\") # Opening the graphical device\nplot(districts$geometry)\nplot(housesales_clean$geometry, add=TRUE) \ndev.off() # Closing the graphical device\n\nquartz_off_screen \n                2 \n\n\nLet’s create a simple map with the variable we just created:\n\ntest_map <- ggplot() \n  geom_sf(data = districts, aes(fill = Level4p)) +\n  theme_void() \n\nNULL\n\n\nLet’s save it, as you can see you can play around with the formatting. For more on ggsave have a look here\n\nggsave(\"maps/map3.pdf\")\n\nSaving 7 x 5 in image\n\nggsave(\"maps/test_map_1.png\", width = 4, height = 4)\nggsave(\"maps/test_map_2.png\", width = 20, height = 20, units = \"cm\")"
  },
  {
    "objectID": "spatialdata_code.html#adding-base-layers-from-web-sources",
    "href": "spatialdata_code.html#adding-base-layers-from-web-sources",
    "title": "Lab",
    "section": "Adding base layers from web sources",
    "text": "Adding base layers from web sources\nAdd in"
  },
  {
    "objectID": "spatialdata_code.html#interactive-maps",
    "href": "spatialdata_code.html#interactive-maps",
    "title": "Lab",
    "section": "Interactive maps",
    "text": "Interactive maps\nEverything we have seen so far relates to static maps. These are useful for publication, to include in reports or to print. However, modern web technologies afford much more flexibility to explore spatial data interactively.\nIn this example, ee will use the package leaflet. This integration connects us with the popular web mapping library Leaflet.js. The key part of the code below is addProviderTiles, We are using CartoDB.Positron but there are many more that you can explore here.\n\nlibrary(leaflet)\npopup = c(\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\")\nleaflet() %>%\n  addProviderTiles(\"CartoDB.Positron\") %>%\n  addMarkers(lng = c(-0.1459604, -0.1272057, -0.1319481, -0.173734),\n             lat = c(51.5045975, 51.5007325, 51.5301701, 51.4938451), \n             popup = popup)"
  },
  {
    "objectID": "spatialdata_code.html#additional-resources",
    "href": "spatialdata_code.html#additional-resources",
    "title": "Lab",
    "section": "Additional resources",
    "text": "Additional resources\nIf you want to have a look at Python."
  },
  {
    "objectID": "intro.html#open-source-gis",
    "href": "intro.html#open-source-gis",
    "title": "1 Introduction",
    "section": "Open Source GIS",
    "text": "Open Source GIS\nOpen source Geographic Information Systems (GIS), such as QGIS, have made geographic analysis accessible worldwide. GIS programs tend to emphasize graphical user interfaces (GUIs), with the unintended consequence of discouraging reproducibility (although many can be used from the command line Python + QGIS). R and Python by contrast, emphasizes the command line interface (CLI).\nThe ‘geodata revolution’ drives demand for high performance computer hardware and efficient, scalable software to handle and extract signal from the noise, to understand and perhaps change the world. Spatial databases enable storage and generation of manageable subsets from the vast geographic data stores, making interfaces for gaining knowledge from them vital tools for the future.\nR and Python are both tools with advanced modeling and visualization capabilities."
  },
  {
    "objectID": "mapraster.html",
    "href": "mapraster.html",
    "title": "4 Mapping Raster Data",
    "section": "",
    "text": "Raster data is more and more popular…"
  },
  {
    "objectID": "mapvector_code.html",
    "href": "mapvector_code.html",
    "title": "Lab",
    "section": "",
    "text": "Loop through the columns and convert to numeric\nfor (col in columns_to_convert) { my_data[[col]] <- as.numeric(my_data[[col]]) }"
  },
  {
    "objectID": "spatialdata_code.html#python",
    "href": "spatialdata_code.html#python",
    "title": "Lab",
    "section": "Python",
    "text": "Python\n:::"
  },
  {
    "objectID": "spatialdata_code.html#joins",
    "href": "spatialdata_code.html#joins",
    "title": "Lab",
    "section": "Joins",
    "text": "Joins"
  },
  {
    "objectID": "spatialdata_code.html#join-districts-with-educational-level-data",
    "href": "spatialdata_code.html#join-districts-with-educational-level-data",
    "title": "Lab",
    "section": "Join districts with educational level data",
    "text": "Join districts with educational level data\n\n# import qualifications data from csv\nqualifications2001_df <- read.csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# take a quick look at the table by reading the first 5 lines\nhead(qualifications2001_df)\n\n  Zone_Code            Zone_Name Population1674 Noquals Level1 Level2 Level3\n1      00AA       City of London           6067     607    359    634    665\n2      00AB Barking and Dagenham         113579   44873  21654  20564   6626\n3      00AC               Barnet         228123   44806  25558  41118  24695\n4      00AD               Bexley         156172   44887  32110  35312  10759\n5      00AE                Brent         198712   48915  23913  33280  21121\n6      00AF              Bromley         212368   47093  34879  48012  19550\n  Level4\n1   3647\n2  11615\n3  80907\n4  20704\n5  60432\n6  49598\n\n\n\nInstall the dplyr package, which is a must have package for data cleaning. More info can be found here. dplyr is a part of the tidyverse!\nJoin merge two datasets join(x, y).\n\nleft_join returns all rows from x (districts), and all columns from x (districts) and y (qualifications2001)\ninner join returns all rows from x where there are matching values in y, and all columns from x and y)\nright join returns all rows from x, and all columns from x and y)\nfull_join returns all rows and all columns from both x and y)\n\nMerge the data from the districts shapefile and the qualifications from the csv file\nJoin districts data to qualifications2001 using district identifiers called DIST_CODE in districts and Zone_Code in qualifications2001_df\n\n\n#join\ndistricts <- left_join(districts, \n                       qualifications2001_df, \n                       by=c(\"DIST_CODE\"=\"Zone_Code\"))\n\n# tidyverse alternative with pipe operator %>%\n\ndistricts_tidy <- districts %>%\n  left_join(qualifications2001_df, by=c(\"DIST_CODE\"=\"Zone_Code\"))\n\n# check the first rows of the merged data table\nhead(districts)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 515484.9 ymin: 156480.8 xmax: 554503.8 ymax: 198355.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 11\n  DIST_CODE DIST_NAME                   geometry   area Zone_Name Population1674\n  <chr>     <chr>                  <POLYGON [m]>  [m^2] <chr>              <int>\n1 00AA      City of L… ((531028.5 181611.2, 531…   3.15 City of …           6067\n2 00AB      Barking a… ((550817 184196, 550814 …  37.8  Barking …         113579\n3 00AC      Barnet     ((526830.3 187535.5, 526…  86.7  Barnet            228123\n4 00AD      Bexley     ((552373.5 174606.9, 552…  64.3  Bexley            156172\n5 00AE      Brent      ((524661.7 184631, 52466…  43.2  Brent             198712\n6 00AF      Bromley    ((533852.2 170129, 53385… 150.   Bromley           212368\n# ℹ 5 more variables: Noquals <int>, Level1 <int>, Level2 <int>, Level3 <int>,\n#   Level4 <int>\n\n\n\nCalculation\nNow, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\ndistricts <- districts %>%\n  mutate(Level4p = Level4/Population1674)"
  },
  {
    "objectID": "spatialdata_code.html#adding-baselayers",
    "href": "spatialdata_code.html#adding-baselayers",
    "title": "Lab",
    "section": "Adding baselayers",
    "text": "Adding baselayers\nVarious R libraries allow us to add static basemaps to out maps. We will be using the base_map() function to(down)load a basemap in our maps. This is from the library(basemapR) which is easy to execute.\nThe style of basemap currently supported are ‘dark’, ‘hydda’, ‘positron’, ‘voyager’, ‘wikimedia’, ‘mapnik’, google, google-nobg, google-hybrid, google-terrain, google-satellite, google-road. The package aims to ease the use of basemaps in different contexts by providing a function interface as minimalist as possible. There are other packages which support more choices like library(basemaps) which you can check out here\nWe simply add base_map () to our ggplot:\n\nggplot() +\n  base_map(st_bbox(districts_4326), increase_zoom = 2) + \n  geom_sf(data = districts_4326, fill = NA)\n\nattribution: &copy; <a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors &copy; <a href=\"https://carto.com/attributions\">CARTO</a>\n\n\n\n\n\nIf we want to specify the map we use basemap =:\n\nggplot() +\n  base_map(st_bbox(districts_4326), basemap = 'google-terrain', increase_zoom = 2) +\n  geom_sf(data = districts_4326, fill = NA) +\n  geom_sf(data = poi_sf) +\n  ggthemes::theme_map()\n\nplease cite: map data © 2020 Google"
  },
  {
    "objectID": "spatialdataDIY.html#data-preparation",
    "href": "spatialdataDIY.html#data-preparation",
    "title": "Do-It-Yourself",
    "section": "Data preparation",
    "text": "Data preparation\n\nPolygons\nFor this section, you will have to push yourself out of the comfort zone when it comes to sourcing the data. As nice as it is to be able to pull a dataset directly from the web at the stroke of a url address, most real-world cases are not that straight forward. Instead, you usually have to download a dataset manually and store it locally on your computer before you can get to work.\nWe are going to use data from the Consumer Data Research Centre (CDRC) about Liverpool, in particular an extract from the Census. You can download a copy of the data at:\n\n\n\n\n\n\nImportant\n\n\n\nYou will need a username and password to download the data. Create it for free at:\nhttps://data.cdrc.ac.uk/user/register\nThen download the Liverpool Census’11 Residential data pack\n\n\nOnce you have the .zip file on your computer, right-click and “Extract all”. The resulting folder will contain all you need. Create a folder called Liverpool in data folder you created in the first Lab.\n\nlibrary(sf)\nlsoas <- read_sf(\"data/Liverpool/Census_Residential_Data_Pack_2011/Local_Authority_Districts/E08000012/shapefiles/E08000012.shp\")\n\n\n\nLines\nFor a line layer, we are going to use a different bit of osmdata functionality that will allow us to extract all the highways. Note the code cell below requires internet connectivity.\n\nhighway <- opq(\"Liverpool, U.K.\") %>%\n   add_osm_feature(key = \"highway\", \n                   value = c(\"primary\", \"secondary\", \"tertiary\")) %>%\n   osmdata_sf()\n\nggplot() + \n  geom_sf(data = highway$osm_lines, color = 'darkorange') + theme_minimal() \n\n\n\n\n\n\nPoints\nFor points, we will find some POI (Points of Interest) : pubs in Liverpool, as recorded by OpenStreetMap. Note the code cell below requires internet connectivity.\n\nbars <- opq(\"Liverpool, U.K.\") %>%\n   add_osm_feature(key = \"amenity\", \n                   value = c(\"bar\")) %>%\n   osmdata_sf()\n\nggplot() + \n  geom_sf(data = bars$osm_points) + theme_minimal()"
  },
  {
    "objectID": "spatialdataDIY.html#points",
    "href": "spatialdataDIY.html#points",
    "title": "Do-It-Yourself",
    "section": "Points",
    "text": "Points\nFor points, we will find some POI (Points of Interest) : pubs in Liverpool, as recorded by OpenStreetMap. Note the code cell below requires internet connectivity.\n\nbars <- opq(\"Liverpool, U.K.\") %>%\n   add_osm_feature(key = \"amenity\", \n                   value = c(\"bar\")) %>%\n   osmdata_sf()\n\nggplot() + \n  geom_sf(data = bars$osm_points) + theme_minimal()"
  },
  {
    "objectID": "spatialdataDIY.html#tasks",
    "href": "spatialdataDIY.html#tasks",
    "title": "Do-It-Yourself",
    "section": "Tasks",
    "text": "Tasks\n\nTask I: Tweak your map\nWith those three layers, try to complete the following tasks:\n\nMake a map of the Liverpool neighborhoods that includes the following characteristics:\n\nFeatures a title\nDoes not include axes frame\nPolygons are all in color #525252 and 50% transparent\nBoundary lines (“edges”) have a width of 0.3 and are of color #B9EBE3\nIncludes a basemap different from the one used in class\n\n\n\n\n\n\n\n\nNote\n\n\n\nNot all of the requirements above are not equally hard to achieve. If you can get some but not all of them, that’s also great! The point is you learn something every time you try.\n\n\n\n\nTask II: Non-spatial manipulations\nFor this one we will combine some of the ideas we learnt in the previous block with this one.\nFocus on the LSOA liverpool layer and use it to do the following:\n\nCalculate the area of each neighbourhood\nFind the five smallest areas in the table. Create a new object (e.g. smallest with them only)\nCreate a multi-layer map of Liverpool where the five smallest areas are coloured in red, and the rest appear in grey.\n\n\n\nTask III: Average price per district\nThis one is a bit more advanced, so don’t despair if you can’t get it on your first try. It relies on the London data you used in the Lab. Here is the questions for you to answer:\nWhat is the district with the highest housing prices in London?\nAnswering this questions involve 3 steps:\n1. Performing a spatial join (st_join) between the district layer (polygons) and the households (points).\n2. Aggregating the data at district level: group_by & summarise()\n3. Figure out the district with the highest price\nReally try not to open the answer below right away.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSpatial overlay between points and polygons\n\nhousesales_districts <- st_join(districts, housesales_clean)\n\nAggregate at district level\n\nhousesales_districts_agg <- housesales_districts %>% \n  group_by(DIST_CODE, DIST_NAME) %>% # group at district level\n  summarise(count_sales = n(),  # create count\n            mean_price = mean(price)) # average price\n\n`summarise()` has grouped output by 'DIST_CODE'. You can override using the\n`.groups` argument.\n\nhead(housesales_districts_agg)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 515484.9 ymin: 156480.8 xmax: 554503.8 ymax: 198355.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 5\n# Groups:   DIST_CODE [6]\n  DIST_CODE DIST_NAME           count_sales mean_price                  geometry\n  <chr>     <chr>                     <int>      <dbl>             <POLYGON [m]>\n1 00AA      City of London                1        NA  ((531028.5 181611.2, 531…\n2 00AB      Barking and Dagenh…          38     91802. ((550817 184196, 550814 …\n3 00AC      Barnet                       83    169662. ((526830.3 187535.5, 526…\n4 00AD      Bexley                       82    119276. ((552373.5 174606.9, 552…\n5 00AE      Brent                        49    174498. ((524661.7 184631, 52466…\n6 00AF      Bromley                     124    142468. ((533852.2 170129, 53385…\n\n\n\n\n\nOnce that’s done, create a map using ggplot and if you’re feeling adventurous the function scale_fill_viridis() to make your map look especially good.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n# map housesales by wards\nmap3 <- ggplot()+\n  geom_sf(data = housesales_districts_agg, inherit.aes = FALSE, aes(fill = mean_price)) + # add the district level housing price \n  scale_fill_viridis(\"Price\", direction = -1, labels = scales::dollar_format(prefix = \"£\"), option = \"magma\" )+ # change the legend scale to £ and the colour to magma\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_minimal() # choose a nicer theme https://ggplot2.tidyverse.org/reference/ggtheme.html\nmap3"
  },
  {
    "objectID": "spatialdata.html#good-old-geo-data",
    "href": "spatialdata.html#good-old-geo-data",
    "title": "2 Spatial Data",
    "section": "“Good old” (geo) data",
    "text": "“Good old” (geo) data\nTo understand what is new in new forms of data, it is useful to begin by considering traditional data. Datasets utilized in the field of social sciences exhibit several key characteristics:\n\nPurposeful Collection: These datasets are meticulously designed and gathered with specific research objectives in mind.\nRich Information: They provide a wealth of detailed and informative data, often offering a comprehensive “rich profile and portraits of the country” under examination.\nHigh Quality: Maintaining a high standard of data accuracy and integrity is a top priority in social science datasets.\n\nHowever, it’s important to note that these datasets also come with certain limitations:\n\nScale and Cost: Building and maintaining such datasets can be massive undertakings, often requiring substantial financial resources.\nCoarse Resolution: To safeguard privacy, data may need to be aggregated, resulting in a loss of fine-grained detail.\nSlowness: The process of data collection, curation, and dissemination can be time-consuming, leading to delays in availability.\nFrequency vs. Detail: Typically, as datasets become more detailed, their availability may decrease, making it challenging to access highly specific data on a regular basis."
  },
  {
    "objectID": "spatialdata.html#new-forms-of-geo-data",
    "href": "spatialdata.html#new-forms-of-geo-data",
    "title": "2 Spatial Data",
    "section": "New forms of (geo) data",
    "text": "New forms of (geo) data\nNew forms of (geo) data are tied into the geo-data revolution. Data is often accidental, which is initially generated for various purposes but becomes available for analysis as a side effect. This data is incredibly diverse, varying in resolution and quality, but holds the potential for much greater detail in both spatial and temporal dimensions.\nHave a look at the two following articles:\n\nData ex Machina: Introduction to Big Data by Lazer & Radford\nAccidental, open and everywhere by Arribas-Bel"
  },
  {
    "objectID": "spatialdata.html#all-maps-are-wrong",
    "href": "spatialdata.html#all-maps-are-wrong",
    "title": "2 Spatial Data",
    "section": "All maps are wrong",
    "text": "All maps are wrong\n\n\nIf you’re still not convinced, try have a mess around with this link.\n\nCoordinates\nWith coordinates, we usually think a numbered measured along a ruler, where the ruler might be an imaginary line: it has an offset (0), a unit (m), and a constant direction. For spatial data we could have two imaginary lines perpendicular to each other, and we call this Cartesian space. Distance between \\((x_1,y_1)\\) and \\((x_2,y_2)\\) in Cartesian space is computed by Euclidean distance: \\[\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\\]\nLeft: geocentric coordinates (Cartesian, three-dimensional, units metres); Right: spherical/ellipsoidal coordinates (angles, units degrees)\n\n\n\n\n\nEuclidean distances do not work for ellipsoidal coordinates: one degree longitude at the equator is about 111 km, at the poles it is 0 km.\n\n\nWhat does coordinate reference system mean?\nCRSs if disregarded can lead to massive problems. CRSs allow you to make the right assumptions without having to guess. They specify what coordinates mean.\n“Data are not just numbers, they are numbers with a context” (Cobb & Moore)\nCoordinate reference systems provide the context of coordinates:\n\nThey tell whether the coordinates are ellipsoidal (angles), or derived, projected (Cartesian) coordinates\nIn case they are projected, they detail the kind of projection used, so that the underlying ellipsoidal coordinates can be recovered\nIn any case, they point out which ellipsoidal model (datum) was used.\n\nKnowing this we can:\n\nConvert between projected and unprojected, or to another projection\nTransform from one datum to another\nCombine the coordinates with any other coordinates that have a coordinate reference system\n\n\n\nProjection and transformation\nEstablished CRSs captured by EPSG codes are well-suited for many applications. A long and growing list of projections has been developed. Here are a few examples applied to the world so you can see that maps do change quite a bit when different projections are applied.\nThe Mollweide projection.\n\n\n\n\n\nCode\nworld_mollweide = st_transform(world, crs = \"+proj=moll\")\nplot(world[\"continent\"])\n\n\n\n\n\nOn the other hand, when mapping the world, it is often desirable to have as little distortion as possible for all spatial properties (area, direction, distance). One of the most popular projections to achieve as little distortion as possible is the Winkel tripel projection\n\n\nCode\nworld_wintri = lwgeom::st_transform_proj(world, crs = \"+proj=wintri\")\nplot(world_wintri[\"continent\"])\n\n\n\n\n\nSpecific PROJ parameters can be modified in most CRS definitions - you will most likely never use this. The below code transforms the coordinates to the Lambert azimuthal equal-area projection centered on longitude and latitude of 0.\n\n\nCode\nworld_laea1 = st_transform(world, \n                           crs = \"+proj=laea +x_0=0 +y_0=0 +lon_0=0 +lat_0=0\")\nplot(world_laea1[\"continent\"])"
  },
  {
    "objectID": "spatialdata.html#geometries",
    "href": "spatialdata.html#geometries",
    "title": "2 Spatial Data",
    "section": "Geometries",
    "text": "Geometries\nThese core spatial geometries are all supported in R package sf.\n\npoints\nlines\npolygons\nand their respective “multi” versions (which group entities of the same type into a single entity).\n\n\nGeometries\nThe basis of every type of geometry is the point. A point is simply a coordinate in 2D, 3D or 4D space such as:\n\n\nPOINT (5 2)\n\n A line string is a sequence of points with a straight line connecting the points, for example:\n\nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\n\nA polygon is a sequence of points that form a closed ring without intersection. Closed means that the first and the last point of a polygon have the same coordinates:\n\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\nA polygone with a hole: POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) \nAll types of objects:\n\n\n\n\n\nIllustration of point, linestring and polygon geometries.\n\n\n\n\n\n\n\nThere are also:\n\nsets of polygons, MULTIPOLYGON(((0 0,1 0,1 1,0 0)), ((3 3,4 3,4 4,3 3)))\ncombinations of these GEOMETRYCOLLECTION(POINT(0 1),LINESTRING(0 0,1 1))- which are inconvenient"
  },
  {
    "objectID": "spatialdata.html#coordinates",
    "href": "spatialdata.html#coordinates",
    "title": "2 Spatial Data",
    "section": "Coordinates",
    "text": "Coordinates\nWith coordinates, we usually think a numbered measured along a ruler, where the ruler might be an imaginary line: it has an offset (0), a unit (m), and a constant direction. For spatial data we could have two imaginary lines perpendicular to each other, and we call this Cartesian space. Distance between \\((x_1,y_1)\\) and \\((x_2,y_2)\\) in Cartesian space is computed by Euclidean distance: \\[\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\\]\nLeft: geocentric coordinates (Cartesian, three-dimensional, units metres); Right: spherical/ellipsoidal coordinates (angles, units degrees)\n\n\n\n\n\nEuclidean distances do not work for ellipsoidal coordinates: one degree longitude at the equator is about 111 km, at the poles it is 0 km."
  },
  {
    "objectID": "spatialdata.html#what-does-coordinate-reference-system-mean",
    "href": "spatialdata.html#what-does-coordinate-reference-system-mean",
    "title": "2 Spatial Data",
    "section": "What does coordinate reference system mean?",
    "text": "What does coordinate reference system mean?\nCRSs if disregarded can lead to massive problems. CRSs allow you to make the right assumptions without having to guess. They specify what coordinates mean.\n“Data are not just numbers, they are numbers with a context” (Cobb & Moore)\nCoordinate reference systems provide the context of coordinates:\n\nthey tell whether the coordinates are ellipsoidal (angles), or derived, projected (Cartesian) coordinates\nin case they are projected, they detail the kind of projection used, so that the underlying ellipsoidal coordinates can be recovered\nin any case, they point out which ellipsoidal model (datum) was used.\n\nKnowing this we can:\n\nconvert between projected and unprojected, or to another projection\ntransform from one datum to another\ncombine the coordinates with any other coordinates that have a coordinate reference system"
  },
  {
    "objectID": "spatialdata.html#lets-look-at-how-crss-are-stored-in-r-spatial-objects-and-how-they-can-be-set",
    "href": "spatialdata.html#lets-look-at-how-crss-are-stored-in-r-spatial-objects-and-how-they-can-be-set",
    "title": "2 Spatial Data",
    "section": "Let’s look at how CRSs are stored in R spatial objects and how they can be set",
    "text": "Let’s look at how CRSs are stored in R spatial objects and how they can be set\nFor this, we need to read-in a vector dataset: https://geocompr.robinlovelace.net/reproj-geo-data.html Our new object, new_vector, is a polygon representing a world map data (?spData::world). CRS in the sf objects is a list of two elements - input and wkt. * The input element is quite flexible, and depending on the input file or user input, can contain SRID representation (e.g., “EPSG:4326”), CRS’s name (e.g., “WGS84”), or even proj4string definition. * The wkt element stores the WKT2 representation, which is used when saving the object to a file or doing any coordinate operations. Above, we can see that the new_vector object has the WGS84 ellipsoid, uses the Greenwich prime meridian, and the latitude and longitude axis order. In this case, we also have some additional elements, such as USAGE explaining the area suitable for the use of this CRS, and ID pointing to the CRS’s SRID - “EPSG:4326”. * Once we know about Coordinate Reference Systems we can start thinking about geometries.\n\nlibrary(sf)\nst_crs('EPSG:4326') #retrieve coordinate reference system from object\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]"
  },
  {
    "objectID": "spatialdata.html#projection-and-transformation",
    "href": "spatialdata.html#projection-and-transformation",
    "title": "2 Spatial Data",
    "section": "Projection and transformation",
    "text": "Projection and transformation\nEstablished CRSs captured by EPSG codes are well-suited for many applications. However in some cases it is desirable to create a new CRS, using a custom proj4string. This system allows a very wide range of projections to be created, as we’ll see in some of the custom map projections in this section.\nA long and growing list of projections has been developed and many of these can be set with the +proj= element of proj4strings.31\nthe Mollweide projection is a good choice. To use this projection, we need to specify it using the proj4string element, “+proj=moll”, in the st_transform function:\n\nlibrary(sf)\nif(!require(\"spData\")) install.packages(\"spData\") # load geographic data\n\nLoading required package: spData\n\nvector_filepath = system.file(\"shapes/world.gpkg\", package = \"spData\")\nworld_mollweide = st_transform(world, crs = \"+proj=moll\")\nplot(world[\"continent\"])\n\n\n\n\nOn the other hand, when mapping the world, it is often desirable to have as little distortion as possible for all spatial properties (area, direction, distance). One of the most popular projections to achieve as little distortion as possible is the Winkel tripel projection st_transform_proj() from the lwgeom package allows for coordinate transformations to a wide range of CRSs.\n\nworld_wintri = lwgeom::st_transform_proj(world, crs = \"+proj=wintri\")\nplot(world_wintri[\"continent\"])\n\n\n\n\nMoreover, PROJ parameters can be modified in most CRS definitions. The below code transforms the coordinates to the Lambert azimuthal equal-area projection centered on longitude and latitude of 0.\n\nworld_laea1 = st_transform(world, \n                           crs = \"+proj=laea +x_0=0 +y_0=0 +lon_0=0 +lat_0=0\")\nplot(world_laea1[\"continent\"])"
  },
  {
    "objectID": "spatialdata.html#geometries-1",
    "href": "spatialdata.html#geometries-1",
    "title": "2 Spatial Data",
    "section": "Geometries",
    "text": "Geometries\nThe basis of every type of geometry is the point. A point is simply a coordinate in 2D, 3D or 4D space such as:\n\n\nPOINT (5 2)\n\n A line string is a sequence of points with a straight line connecting the points, for example:\n\nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\n\nA polygon is a sequence of points that form a closed ring without intersection. Closed means that the first and the last point of a polygon have the same coordinates:\n\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\nA polygone with a hole: POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) \nAll types of objects:\n\n\n\n\n\nIllustration of point, linestring and polygon geometries.\n\n\n\n\n\n\n\nThere are also: * sets of polygons, MULTIPOLYGON(((0 0,1 0,1 1,0 0)), ((3 3,4 3,4 4,3 3))) * combinations of these GEOMETRYCOLLECTION(POINT(0 1),LINESTRING(0 0,1 1))- which are inconvenient"
  },
  {
    "objectID": "spatialdata.html#geometries-2",
    "href": "spatialdata.html#geometries-2",
    "title": "2 Spatial Data",
    "section": "Geometries",
    "text": "Geometries\nThe basis of every type of geometry is the point. A point is simply a coordinate in 2D, 3D or 4D space (see thumbnail(“sf1”) for more information) such as:\n\n\nPOINT (5 2)\n\n A line string is a sequence of points with a straight line connecting the points, for example:\n\nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\n\nA polygon is a sequence of points that form a closed ring without intersection. Closed means that the first and the last point of a polygon have the same coordinates:\n\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\nA polygone with a hole: POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) \nAll types of objects:\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n\n\n\nIllustration of point, linestring and polygon geometries.\n\n\n\n\n\n\n\nThere are also: * sets of polygons, MULTIPOLYGON(((0 0,1 0,1 1,0 0)), ((3 3,4 3,4 4,3 3))) * combinations of these GEOMETRYCOLLECTION(POINT(0 1),LINESTRING(0 0,1 1))- which are inconvenient"
  },
  {
    "objectID": "spatialdata.html#special-about-spatial-data",
    "href": "spatialdata.html#special-about-spatial-data",
    "title": "2 Spatial Data",
    "section": "Special about spatial data",
    "text": "Special about spatial data\nAll data is spatial - data comes from observation, and observation needs to happen somewhere and at some time. This makes all data spatial. For a lot of data, the location expressed in spatial, earth-bound coordinates of observation is not of prime importance:\n\nIf a patient undergoes a brain scan, the location of the scanner is not important; the location of the person relative to the scanner is.\nIf a person receives a positive COVID-19 test result, the location of testing may not be important for the person’s decision on whether to go into quarantine or not\nFor someone trying to do contact tracing, this person’s location history may be very relevant."
  },
  {
    "objectID": "spatialdata.html#further-readings",
    "href": "spatialdata.html#further-readings",
    "title": "2 Spatial Data",
    "section": "Further readings",
    "text": "Further readings\nWatch: Nathan Yau’s Flowing Data\n\nThe Problem with our maps by Nick Routely\nGeocomputation with R\nSpatial Data Science"
  },
  {
    "objectID": "mapvectorDIY.html#data-preparation",
    "href": "mapvectorDIY.html#data-preparation",
    "title": "Do-It-Yourself",
    "section": "Data preparation",
    "text": "Data preparation\n\n\n\n\n\n\nNote\n\n\n\nThe AHAH dataset was invented by a University of Liverpool team. If you want to find out more about the background and details of the project, have a look at the information page at the CDRC website.\n\n\nWe are going to use the Access to Healthy Assets and Hazards (AHAH) index. This is a score that ranks LSOAs (the same polygons we used in the spatial DIY section) by the proximity to features of the environment that are considered positive for health (assets) and negative (hazards). The resulting number gives us a sense of how “unhealthy” the environment of the LSOA is. The higher the score, the less healthy the area is assessed to be.\nTo download the Liverpool AHAH pack, please go over to:\n\n\n\n\n\n\nImportant\n\n\n\nIf you haven’t already, you will need a username and password to download the data. Create it for free at:\nhttps://data.cdrc.ac.uk/user/register\nThen download the Liverpool AHAH GeoData pack.\n\n\nOnce you have the .zip file on your computer, right-click and “Extract all”. The resulting folder will contain all you need. For the sake of the example, let’s assume you place the resulting folder in the same location as the notebook you are using. If that is the case, you can load up the dataframe of Liverpool neighborhoods with:\n\nlibrary(sf)\nlsoas <- read_sf(\"data/Liverpool/Access_to_Healthy_Assets_and_Hazards_AHAH/Local_Authority_Districts/E08000012/shapefiles/E08000012.shp\")\n\nNow, this gets us the geometries of the LSOAs, but not the AHAH data. For that, we need to read in the data and join it to ahah. Assuming the same location of the data as above, we can do as follows:\n\nahah_data <- read.csv(\"data/Liverpool/Access_to_Healthy_Assets_and_Hazards_AHAH/Local_Authority_Districts/E08000012/tables/E08000012.csv\") # import"
  },
  {
    "objectID": "mapvectorDIY.html#tasks",
    "href": "mapvectorDIY.html#tasks",
    "title": "Do-It-Yourself",
    "section": "Tasks",
    "text": "Tasks\n\nTask I: Join by attribute\nConduct a left_join to prepare your final sf object ready for mapping.\n\n\nTask II: AHAH choropleths\nCreate the following choropleths and, where possible, complement them with a figure that displays the distribution of values using a KDE:\n\nEqual Interval with five classes\nQuantiles with five classes\nFisher-Jenks with five classes\nUnique Values with the following setup:\n\nSplit the LSOAs in two classes: above and below the average AHAH score\nAssign a qualitative label (above or below) to each LSOA\nCreate a unique value map for the labels you have just created\n\n\n\n\nTask III: Zoom maps\nGenerate the following maps:\n\nZoom of the city centre of Liverpool with the same color for every LSOA\nQuantile map of AHAH for all of Liverpool, zoomed into north of the city centre\nZoom to north of the city centre with a quantile map of AHAH for the section only"
  },
  {
    "objectID": "mapvector_code.html#choropleths",
    "href": "mapvector_code.html#choropleths",
    "title": "Lab",
    "section": "Choropleths",
    "text": "Choropleths\nIn this session, we will build on all we have learnt so far about loading and manipulating (spatial) data and apply it to one of the most commonly used forms of spatial analysis: choropleths. Remember these are maps that display the spatial distribution of a variable encoded in a color scheme, also called palette. Although there are many ways in which you can convert the values of a variable into a specific color, we will focus in this context only on a handful of them, in particular:\n\nUnique values\nEqual interval\nQuantiles\nFisher-Jenks"
  },
  {
    "objectID": "mapvector_code.html#installing-packages",
    "href": "mapvector_code.html#installing-packages",
    "title": "Lab",
    "section": "Installing Packages",
    "text": "Installing Packages\nBefore all this mapping fun, let us get the importing of libraries and data loading out of the way:\n\n# Load the 'sf' library, which stands for Simple Features, used for working with spatial data.\nlibrary(sf)\n# Load the 'tidyverse' library, a collection of packages for data manipulation and visualization.\nlibrary(tidyverse)\n# Load the 'tmap' library, which is used for creating thematic maps and visualizing spatial data.\nlibrary(tmap)\n# The 'readr' library provides a fast and user-friendly way to read data from common formats like CSV.\nlibrary(readr)\n# Converts Between GeoJSON and simple feature objects\nlibrary(geojsonsf) \n# RColorBrewer library for creating visually appealing color schemes for plots and data visualizations\nlibrary(RColorBrewer)\n# Corking with class intervals and classification methods, esp in the context of spatial data analysis.\nlibrary(classInt)"
  },
  {
    "objectID": "mapvector_code.html#data",
    "href": "mapvector_code.html#data",
    "title": "Lab",
    "section": "Data",
    "text": "Data\nWe will be using World Bank data for this section, looking at World Development Indicators and Education Statistics. We will be focusing on the Middle East and North Africa (MENA). We start by loading the relevant geometries:\n\nmena_sf &lt;- geojson_sf(\"data/MENA/MENA.geojson\") # we load the geojson using `geojson_sf`\n\nplot(mena_sf$geometry) # we plot the geometry to make sure it looks like it should\n\n\n\n\nDon’t forget that before you go further, you want to check the CRS of the sf object as well as the dataframe.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nst_crs(mena_sf)\n\nCoordinate Reference System:\n  User input: 4326 \n  wkt:\nGEOGCS[\"WGS 84\",\n      DATUM[\"WGS_1984\",\n        SPHEROID[\"WGS 84\",6378137,298.257223563,\n          AUTHORITY[\"EPSG\",\"7030\"]],\n        AUTHORITY[\"EPSG\",\"6326\"]],\n      PRIMEM[\"Greenwich\",0,\n        AUTHORITY[\"EPSG\",\"8901\"]],\n      UNIT[\"degree\",0.0174532925199433,\n        AUTHORITY[\"EPSG\",\"9122\"]],\n      AXIS[\"Latitude\",NORTH],\n      AXIS[\"Longitude\",EAST],\n    AUTHORITY[\"EPSG\",\"4326\"]]\n\nhead(mena_sf)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 34.24835 ymin: 22.62095 xmax: 63.31963 ymax: 39.77153\nGeodetic CRS:  WGS 84\n                  name              formal_en_name code_a2 code_a3\n1 United Arab Emirates        United Arab Emirates      AE     ARE\n2              Bahrain          Kingdom of Bahrain      BH     BHR\n3                 Iran    Islamic Republic of Iran      IR     IRN\n4                 Iraq            Republic of Iraq      IQ     IRQ\n5               Israel             State of Israel      IL     ISR\n6               Jordan Hashemite Kingdom of Jordan      JO     JOR\n                        geometry\n1 MULTIPOLYGON (((53.86305 24...\n2 POLYGON ((50.55161 26.19424...\n3 MULTIPOLYGON (((55.05437 25...\n4 POLYGON ((42.89674 37.32491...\n5 POLYGON ((35.80363 33.24846...\n6 POLYGON ((39.04633 32.30849...\n\n\n\n\n\nWe then load the csv with some World Development Indicators data.\n\nworld_dev &lt;- read.csv(\"data/MENA/mena_worlddevelop.csv\") \n\nAnd join the two objects using the relevant codes.\n\nworld_dev_sf &lt;- left_join(mena_sf, \n                       world_dev, \n                       by=c(\"code_a3\"=\"Country.Code\"))\n\nNow we are fully ready to map!\nWe will be using two packages throughout the module. Both tmap (see here), and ggplot2 (, see here). There is also mapsf (for thematic cartography, see here) as another alternative."
  },
  {
    "objectID": "mapvector_code.html#unique-values",
    "href": "mapvector_code.html#unique-values",
    "title": "Lab",
    "section": "Unique values",
    "text": "Unique values\nA choropleth for categorical variables simply assigns a different color to every potential value in the series. Variables could be both nominal or ordinal.\nNominal: Nominal variables represent categories or labels without any inherent order or ranking. The categories are distinct and do not have a natural progression or hierarchy, such as “apple,” “banana,” and “orange” for fruit types.\nOrdinal : Ordinal variables represent categories or labels with a meaningful order or ranking. The relative order or hierarchy among the categories is significant, indicating a clear progression from lower to higher values, such as “low,” “medium,” and “high” for satisfaction levels.\nIn R, creating categorical choropleths is possible with one line of code. To demonstrate this, we can plot the the income level of countries in the MENA region (coded in our table as the income_group variable). The code below uses base R, ggplot and tmap to plot.\n\nplottmapggplot\n\n\nWhen using plot the code for a simple map of categorical values is one line. We call the data world_dev_sf and then the variable of interest income_group.\n\nplot(world_dev_sf[,\"income_group\"])\n\n\n\n\nThe same, perhaps better map, can be made with a little more effort using various R packages.\n\n\ntmap provides another option using the function tm_fill and tm_shape. A line has been added to to start editing the placement of the legend as well with tm_layout.\n\ntm_shape(world_dev_sf) + # data\n  tm_fill(\"income_group\", title = \"Income Groups\")+ # variable and giving a title\n  tm_borders() + # add borders\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"bottom\") # placing the legend\n\n\n\n\n\n\nThis is similar in ggplot. geom_sf is calling the geometric object and fill is defining what values we want to fill the polygons with.\n\nggplot(data = world_dev_sf) +\n    geom_sf(aes(fill = income_group)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThese maps are all all a bit rough a need quite a bit more work. They are just a starting point."
  },
  {
    "objectID": "mapvector_code.html#equal-interval",
    "href": "mapvector_code.html#equal-interval",
    "title": "Lab",
    "section": "Equal Interval",
    "text": "Equal Interval\nIf, instead of categorical variables, we want to display the geographical distribution of a continuous phenomenon, we need to select a way to encode each value into a color. One potential solution is applying what is usually called “equal intervals”. The intuition of this method is to split the range of the distribution, the difference between the minimum and maximum value, into equally large segments and to assign a different color to each of them according to a palette that reflects the fact that values are ordered.\nCreating the choropleth is relatively straightforward in R. For example, to create an equal interval of GDP per capita in 2015 (v_2015).\nFirst we need to prepare the data, going back to our data wrangling.\n\nworld_dev_filtered &lt;- world_dev_sf %&gt;%\n  # Step 1: Filter rows where Series.Name is \"GDP per capita, PPP (current international $)\"\n  filter(Series.Name == \"GDP per capita, PPP (current international $)\") %&gt;% \n  # Step 2: Further filter out rows where 'v_2015' is not missing (i.e., remove NA values)\n  filter(!is.na(v_2015)) %&gt;% \n  # Step 3: Mutate (modify) the 'v_2015' variable by rounding it to a whole number\n  mutate(v_2015 = round(as.numeric(v_2015))) \n\n\n\n\n\n\n\nAdvanced - Looping through different columns\n\n\n\n\n\nYou can also loop through different columns, is for example you wanted to convert GDP per capita for all the years to numeric.\n\n# All the columns to convert\ncolumns_to_convert &lt;- c(\"v_2010\", \"v_2015\", \"v_2020\")\n\n# Loop through the columns and convert to numeric\nfor (col in columns_to_convert) {\n  world_dev_sf[[col]] &lt;- as.numeric(world_dev_sf[[col]])\n}\n\n\n\n\nNow let’s map using equal intervals.\n\ntmapggplot\n\n\nWith a few easy functions tmap allows you to plot your data with different styles. Here we are using equal for equal intervals. Have a look at the code annotation for more detail.\n\ntm_basemap() +\n# Create a basic map using the tm_basemap() function.\n  tm_shape(world_dev_filtered) + \n# Define the data source and shape to be used for the map using tm_shape().\n  tm_polygons(\"v_2015\", palette = \"YlGn\", id = \"name\", n = 7, style= \"equal\") +\n# Add polygons to the map using the tm_polygons() function.\n# 'v_2015bis' is our variable within the 'world_dev_filtered' dataset.\n# The palette \"YlGn\" specifies the color palette for the polygons.\n# 'id' is set to \"name,\" which means the 'name' column will be used to identify polygons.\n# 'n' is set to 7, which means the data will be divided into 7 classes.\n# 'style' is set to \"equal,\" which indicates equal interval classification for the data.\n    tm_layout(\n    legend.outside = TRUE, legend.outside.position = \"bottom\",\n    title = \"GDP per capita by Equal Interval Classification\")\n\n\n\n# Customize the map layout using tm_layout().\n\nOf course, this could do with some further work. You might want to check out tm_layout\n\n\nMapping in ggplot can be a bit tricky at the beginning. You will want to take a look at the package classInt here.\nStep 1: Using the dplyr pipe operator %&gt;%, we’re modifying the world_dev_filtered data frame. We’re using the mutate function to create a new column ‘v_2015bis’ in ‘world_dev_filtered’.\nStep 2: The ‘v_2015’ column in ‘world_dev_filtered’ is being divided by 1000 so we can talk about thousands of $ in GDP per capita, and the result is being rounded to the nearest integer.\n\nworld_dev_filtered &lt;- world_dev_filtered %&gt;%\n  mutate(v_2015bis = round(v_2015 / 1000))\n\nStep 3: Calculate equal interval breaks with function classIntervals and store them.\n\ne_breaks &lt;- classIntervals(world_dev_filtered$v_2015bis, n = 7, style = \"equal\")\n\n# Assign the class breaks to the data\nworld_dev_filtered$e_breaks &lt;- cut(world_dev_filtered$v_2015bis, e_breaks$brks)\n\n\ne_breaks: This is a variable name that you are assigning to store the result of the class intervals calculation.\nclassIntervals(): This is a function that calculates class intervals for a given numeric vector. It is likely provided by a package like ‘classInt’ or ‘classIntervals’ in R.\nworld_dev_filtered$v_2015: This is the data vector that you want to create class intervals for. It appears to be a column named ‘v_2015’ within the ‘world_dev_filtered’ dataset. n = 7: This argument specifies that you want to divide the data into 7 classes. style = \"equal\": This argument specifies that you want to use equal interval classification, which means that the data range will be divided into equal-sized intervals. These class intervals can be useful for creating data visualizations like choropleth maps or histograms.\n\nStep 4: Finally we can map! as you see a bit of extra work was needed, but you ultimately have more control.\n\nnum_bins &lt;-7\ncmap &lt;- brewer.pal(num_bins, \"YlGn\")\n\nggplot() +\n  geom_sf(data = world_dev_filtered, aes(fill = e_breaks)) +\n  scale_fill_manual(\n    values = cmap,\n    name = \"GDP per capita (in 1000s)\",  # Improved legend title\n    labels = gsub(\"[,]\", \"-\", paste0(\"$\", gsub(\"[\\\\[\\\\]()]\", \" \", levels(world_dev_filtered$e_breaks), perl = TRUE)))  # Replace comma with hyphen, add dollar sign, and remove brackets/parentheses from labels\n  ) +\n  labs(\n    title = \"GDP per capita by Equal Interval Classification\",\n    fill = NULL  # Remove the fill label\n  ) +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nIt is important to understand that equal intervals can first and foremost be visualised on the data distribution. We have already created these intervals with the function classIntervals in Step 3 of the ggplot tab above. Here we need a couple of extra steps to collect the break values and plot them in histogram form.\n\n\n\n\n\n\nNote\n\n\n\nThe function classIntervals has the following styles: “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, “jenks”, “dpih”, “headtails”, “maximum”, or “box”.\n\n\n\n# Same step as above\ne_breaks &lt;- classIntervals(world_dev_filtered$v_2015, n = 7, style = \"equal\")\nworld_dev_filtered$e_breaks &lt;- cut(world_dev_filtered$v_2015, e_breaks$brks)\n\n# Collect the values of the breaks\ne_break_values &lt;- e_breaks$brks\n\n# Place the values in a dataframe\ne_break_values_df &lt;- data.frame(BreakValues = e_break_values)\n\n# Create a ggplot2 visualization with 'world_dev_filtered' dataset as the data source\n# and 'v_2015' as the variable for the x-axis.\nggplot(world_dev_filtered, aes(x = v_2015)) +\n# Add a density plot to the visualization with fill color set to dark blue\n# and transparency (alpha) set to 0.4.\n  geom_density(fill = \"darkblue\", alpha = 0.4) +\n# Add a rug plot (small tick marks) along the x-axis with transparency (alpha) set to 0.5.\n  geom_rug(alpha = 0.5) +\n# Add vertical lines to the plot based on the 'e_break_values_df' dataset\n# with x-intercepts specified by the 'BreakValues' variable.\n# The color of these lines is set to green.\n  geom_vline(data = e_break_values_df, aes(xintercept = BreakValues), color = \"green\") +\n# Apply the 'theme_minimal()' theme to the plot for a minimalistic appearance.\n  theme_minimal() +\n# Modify the x-axis label to display \"GDP per capita in 2015\".\n  labs(x = \"GDP per capita in 2015\")\n\n\n\n\nTechnically speaking, the figure is created by overlaying a KDE plot with vertical bars for each of the break points. This makes much more explicit the issue highlighted by which the first two bin contain a large amount of observations while the one with top values only encompasses a handful of them."
  },
  {
    "objectID": "mapvector_code.html#quantiles",
    "href": "mapvector_code.html#quantiles",
    "title": "Lab",
    "section": "Quantiles",
    "text": "Quantiles\nOne solution to obtain a more balanced classification scheme is using quantiles. This, by definition, assigns the same amount of values to each bin: the entire series is laid out in order and break points are assigned in a way that leaves exactly the same amount of observations between each of them. This “observation-based” approach contrasts with the “value-based” method of equal intervals and, although it can obscure the magnitude of extreme values, it can be more informative in cases with skewed distributions.\nThe code required to create the choropleth mirrors that needed above for equal intervals:\n\ntmapggplot\n\n\nAs in our previous example, the coding is a bit simpler if we use tmap.\n\ntm_basemap() +\n  tm_shape(world_dev_filtered) +\n  tm_polygons(\"v_2015\", palette = \"YlGn\", id = \"WARD_NAME\", n = 4, style= \"quantile\")   \n\n\n\n\n\n\nAs before, first we create the intervals, in this case quantiles.\n\n# Find quantile breaks for data segmentation into four groups.\nqt_breaks &lt;- classIntervals(world_dev_filtered$v_2015bis, n = 4, style = \"quantile\")\n\n# Assign the class breaks to the data\nworld_dev_filtered$qt_breaks &lt;- cut(world_dev_filtered$v_2015bis, qt_breaks$brks)\n\nThen we map the data:\n\nnum_bins &lt;-4\n# Define a color palette for visualizing data.\ncmap &lt;- brewer.pal(num_bins, \"YlGn\")\n\n# plot\nggplot() +\n  geom_sf(data = world_dev_filtered, aes(fill = qt_breaks)) +\ntheme_void() + # remove x and y axis\n  scale_fill_manual(\n    values = cmap,\n    name = \"GDP per capita (in 1000s)\",  # Improved legend title\n    labels = gsub(\"[,]\", \"-\", paste0(\"$\", gsub(\"[\\\\[\\\\]()]\", \" \", levels(world_dev_filtered$qt_breaks), perl = TRUE)))) +  # Replace comma with hyphen, add dollar sign, and remove brackets/parentheses from labels \n  labs(\n    title = \"GDP per capita (Quantiles)\",\n    fill = NULL  # Remove the fill label\n  ) +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAs we are dealing with a limited number of countries it is easier to see how the data is being divided differently in the histogram.\n\nqt_breaks &lt;- classIntervals(world_dev_filtered$v_2015, n = 4, style = \"quantile\")\nworld_dev_filtered$qt_breaks &lt;- cut(world_dev_filtered$v_2015, qt_breaks$brks)\n\n# Collect the values of the breaks\nqt_break_values &lt;- qt_breaks$brks\n# Place the values in a dataframe\nqt_break_values_df &lt;- data.frame(BreakValues = qt_break_values)\n\n# Create a ggplot2 visualization\nggplot(world_dev_filtered, aes(x = v_2015)) +\n# Density plot \n  geom_density(fill = \"darkblue\", alpha = 0.4) +\n# Add a rug plot (small tick marks) \n  geom_rug(alpha = 0.5) +\n# Add vertical lines at 'BreakValues' \n  geom_vline(data = qt_break_values_df, aes(xintercept = BreakValues), color = \"green\") +\n  theme_minimal() +\n  labs(x = \"GDP per capita in 2015\")"
  },
  {
    "objectID": "mapvector_code.html#fisher-jenks",
    "href": "mapvector_code.html#fisher-jenks",
    "title": "Lab",
    "section": "Fisher-Jenks",
    "text": "Fisher-Jenks\nEqual interval and quantiles are only two examples of very many classification schemes to encode values into colors. As an example of a more sophisticated one, let us create a Fisher-Jenks choropleth:\n\ntmapggplot\n\n\nAs in our previous example, the coding is a bit simpler if we use tmap.\n\ntm_basemap() +\n  tm_shape(world_dev_filtered) +\n  tm_polygons(\"v_2015\", palette = \"YlGn\", id = \"WARD_NAME\", n = 4, style= \"fisher\")   \n\n\n\n\n\n\nAs before, first we create the intervals, in this case fisher jenks\n\n# Find fisher breaks for data segmentation into 5 groups.\nfish_breaks &lt;- classIntervals(world_dev_filtered$v_2015bis, n = 4, style = \"fisher\")\n\n# Assign the class breaks to the data\nworld_dev_filtered$fish_breaks &lt;- cut(world_dev_filtered$v_2015bis, fish_breaks$brks)\n\nThen we map the data:\n\nnum_bins &lt;-4\n# Define a color palette for visualizing data.\ncmap &lt;- brewer.pal(num_bins, \"YlGn\")\n\n# plot\nggplot() +\n  geom_sf(data = world_dev_filtered, aes(fill = fish_breaks)) +\ntheme_void() + # remove x and y axis\n  scale_fill_manual(\n    values = cmap,\n    name = \"GDP per capita (in 1000s)\",  # Improved legend title\n    labels = gsub(\"[,]\", \"-\", paste0(\"$\", gsub(\"[\\\\[\\\\]()]\", \" \", levels(world_dev_filtered$fish_breaks), perl = TRUE)))) +  # Replace comma with hyphen, add dollar sign, and remove brackets/parentheses from labels \n  labs(\n    title = \"GDP per capita (Quantiles)\",\n    fill = NULL  # Remove the fill label\n  ) +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nNow let’s look at the density plot\n\nfish_breaks &lt;- classIntervals(world_dev_filtered$v_2015, n = 4, style = \"fisher\")\nworld_dev_filtered$fish_breaks &lt;- cut(world_dev_filtered$v_2015, fish_breaks$brks)\n\n# Collect the values of the breaks\nfish_break_values &lt;- fish_breaks$brks\n# Place the values in a dataframe\nfish_break_values_df &lt;- data.frame(BreakValues = fish_break_values)\n\n# Create a ggplot2 visualization\nggplot(world_dev_filtered, aes(x = v_2015)) +\n# Density plot \n  geom_density(fill = \"darkblue\", alpha = 0.4) +\n# Add a rug plot (small tick marks) \n  geom_rug(alpha = 0.5) +\n# Add vertical lines at 'BreakValues' \n  geom_vline(data = fish_break_values_df, aes(xintercept = BreakValues), color = \"green\") +\n  theme_minimal() +\n  labs(x = \"GDP per capita in 2015\")\n\n\n\n\nYou will notice a lot cooler difference once you play around with a larger dataset."
  },
  {
    "objectID": "mapvector_code.html#zooming-into-the-map",
    "href": "mapvector_code.html#zooming-into-the-map",
    "title": "Lab",
    "section": "Zooming into the map",
    "text": "Zooming into the map\nA general map of an entire region, or urban area, can sometimes obscure local patterns because they happen at a much smaller scale that cannot be perceived in the global view. One way to solve this is by providing a focus of a smaller part of the map in a separate figure. Although there are many ways to do this in R, the most straightforward one is to define the bounding box.\nAs an example, let us consider the first ggplot map of this Lab:\n\nZoom into full map\nWe use the function coord_sf to zoom at the desired level.It’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\n\nggplot(data = world_dev_sf) +\n  geom_sf(aes(fill = income_group)) +\n  scale_fill_brewer(palette = \"Set4\") +  # Use ColorBrewer palette\n  theme_void() +\n  coord_sf(xlim = c(30.763478, 40.332570), ylim = c(30.520606, 36.285000)) +\n  labs(fill = \"Income Group\")  # Add a legend title\n\nWarning in pal_name(palette, type): Unknown palette Set4"
  },
  {
    "objectID": "mapvector_code.html#additional-resources",
    "href": "mapvector_code.html#additional-resources",
    "title": "Lab",
    "section": "Additional resources",
    "text": "Additional resources\n\nOn Drawing beautiful maps with sf and ggplot see here\nIf you want to have a look at Choropleths in Python have a look at the chapter on choropleth mapping by Rey, Arribas-Bel and Wolf"
  },
  {
    "objectID": "mapvector.html#geovisualisation",
    "href": "mapvector.html#geovisualisation",
    "title": "3 Mapping Vector Data",
    "section": "Geovisualisation",
    "text": "Geovisualisation\nGeovisualisation is an area that underpins much what we will discuss in this course. Often, we will be presenting the results of more sophisticated analyses as maps. So getting the principles behind mapping right is critical. In this clip, we cover what is (geo)visualisation and why it is important."
  },
  {
    "objectID": "mapvector.html#geographical-containers-for-data",
    "href": "mapvector.html#geographical-containers-for-data",
    "title": "3 Mapping Vector Data",
    "section": "Geographical containers for data",
    "text": "Geographical containers for data\nThis section tries to get you to think about the geographical containers we use to represent data in maps. By that, we mean the areas, delineations and aggregations we, implicitly or explicitly, incur in when mapping data. This is an important aspect, but Geographers have been aware of them for a long time, so we are standing on the shoulders of giants."
  },
  {
    "objectID": "mapvector.html#choropleths",
    "href": "mapvector.html#choropleths",
    "title": "3 Mapping Vector Data",
    "section": "Choropleths",
    "text": "Choropleths\nChoropleths are thematic maps and, these days, are everywhere. From elections, to economic inequality, to the distribution of population density, there’s a choropleth for everyone. Although technically, it is easy to create choropleths, it is even easier to make bad choropleths. Fortunately, there are a few principles that we can follow to create effective choropleths. Get them all delivered right to the conform of your happy place in the following clip and slides!"
  },
  {
    "objectID": "mapvector.html#further-readings",
    "href": "mapvector.html#further-readings",
    "title": "3 Mapping Vector Data",
    "section": "Further readings",
    "text": "Further readings\n\nCynthia Brewer’s “Designing Better Maps” covers several core aspects of building effective geovisualisations.\nChoropleth Mapping chapter by Rey, Arribas-Bel and Wolf"
  },
  {
    "objectID": "mapraster.html#further-readings",
    "href": "mapraster.html#further-readings",
    "title": "4 Mapping Raster Data",
    "section": "Further readings",
    "text": "Further readings\n\nRaster Data in R"
  },
  {
    "objectID": "mapraster_code.html",
    "href": "mapraster_code.html",
    "title": "Lab",
    "section": "",
    "text": "Resources"
  },
  {
    "objectID": "mapraster_code.html#installing-packages",
    "href": "mapraster_code.html#installing-packages",
    "title": "Lab",
    "section": "Installing Packages",
    "text": "Installing Packages\n\n# Provides various utility functions for R programming.\nlibrary(R.utils)\n# For data manipulation and transformation.\nlibrary(dplyr)\n# Spatial data\nlibrary(sf)\n# Popular data visualization package in R.  \nlibrary(ggplot2)\n# For creating thematic maps \nlibrary(tmap)\n# Color palettes suitable for data visualization, especially for those with color vision deficiencies.\nlibrary(viridis)\n# A collection of color palettes for data visualization.\nlibrary(RColorBrewer)\n# For working with raster data, such as gridded spatial data like satellite imagery or elevation data.\nlibrary(raster)\n# An alternative to the 'raster' package and is used for working with large raster datasets efficiently.\nlibrary(terra)\n# Tools for extracting data from raster layers at exact locations, often used in spatial analysis.\nlibrary(exactextractr)\n# Common methods of the tidyverse packages for objects created with the {terra} package: SpatRaster and SpatVector\nlibrary(tidyterra)"
  },
  {
    "objectID": "mapraster_code.html#terrain-data",
    "href": "mapraster_code.html#terrain-data",
    "title": "Lab",
    "section": "Terrain data",
    "text": "Terrain data\n\nImport raster data\nRaster terrain data consists of gridded elevation values that represent the topography of a geographic area. You can download this from the relevant github folder. A good place to download elevation data is Earth Explorer. This video takes you through the download process if you want to try this out yourself.\nWe first import a raster file for elevation.\n\nelevation &lt;- rast(\"data/Lebanon/LBN_elevation_w_bathymetry.tif\")\n\nPlot it.\n\nplot(elevation) \n\n\n\n\nHave a look at the CRS.\n\ncrs(elevation)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\n\n\nImport the Lebanon shapefile\nImport the Lebanon shapefile, plot it, and verify its Coordinate Reference System (CRS). Is it the same as the raster’s CRS?\n\nLebanon_adm1 &lt;- read_sf(\"data/Lebanon/LBN_adm1.shp\")\nplot(Lebanon_adm1$geometry)\n\n\n\ncrs(Lebanon_adm1)\n\n[1] \"PROJCRS[\\\"Deir ez Zor / Syria Lambert\\\",\\n    BASEGEOGCRS[\\\"Deir ez Zor\\\",\\n        DATUM[\\\"Deir ez Zor\\\",\\n            ELLIPSOID[\\\"Clarke 1880 (IGN)\\\",6378249.2,293.466021293627,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4227]],\\n    CONVERSION[\\\"Syria Lambert\\\",\\n        METHOD[\\\"Lambert Conic Conformal (1SP)\\\",\\n            ID[\\\"EPSG\\\",9801]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",34.65,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",37.35,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",0.9996256,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",300000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",300000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting (X)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"northing (Y)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Engineering survey, topographic mapping.\\\"],\\n        AREA[\\\"Lebanon - onshore. Syrian Arab Republic - onshore.\\\"],\\n        BBOX[32.31,35.04,37.3,42.38]],\\n    ID[\\\"EPSG\\\",22770]]\"\n\n\n\n\nReproject the Raster\nAs we are using both the raster and terra packages to handle the raster data it is useful to write terra:: or raster:: in front of the function we are using.\nWe use the terra project() function, we need to define two things:\n\nThe object we want to reproject and\nThe CRS that we want to reproject it to.\n\n\nelevation &lt;- terra::project(elevation, crs(Lebanon_adm1)) # reporjectig the elevation data to the crs of the Lebanon shapefile\ncrs(elevation)\n\n[1] \"PROJCRS[\\\"Deir ez Zor / Syria Lambert\\\",\\n    BASEGEOGCRS[\\\"Deir ez Zor\\\",\\n        DATUM[\\\"Deir ez Zor\\\",\\n            ELLIPSOID[\\\"Clarke 1880 (IGN)\\\",6378249.2,293.466021293627,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4227]],\\n    CONVERSION[\\\"Syria Lambert\\\",\\n        METHOD[\\\"Lambert Conic Conformal (1SP)\\\",\\n            ID[\\\"EPSG\\\",9801]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",34.65,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",37.35,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",0.9996256,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",300000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",300000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting (X)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"northing (Y)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Engineering survey, topographic mapping.\\\"],\\n        AREA[\\\"Lebanon - onshore. Syrian Arab Republic - onshore.\\\"],\\n        BBOX[32.31,35.04,37.3,42.38]],\\n    ID[\\\"EPSG\\\",22770]]\"\n\n\n\n\nCropping and Masking\nCropping and masking are both spatial operations used in raster data analysis.\nCropping:\n\nPurpose: Cropping a raster involves changing the extent of the raster dataset by specifying a new bounding box or geographic area of interest. The result is a new raster that covers only the specified region.\nTypical Use: Cropping is commonly used when you want to reduce the size of a raster dataset to focus on a smaller geographic area of interest while retaining all the original data values within that area.\n\nMasking:\n\nPurpose: Applying a binary mask to the dataset. The mask is typically a separate raster or polygon layer where certain areas are designated as “masked” (1) or “unmasked” (0).\nTypical Use: Masking is used when you want to extract or isolate specific areas or features within a raster dataset. For example, you might use a mask to extract land cover information within the boundaries of a protected national park.\n\nIn many cases, these cropping and masking are executed one after the other because it is computationally easier to crop when dealing with large datasets, and then masking.\n\nelevation_lebanon &lt;- crop(elevation, extent(Lebanon_adm1))\nelevation_lebanon_mask &lt;- mask(elevation_lebanon, Lebanon_adm1)\n\n\n\nPlot elevation\n\nplot(elevation_lebanon_mask)\nplot(Lebanon_adm1$geometry, col= NA, add=T)\n\n\n\n\nLet’s improve this a bit. Remember that there is a lot we can do with ColorBrewer.\n\nplottmap\n\n\n\npal = rev(brewer.pal(6,\"Oranges\"))\nplot(elevation_lebanon_mask, breaks=c(-100,0,700,1200,1800,3300), col=pal)\nplot(Lebanon_adm1$geometry, col= NA, add=T)\n\n\n\n\n\n\n\n# Define the palette\npal &lt;- rev(brewer.pal(6, \"Oranges\"))\n\n# Create the base map\ntm_shape(elevation_lebanon_mask) +\n  tm_raster(breaks = c(-100, 0, 700, 1200, 1800, 3300),\n            palette = pal) +  # Plot the raster with breaks and palette\n  tm_shape(Lebanon_adm1) +\n  tm_borders(lwd = 2) +  # Add borders to the administrative boundaries\ntm_layout(frame = FALSE, legend.outside = TRUE, legend.outside.position = \"right\")  # Remove frame\n\n\n\n\n\n\n\nQuestions to ask yourself about how you can improve these maps, going back to geo-visualisation and choropleths.\n\nWhat are the logical breaks for elevation data?\nShould the colours be changed to standard elevation pallettes?\nHave a look at some of the tmap documentation to improve your map further .\n\n\n\nSlope\nWe are now going to calculate slopes of the terra package.\n\n# import elevation data\nelevation_proj &lt;- rast(\"data/Lebanon/DEM_Leb_projected.tif\")\n\nslope &lt;- terra::terrain(elevation_proj, v=\"slope\", neighbors=8, unit=\"degrees\")\nplot(slope)\n\n\n\n\nAlternatively you can try out the function raster::terrain, details here.\nNow let’s stylise the contours. The package tidyterra provides a great solution by combingn the methods of the tidyverse packages for objects SpatRaster objects. See here for more information.\n\nggplot() +\n  geom_spatraster_contour(data = elevation_proj)\n\n\n\n# Create a new ggplot2 plot object.\nggplot() +\n  # Add a spatial raster contour layer to the plot.\n  geom_spatraster_contour(\n    data = elevation_proj,               # Use the 'elevation' data for this layer.\n    aes(color = after_stat(level)), # Color the contour lines based on 'level'.\n    binwidth = 100,                # Define the binwidth for contouring. Every 100 metres\n    linewidth = 0.4               # Set the line width for contour lines.\n  ) +\n  # Customize the color scale for contour lines.\n  scale_color_gradientn(\n    colours = hcl.colors(20, \"Terrain2\"), # Specify a color palette.\n    guide = guide_coloursteps()           # Use a color step guide for legend.\n  ) +\n  # Apply a minimalistic theme to the plot.\n  theme_minimal()\n\n\n\n\nHave a play around with hcl.color.\n\n\nFlood risk area\nWe can employ reclassification techniques to delineate flood risk areas. Specifically, we identify and classify areas where the elevation is under 10 meters above sea level as high-risk zones. This critical step in flood risk assessment helps communities and decision-makers pinpoint vulnerable areas, enabling them to implement effective mitigation strategies and disaster preparedness plans to safeguard against potential flooding events.\n\nflood_risk &lt;- app(elevation_proj, fun= function(x) ifelse(x&lt;10, 1, 0))\n# This is an anonymous function defined within the 'app' function.\n# It checks each pixel (or cell) value in the 'elevation' raster or matrix.\n# If the pixel value is less than 10 meters, it assigns 1 to 'flood_risk'; otherwise, it assigns 0. This effectively creates a binary flood risk map, where 1 represents areas with a flood risk (elevation &lt; 10m), and 0 represents areas with no flood risk (elevation &gt;= 10m).\nplot(flood_risk)\n\n\n\n\n\n\nSpatial join with vector data\nYou might want to extract values from a raster data set, and then map them eith in a vector sf framework or extract them to analyse them statistically. If it therefore very useful to know hoe to extract:\n\n# Load some geo-localised survey data \nhouseholds &lt;- read_sf(\"data/Lebanon/random_survey_LBN.shp\")\n\n# Using the 'raster::extract' function, it calculates the elevation values at the coordinates of the points. 'elevation' is a raster layer, and 'households' is point data representing household locations.\nhousesales_elevation &lt;- raster::extract(elevation,\n                                households)\n\n# Attach elevation at each point to the original housesales dataframe\nhouseholds &lt;- cbind(households, housesales_elevation)\n\n# Check out the data\nhead(households)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 35.69817 ymin: 33.72841 xmax: 36.17128 ymax: 34.33229\nGeodetic CRS:  WGS 84\n  id ID LBN_elevation_w_bathymetry                  geometry\n1  0  1                         NA POINT (35.90386 33.72841)\n2  1  2                         NA POINT (36.17128 34.20268)\n3  2  3                         NA POINT (35.81425 34.19914)\n4  3  4                         NA POINT (36.03916 34.06442)\n5  4  5                         NA  POINT (35.69817 34.0405)\n6  5  6                         NA POINT (35.98001 34.33229)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure all your data is in the same CRS, otherwise the raster::extract function will not work properly.\nYou should not be seeing NAs, if you do you should use the terra project() function."
  },
  {
    "objectID": "mapraster_code.html#night-lights",
    "href": "mapraster_code.html#night-lights",
    "title": "Lab",
    "section": "Night Lights",
    "text": "Night Lights\nThis section is a bit more advanced, there are hints along the way to make it simpler.\n\nSome useful functions\nTo start this exercise, let’s see some useful functions to manipulate data in R.\n\n# Before we get to clean the data, let's go over a few useful function\n# set the working directory to where you saved the GY476 data\n\n# list files \nlist.files()\n\n# list files in a specific folder\nlist.files(file.path(\"data/Lebanon/Polygons\"))\n\n# list files corresponding to a specific pattern (\"shp\" in the filename)\nlist.files(file.path(\"data/Lebanon/Polygons\"), pattern = \"shp\")\n\n# list files corresponding to a specific pattern (\"shp\" at the end of the filename)\nshps &lt;- list.files(\"data/Lebanon/Polygons\", pattern = \"*.shp\")\n\n# we can also select strings following a pattern inside a list or vector using grepl\nshps &lt;- shps[grepl(\"Lebanon\", shps)] \n\n# let's extract the first element of the list \"shps\"\nfile1 &lt;- shps[1]\nfile1\n\n# how many characters in the filename\nnchar(file1)\n\n# let's remove the last 4 charcters (the file extension)\nfile1_short &lt;- substr(file1, 1, nchar(file1)-4)\n\n# let's add something to the name (concatenate strings) - for example, a new extension \".tif\"\npaste(file1_short, \".tif\", sep=\"\")\n\n# finally let's create a function MathOperations that first calculate the square and then add 3\nMathOperations &lt;- function(x) {\n  sq &lt;- x^2\n  z &lt;- sq+3\n  return(z)\n}\n\n# try the function on 4, 5, 6\nMathOperations(4)\nMathOperations(5)\nMathOperations(6)\n\n# repeat this operation for the vector 4 to 6 (similar to a loop in STATA)\nlapply(4:6, function(x) MathOperations(x))\n\n\n\nDownload data\nWe need to download some raster data. NOAA has made nighttime lights data available for 1992 to 2013. It is called the Version 4 DMSP-OLS Nighttime Lights Time Series. The files are cloud-free composites made using all the available archived DMSP-OLS smooth resolution data for calendar years. In cases where two satellites were collecting data - two composites were produced. The products are 30 arc-second grids, spanning -180 to 180 degrees longitude and -65 to 75 degrees latitude. We can download the Average, Visible, Stable Lights, & Cloud Free Coverages for 1992 and 2013 and put them in the data/Kenya_Tanzania folder.\n\n\n\n\n\n\nImportant\n\n\n\nYou can also download the data here. You will need to be logged into your UoL account.\nThe downloaded files are going to be in a “TAR” format. A TAR file is an archive created by tar, a Unix-based utility used to package files together for backup or distribution purposes. It contains multiple files stored in an uncompressed format along with metadata about the archive. Tars files are also used to reduce files’ size. TAR archives compressed with GNU Zip compression may become GZ, .TAR.GZ, or .TGZ files. We need to decompress them before using them.\n\n\nBefore you move forward download the data for 1992 and 2013. It is also good practice to create a scratch folder where you do all your unzipping.\nIn our example, we will only download two years, but generally, you will have to repeat the same cleaning operations many times. Therefore, to speed up the process, we are going to create a new function. The function is going to:\n\nDecompress the files using the untar command,\nList the decompressed files using list.files command (notice there are compressed files inside the TAR archive)\nIdentify the TIF archive files using grepl\nDecompress using the gunzip command.\n\nWe are then going to run the function on all the TAR files.\n\n\n\n\n\n\nNote\n\n\n\nYou can do these steps manually if you can’t get the below chunk to work.\n\n\n\ndatafolder &lt;- file.path(\"./data\") # define the location of the data folder\n\n# list downloaded files: they are compressed files using the \"tar\" format\ntars &lt;- list.files(file.path(\"data/Kenya_Tanzania/scratch\"), pattern = \"*.tar\")\n\n# unzip\nUnzipSelect &lt;- function(i) {\n  untar(file.path(datafolder,\"Kenya_Tanzania/scratch\",i), exdir = file.path(datafolder, \"Kenya_Tanzania/scratch\")) # unzip\n  all.files &lt;- list.files(file.path(datafolder,\"Kenya_Tanzania/scratch\"), pattern = paste0(substr(i, 6, 12), \"*\")) # list extracted files\n  gz &lt;- all.files[grepl(\"web.stable_lights.avg_vis.tif.gz\", all.files)] # select the TIF files\n  R.utils::gunzip(filename  = file.path(datafolder,\"Kenya_Tanzania/scratch\", gz),\n                  destname = file.path(datafolder,\"Kenya_Tanzania\", substr(gz, 1, nchar(gz) -3)),\n                  overwrite = TRUE) # unzip again\n}\n\n# loop over the TAR files\n# note that the function returns the last element created - in this example, the TIF files\nnl &lt;- lapply(tars, UnzipSelect)\n\n# you can delete the scratch folder with the data we don't need\n# unlink(file.path(datafolder,\"Kenya_Tanzania/scratch\"), recursive = TRUE)\n\nWe can load and plot the nighttime lights data. When working with many rasters of the same origin, it is usually faster to stack them together for faster processing.\n\n# load NL\n# we apply the function raster to each tif file to load the raster in the workspace\nnl_rasters &lt;- lapply(nl, raster)\n# we stack the raster (only possible for rasters of the same extent and definition)\nnl_rasters_stack &lt;- stack(nl_rasters)\n# plot the result\nplot(nl_rasters_stack,  \n     main=c(\"Nightlights 1992\", \"Nightlights 2013\"),\n     axes=FALSE)\n\n\n\n# change the names \nnames(nl_rasters_stack) &lt;- c(\"NL1992\", \"NL2013\")\n\nWhy can’t you see much? Discuss with the person next to you.\n\n\nCountry shapefiles\nThe second step is to download the shapefiles for Kenya and Tanzania. GADM has made available national and subnational shapefiles for the world. The zips you download, such as gadm36_KEN_shp.zip from GADM should be placed in the Kenya_Tanzania folder. This is the link gadm.\n\n# list country shp that we downloaded from the GADM website\nfiles &lt;- list.files(file.path(datafolder,\"Kenya_Tanzania\"), pattern = \"_shp.zip*\", recursive = TRUE, full.names = TRUE)\nfiles\n\n# create a scratch folder\n# dir.create(file.path(datafolder,\"Kenya_Tanzania/scratch\"))\n\n# unzip\nlapply(files, function(x) unzip(x, exdir = file.path(datafolder,\"Kenya_Tanzania/scratch\")))\n\n# GADM has shapefiles for different regional levels (e.g. country, region, district, ward) \ngadm_files &lt;- list.files(file.path(datafolder,\"Kenya_Tanzania\"), pattern = \"gadm*\", recursive = TRUE, full.names = TRUE)\ngadm_files\n\n# let's select regional level 2\ngadm_files_level2 &lt;- gadm_files[grepl(\"2.shp\", gadm_files)]\ngadm_files_level2\n\n# load the shapefiles\nshps &lt;- lapply(gadm_files_level2, read_sf)\nshps\n\n# delete the scratch folder with the data we don't need\n# unlink(file.path(datafolder,\"Kenya_Tanzania/scratch\"), recursive = TRUE)\n\n\n\nZonal statistics\nWe use the package exactextractr to calculate the sum and average nighttime for each region. The nighttime lights rasters are quite large, but as we do not need to do any operations on them (e.g. calculations using the overlay function, cropping or masking to the shapefiles extent), the process should be relatively fast.\nAgain, we use the lapply function to process the two countries successively.\n\n# summarize\nex &lt;- lapply(shps, function(x) exact_extract(nl_rasters_stack, x, c(\"sum\", \"mean\", \"count\"), progress = FALSE))\n# lapply returns a list of two dataframes, we can use \"do.call\" to return each element of the list and iterate the function rbind\n# the results is a dataframe with the merged rows of the dataframes\nex &lt;- do.call(\"rbind\", ex)\n\n# show first files\nhead(ex)\n\n  sum.NL1992 sum.NL2013 mean.NL1992 mean.NL2013 count.NL1992 count.NL2013\n1          0          0  0.00000000   0.0000000     203.3606     203.3606\n2         29        109  0.03748985   0.1409101     773.5427     773.5427\n3          0          0  0.00000000   0.0000000    1910.4520    1910.4520\n4          0          0  0.00000000   0.0000000    2205.3706    2205.3706\n5          0        150  0.00000000   0.1415757    1059.5037    1059.5037\n6          0          0  0.00000000   0.0000000    1353.7744    1353.7744\n\n# summary\nsummary(ex)\n\n   sum.NL1992         sum.NL2013        mean.NL1992        mean.NL2013      \n Min.   :   0.000   Min.   :    0.00   Min.   : 0.00000   Min.   : 0.00000  \n 1st Qu.:   0.000   1st Qu.:   34.61   1st Qu.: 0.00000   1st Qu.: 0.00871  \n Median :   1.322   Median :  211.05   Median : 0.00037   Median : 0.10573  \n Mean   : 204.362   Mean   :  629.85   Mean   : 2.18043   Mean   : 3.67160  \n 3rd Qu.: 158.505   3rd Qu.:  651.81   3rd Qu.: 0.17560   3rd Qu.: 1.07067  \n Max.   :8505.548   Max.   :17562.78   Max.   :60.14768   Max.   :62.27207  \n  count.NL1992      count.NL2013    \n Min.   :    0.0   Min.   :    0.0  \n 1st Qu.:  321.9   1st Qu.:  321.9  \n Median :  952.3   Median :  952.3  \n Mean   : 3706.6   Mean   : 3706.6  \n 3rd Qu.: 4589.1   3rd Qu.: 4589.1  \n Max.   :47327.1   Max.   :47327.1  \n\n\n\n\nMerge shapefiles\nEven though it is not necessary here, we can merge the shapefile to visualize all the regions at once.\nUsually, it is easier to process data in small chunks using a function like “sapply, lapply, mapply” or a loop before merging. For example, when doing zonal statistics, it is faster and easier to process one country at a time and then combine the resulting tables. If you have access to a computer with multiple cores, it is also possible to do “parallel processing” to process each chunk at the same time in parallel.\n\n# merge together\n# we select each sf object and merge the rows\n# do.call() in R to apply a given function to a list as a whole\n# The rbind()  function can be used to bind or combine several vectors, matrices, or data frames by rows\ntza_ken &lt;- do.call(\"rbind\", shps)\n\n# inspect\nstr(tza_ken)\n\nsf [484 × 14] (S3: sf/tbl_df/tbl/data.frame)\n $ GID_0    : chr [1:484] \"KEN\" \"KEN\" \"KEN\" \"KEN\" ...\n $ NAME_0   : chr [1:484] \"Kenya\" \"Kenya\" \"Kenya\" \"Kenya\" ...\n $ GID_1    : chr [1:484] \"KEN.1_1\" \"KEN.1_1\" \"KEN.1_1\" \"KEN.1_1\" ...\n $ NAME_1   : chr [1:484] \"Baringo\" \"Baringo\" \"Baringo\" \"Baringo\" ...\n $ NL_NAME_1: chr [1:484] NA NA NA NA ...\n $ GID_2    : chr [1:484] \"KEN.1.1_1\" \"KEN.1.2_1\" \"KEN.1.3_1\" \"KEN.1.4_1\" ...\n $ NAME_2   : chr [1:484] \"805\" \"Baringo Central\" \"Baringo North\" \"Baringo South\" ...\n $ VARNAME_2: chr [1:484] NA NA NA NA ...\n $ NL_NAME_2: chr [1:484] NA NA NA NA ...\n $ TYPE_2   : chr [1:484] \"Constituency\" \"Constituency\" \"Constituency\" \"Constituency\" ...\n $ ENGTYPE_2: chr [1:484] \"Constituency\" \"Constituency\" \"Constituency\" \"Constituency\" ...\n $ CC_2     : chr [1:484] \"162\" \"159\" \"158\" \"160\" ...\n $ HASC_2   : chr [1:484] NA NA NA NA ...\n $ geometry :sfc_MULTIPOLYGON of length 484; first list element: List of 1\n  ..$ :List of 1\n  .. ..$ : num [1:1017, 1:2] 35.9 35.9 35.9 35.9 35.9 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:13] \"GID_0\" \"NAME_0\" \"GID_1\" \"NAME_1\" ...\n\n# plot\nplot(tza_ken$geometry)\n\n\n\n\n\n\nVisualize\nLet’s have a first look at our result.\n\n# merge back with shapefile attribute table\n# this time instead of merging the rows, we append the columns using cbind\ndf &lt;- cbind(tza_ken, ex)\n\n# Create a map object for \"mean.NL1992\"\nmap1992 &lt;- tm_shape(df) +\n  tm_fill(col = \"mean.NL1992\", palette = \"seq\", title = \"Mean 1992\") +\n  tm_layout(aes.palette = list(seq = \"-YlGnBu\"))\n\n# Create a map object for \"mean.NL2013\"\nmap2013 &lt;- tm_shape(df) +\n  tm_fill(col = \"mean.NL2013\", palette = \"seq\", title = \"Mean 2013\") +\n  tm_layout(aes.palette = list(seq = \"-YlGnBu\"))\n\n# Plot both maps side by side\ntmap_arrange(map1992, map2013)\n\n\n\n\nMost of the Kenya and Tanzania have really low values. To make the maps tell a story, we can use fixed breaks and make the maps interactive using the tmap package:\n\ntmap_mode(\"view\") # switch to other mode: ttm()\n\ntmap mode set to interactive viewing\n\ntm_shape(df) +\n  tm_fill(c(\"mean.NL1992\", \"mean.NL2013\"), title=c(\"Average nightlights\"), style=\"fixed\", palette = \"seq\", breaks=c(0, 0.05, 0.1, 2, 63)) +\n  tm_layout(aes.palette = list(seq = \"-YlGnBu\"), legend.outside = TRUE, legend.outside.position = \"right\") +\n  tm_facets(sync = TRUE, ncol = 2) +\n  tm_borders()\n\n\n\n\n\n\n\n\n\n\n\n\n\nHave a think about what the data is telling you. What’s the story?"
  },
  {
    "objectID": "maprasterDIY.html#data-preparation",
    "href": "maprasterDIY.html#data-preparation",
    "title": "Do-It-Yourself",
    "section": "Data preparation",
    "text": "Data preparation\nWe are going to work with some data of Corsica, a region of France. If you want to challenge yourself you can download the data and pre-process it yourself using Earth Explorer.\nIf not you can download it here. You will need to be signed into your UoL account.\nYou will then need to download the shapefiles for France. You can use GADM which has made available national and subnational shapefiles for the world.\n\nTask I: Join by attribute\n\nPlot your data and the CRS. Make sure your files are in the same CRS. Usually France uses the projected coordinate system Lambert 93 or EPSG = 2154\nCrop and Mask the data\nVisualise the elevation using tmap\nuse geom_spatraster_contour_filled to create a contour map with specific breaks"
  },
  {
    "objectID": "maprasterDIY.html#data-preparation-1",
    "href": "maprasterDIY.html#data-preparation-1",
    "title": "Do-It-Yourself",
    "section": "Data preparation",
    "text": "Data preparation\nNights Lights Google Earth Engine"
  },
  {
    "objectID": "maprasterDIY.html#advanced",
    "href": "maprasterDIY.html#advanced",
    "title": "Do-It-Yourself",
    "section": "Advanced",
    "text": "Advanced\nWorking on a time-series representation of the evolution of Nights Lights in a country of your choice. As in the lab, the data can be found here.\n\nTask II:\n\nProcess the data and make a map with at least two years of night lights data.\nThink about the story the data is telling you.\n\nHow have night lights evolved over time in your country of choice? Remember this will mean different things for different countries.\nWhat choices have you made in terms of bins, colours, zoom level to tell your story?"
  },
  {
    "objectID": "mapraster.html#raster-data",
    "href": "mapraster.html#raster-data",
    "title": "4 Mapping Raster Data",
    "section": "Raster Data",
    "text": "Raster Data\nRaster data is a digital image format that represents data as a grid of individual pixels, with each pixel containing a specific value or color information.\n\nSquare grid of pixels.\nPixel values can represent continuous or categorical variables:\n\nDivides 2-D space into regular cells - pixels\nEach cell has a single value\n\nValues assigned according to value at mean, centre point, or some other rule"
  },
  {
    "objectID": "mapraster.html#data-types",
    "href": "mapraster.html#data-types",
    "title": "4 Mapping Raster Data",
    "section": "Data Types",
    "text": "Data Types\n\nGrayscale Rasters\nGrayscale raster data is a type of digital image representation that uses varying shades of gray to depict the intensity or brightness of a particular phenomenon at different locations. In grayscale images, pixel values typically range from 0 (black) to 255 (white), with intermediate values representing different levels of gray. These pixel values can represent continuous data, making grayscale rasters suitable for visualizing and analyzing phenomena where intensity or variation needs to be conveyed, such as medical X-rays, satellite nightlights data, or other scenarios where the focus is on quantifying the degree of a single attribute without using color.\nExample: Nightlights Data\nNightlights data can be represented as a grayscale raster, where darker areas indicate lower levels of artificial light, and lighter areas represent higher levels of artificial light. The pixel values may represent the radiance or luminance values of nighttime lights. Satellite-based nightlights data is used for various applications, such as monitoring urban development, assessing light pollution, and understanding human activity patterns at night.\n\n\nMultispectral Rasters\nMultispectral rasters are a type of digital image representation that incorporates multiple spectral bands or channels to capture a diverse range of information about a scene or phenomenon. In multispectral rasters, each channel represents a specific part of the electromagnetic spectrum, such as visible light, near-infrared, or thermal infrared. By combining these spectral bands, multispectral data provides a comprehensive view of the landscape, enabling analysis of various aspects like vegetation health, land use, or changes in urban and rural conditions. This type of raster data is commonly used in applications such as remote sensing, agriculture, and natural resource management, where a nuanced understanding of different attributes of the environment is essential.\nExample: Landsat Satellite Imagery\nLandsat satellite imagery, with its multispectral bands, is extensively used for environmental monitoring. It allows for tracking changes in land cover, assessing vegetation health, monitoring water quality, and detecting forest fires. Researchers and environmental agencies use multispectral data to make informed decisions regarding conservation and resource management.\n\n\nColor Rasters\nColour rasters are a digital image representation that utilizes the combination of three primary colour channels: red, green, and blue (RGB) to create a full range of colors. Each pixel in a colour raster is assigned values for these three color channels, which determine the pixel’s color.\nExample: Digital Photographs\nColuor rasters, as seen in digital photographs can be used for social sciences by analysing pixel differences.\n\n\nElevation Rasters\nElevation rasters are a type of digital representation that use a grid of values to depict the varying heights or elevations of the Earth’s surface across different locations. Each pixel in an elevation raster contains a numerical value representing the height or elevation above a reference point, such as sea level. Elevation rasters are commonly used in geospatial applications, cartography, and terrain modeling. They enable precise visualization and analysis of topographic features, such as mountains, valleys, and slopes, making them essential for tasks like mapmaking, land-use planning, flood risk assessment, and 3D visualization of landscapes.\nExample: Digital Elevation Models (DEMs)\nDEMs are vital for creating topographic maps, analyzing terrain, and conducting spatial modeling. They find applications in urban planning, flood risk assessment, route planning, and 3D visualization. DEMs enable precise representation of Earth’s surface elevation, supporting a wide range of geospatial applications."
  },
  {
    "objectID": "mapraster.html#satellite-data-for-social-science",
    "href": "mapraster.html#satellite-data-for-social-science",
    "title": "4 Mapping Raster Data",
    "section": "Satellite data for Social Science",
    "text": "Satellite data for Social Science\nHave a look at\n\nJean, Neal, et al. 2016 “Combining satellite imagery and machine learning to predict poverty.” Science\nHenderson, J. Vernon, Adam Storeygard, and David N. Weil. 2012. Measuring Economic Growth from Outer Space American Economic Review, 102 (2): 994-1028. Replication data"
  },
  {
    "objectID": "spatialw_code.html#data",
    "href": "spatialw_code.html#data",
    "title": "Lab **Under construction**",
    "section": "Data",
    "text": "Data\nFor this session, we will use a dataset of small areas (or Lower layer Super Output Areas, LSOAs) for Liverpool, UK. The table is available as part of this course, so can be accessed remotely through the web. If you want to see how the table was created, a notebook is available here.\nTo make things easier, we will read data from a file posted online so, for now, you do not need to download any dataset:\n\n# Read the file in\ndf = read_sf(\"./data/Liverpool/liv_lsoas.gpkg\")\n\n\n# Display first few lines\nhead(df)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 334715 ymin: 385417 xmax: 339020.8 ymax: 390548\nProjected CRS: Transverse_Mercator\n# A tibble: 6 × 3\n  LSOA11CD  MSOA11CD                                                        geom\n  &lt;chr&gt;     &lt;chr&gt;                                             &lt;MULTIPOLYGON [m]&gt;\n1 E01006512 E02001377 (((336103.4 389628.6, 336103.4 389629.2, 336103.4 389629.…\n2 E01006513 E02006932 (((335173.8 389691.5, 335169.8 389693.8, 335160.4 389699.…\n3 E01006514 E02001383 (((335495.7 389697.3, 335495.4 389699.3, 335486.8 389699.…\n4 E01006515 E02001383 (((334953 389029, 334951 389035, 334950 389040, 334949 38…\n5 E01006518 E02001390 (((335354 388601.9, 335354 388602, 335347 388600, 335335.…\n6 E01006519 E02001402 (((338007.9 385540.8, 338000 385547, 337997 385549.3, 337…"
  },
  {
    "objectID": "spatialw_code.html#building-spatial-weights",
    "href": "spatialw_code.html#building-spatial-weights",
    "title": "Lab **Under construction**",
    "section": "Building spatial weights",
    "text": "Building spatial weights\n\nContiguity\nContiguity weights matrices define spatial connections through the existence of common boundaries. This makes it directly suitable to use with polygons: if two polygons share boundaries to some degree, they will be labeled as neighbors under these kinds of weights. We will learn two approaches, namely queen and rook, characterised by how much they need to share.\n\nQueen\n\nUnder the queen criterion, two observations only need to share a vertex (a single point) of their boundaries to be considered neighbors. Constructing a weights matrix under these principles can be done by running:\n\n# list all adjacent polygons for each polygon\nnb_q &lt;- poly2nb(df, queen = TRUE) # Construct neighbours list from polygon list\n\nand then:\n\nw_queen &lt;- nb2listw(nb_q, style = \"B\") # Create a spatial weights matrix using queen contiguity\n\nIn reality, w_queen is not stored as a matrix containing values for each pair of observations, including zeros for those that are not neighbours. Instead, to save memory, for each observation it stores a list of those other observations that are neighbours according to the queen criterion, and it does not store any values for those observations that are not neighbours. To access summary information about the “spatial weights matrix”, we run the following code:\n\nsummary(w_queen) # Display summary information about the spatial weights matrix\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 298 \nNumber of nonzero links: 1674 \nPercentage nonzero weights: 1.88505 \nAverage number of links: 5.61745 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 \n 1  4 17 49 82 60 49 24  8  3  1 \n1 least connected region:\n185 with 1 link\n1 most connected region:\n109 with 11 links\n\nWeights style: B \nWeights constants summary:\n    n    nn   S0   S1    S2\nB 298 88804 1674 3348 40680\n\n\nNote that when we created w_queen using the nb2list2 function, we set style to \"B\". This means that the weights are recorded as a binary variable taking the value 1 to mark the presence of a link between an observation and a neighbouring one.\nTo see what the ID of the neighbouring polygons for the first polygon in df, we can run the following:\n\nnb_q[[1]] # Access the neighbors of the first polygon in the list\n\n[1]   2 149 155 221 287 298\n\n\nWe can check if polygon 149 is a neighbour of polygon 1:\n\n149 %in% nb_q[[1]] # Check if district 149 is a neighbor of the first polygon in the list\n\n[1] TRUE\n\n\nYes it is, as it was obvious from the output of nb_q[[1]], which includes 149 as one of the neighbours of 1. We can also check if polygon 150 is a neighbour of polygon 1. This should not be the case.\n\n150 %in% nb_q[[1]] # Check if district 150 is a neighbor of the first polygon in the list\n\n[1] FALSE\n\n\nWhat are the weights assigned to the neighbours of polygon 1?\n\nw_queen$neighbours[[1]] # Display the neighbors of the first polygon in the spatial weights matrix\n\n[1]   2 149 155 221 287 298\n\nw_queen$weights[[1]]    # Display the corresponding weights for the neighbors of the first polygon\n\n[1] 1 1 1 1 1 1\n\n\nThe weights are set to 1 for each of the neighbours of 1. This is because we set style to \"B\" when we created w_queen. More options are available as we will see later.\nHow many neighbours does observation 1 have? This can be easily checked using the length() function:\n\nlength(w_queen$neighbours[[1]]) # Calculate the number of neighbors for the first polygon in the spatial weights matrix\n\n[1] 6\n\n\nBut if we wanted to have a more comprehensive understanding of the number of neighbours for all the observations in our dataset, we need to obtain a histogram. This is achieved by running:\n\n# Get the number of neighbors for each element\nnum_nb_q &lt;- sapply(nb_q, function(x) length(x))\n\n# Create a dataframe with LSOA11CD and num_neighbors\nnb_counts_q &lt;- data.frame(LSOA11CD = df$LSOA11CD, num_nb_q = num_nb_q)\n\n# Create a histogram of the number of queen neighbors\nhist(nb_counts_q$num_nb_q, breaks = 10, col = \"blue\", main = \"Histogram of no. of queen neighbours\", xlab = \"No. of queen neighbours\")\n\n\n\n\nLooking at the histogram, we conclude that the mode is 4 queen neighbours. We can obtain some additional summary statistics:\n\n# Calculate the mean number of queen neighbours\nmean(nb_counts_q$num_nb_q) \n\n[1] 5.61745\n\n# Find the maximum number of queen neighbours\nmax(nb_counts_q$num_nb_q)  \n\n[1] 11\n\n# Find the minimum number of queen neighbours\nmin(nb_counts_q$num_nb_q) \n\n[1] 1\n\n\nAre there any isolated nodes, or in other words, are there any polygons that have zero queen neighbours?\n\n# Check if there are elements with zero queen neighbours\n0 %in% nb_counts_q$num_nb_q \n\n[1] FALSE\n\n\nThe answer is no!\nLet’s visualise the queen neighbourhood of the first observation. To do this, we first create sub data frames including polygon 1 and the queen neighbourhood of polygon 1:\n\n # Extract the first row of the dataframe as 'obs1'\nobs1 &lt;- df[1,]\n\n# Extract the rows corresponding to the neighbors of the first polygon using queen contiguity\nobs1_nb_q &lt;- df[c(nb_q[[1]]),]\n\nWe then create a map this using different colors with the tmap package, which inherits lots of functionalities from ggplot2. We store the plot in the final_map_q variable, but we will not plot it just yet. We will plot it side to side with other maps representing other types of neighbourhoods so we can compare these:\n\n# Create a map for all the units in mistyrose3 \nrest_map &lt;- tm_shape(df) +  \n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"mistyrose3\")  \n\n# Create a map for neighbors in steelblue4\nneighbors_map &lt;- tm_shape(obs1_nb_q) +\n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"steelblue4\")  \n\n# Create a map for observation 1 in red2\nobs1_map &lt;- tm_shape(obs1) +\n  tm_borders(col = \"black\", lwd = 0.5) + \n  tm_fill(col = \"red2\")  \n\n# Combine all the maps, add compass, scale bar, and legend\nfinal_map_q &lt;- rest_map + neighbors_map + obs1_map +\n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_scale_bar(position = c(\"right\", \"bottom\")) + \n  tm_add_legend(type = \"fill\", col = c(\"red2\", \"steelblue4\",\"mistyrose3\"), \n                labels = c(\"Observation 1\", \"Queen neighbourhood\", \"Rest of LSOAs\"), title = \"\") + \n  tm_layout(legend.text.size = 0.55, inner.margins = c(0.01, 0.1, 0.01, 0.05), \n            legend.position = c(0.03,0.03), legend.width=0.55)\n\n\nRook\n\nRook contiguity is similar to and, in many ways, superseded by queen contiguity. However, since it sometimes comes up in the literature, it is useful to know about it. The main idea is the same: two observations are neighbors if they share some of their boundary lines. However, in the rook case, it is not enough with sharing only one point, it needs to be at least a segment of their boundary. In most applied cases, these differences usually boil down to how the geocoding was done, but in some cases, such as when we use raster data or grids, this approach can differ more substantively and it thus makes more sense.\nWe create the list of neighbours using poly2nb() again, but this time setting the queen argument to FALSE.\n\nnb_r &lt;- poly2nb(df, queen = FALSE) # Construct neighbors list using rook contiguity\n\nFrom the list of neighbours for each polygon, we can create a rook spatial weights matrix, setting the weights to 1’s to mark the presence of a connection between two polygons:\n\n# Create a spatial weights matrix using rook contiguity\nw_rook &lt;- nb2listw(nb_r, style = \"B\") \n# Display summary information about the spatial weights matrix\nsummary(w_rook) \n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 298 \nNumber of nonzero links: 1642 \nPercentage nonzero weights: 1.849016 \nAverage number of links: 5.510067 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 \n 1  4 18 52 87 62 44 18  9  2  1 \n1 least connected region:\n185 with 1 link\n1 most connected region:\n109 with 11 links\n\nWeights style: B \nWeights constants summary:\n    n    nn   S0   S1    S2\nB 298 88804 1642 3284 39104\n\n\n\n\nDistance\nDistance-based matrices assign the weight to each pair of observations as a function of how far from each other they are. How this is translated into an actual weight varies across types and variants, but they all share that the ultimate reason why two observations are assigned some weight is due to the distance between them.\n\n\\(K\\)-nearest neighbours\n\nOne approach to define weights is to take the distances between a given observation and the rest of the set, rank them, and consider as neighbors the \\(k\\) closest ones. That is exactly what the \\(k\\)-nearest neighbors (KNN) criterion does.\nTo calculate the 5 nearest neighbours for each polygon, we can use the knearneigh() function, setting k=5. Note that this function only takes point geometries, so instead of passing df directly, we compute the coordinates of the centroids for each polygon using st_centroid() and st_coordinates():\n\n# Create k-Nearest Neighbors list with k=5\nnb_knn &lt;- knearneigh(st_coordinates(st_centroid(df)), k=5) \n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nAnd once again, from the list of neighbours, we can create the “spatial weights matrix”, this time using the knn2nb() function:\n\n# Convert k-Nearest Neighbors list to a spatial weights matrix\nw_knn &lt;- knn2nb(nb_knn)\n\nLike before, we create a map that will help us visualise the neighbourhood of polygon 1 according to the 5-nearest neighbours criterion:\n\n# Extract the first row of the dataframe as 'obs1'\nobs1 &lt;- df[1,]\n\n# Extract the rows corresponding to the k-Nearest Neighbors of the first centroid\nobs1_nb_knn &lt;- df[c(w_knn[[1]]),]\n\n\n# Create a map for all the units in mistyrose3 \nrest_map &lt;- tm_shape(df) +  \n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"mistyrose3\")  \n\n# Create a map for neighbors in steelblue4\nneighbors_map &lt;- tm_shape(obs1_nb_knn) +\n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"steelblue4\")  \n\n# Create a map for observation 1 in red2\nobs1_map &lt;- tm_shape(obs1) +\n  tm_borders(col = \"black\", lwd = 0.5) + \n  tm_fill(col = \"red2\")  \n\n# Combine all the maps, add compass, scale bar, and legend\nfinal_map_knn &lt;- rest_map + neighbors_map + obs1_map +\n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_scale_bar(position = c(\"right\", \"bottom\")) + \n  tm_add_legend(type = \"fill\", col = c(\"red2\", \"steelblue4\",\"mistyrose3\"), \n                labels = c(\"Observation 1\", \"k=5 nearest neighbours\", \"Rest of LSOAs\"), title = \"\") + \n  tm_layout(legend.text.size = 0.55, inner.margins = c(0.01, 0.1, 0.01, 0.05), \n            legend.position = c(0.03,0.03), legend.width=0.55)\n\n\nDistance band\n\nAnother approach to build distance-based spatial weights matrices is to “draw a circle” of certain radius (in case your observation is a polygon, as is our case, the circle should be centered at the centroid of each polygon) and consider as neighbours every observation (whose centroid) falls within the circle. The technique has two main variations: binary and continuous. In the former one, every neighbor is given a weight of one, while in the second one, the weights can be further tweaked by the distance to the observation of interest. We start looking at the binary variation.\nFirst, we create a list of neighbours at a distance less than 2000 meters away from each polygon:\n\n# Create a distance-based neighbors list with a minimum distance of 0 and maximum distance of 2000 meters\nnb_d &lt;- dnearneigh(st_coordinates(st_centroid(df)), d1=0, d2=2000)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nWe use this to obtain a spatial weights matrix according to the distance-based criterion\n\n# Create a spatial weights matrix using distance-based neighbors with binary style\nw_d &lt;- nb2listw(nb_d, style = \"B\")\n\nOnce again, we create a map to visualise this neighbourhood:\n\nobs1 &lt;- df[1,]\nobs1_nb_d &lt;- df[c(nb_d[[1]]),]\n\n\n# Create a map for all the units in mistyrose3 \nrest_map &lt;- tm_shape(df) +  \n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"mistyrose3\")  \n\n# Create a map for neighbors in steelblue4\nneighbors_map &lt;- tm_shape(obs1_nb_d) +\n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"steelblue4\")  \n\n# Create a map for observation 1 in red2\nobs1_map &lt;- tm_shape(obs1) +\n  tm_borders(col = \"black\", lwd = 0.5) + \n  tm_fill(col = \"red2\")  \n\n# Combine all the maps, add compass, scale bar, and legend\nfinal_map_d &lt;- rest_map + neighbors_map + obs1_map +\n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_scale_bar(position = c(\"right\", \"bottom\")) + \n  tm_add_legend(type = \"fill\", col = c(\"red2\", \"steelblue4\",\"mistyrose3\"), \n                labels = c(\"Observation 1\", \"Distance neighbourhood\", \"Rest of LSOAs\"), title = \"\") + \n  tm_layout(legend.text.size = 0.55, inner.margins = c(0.01, 0.1, 0.01, 0.05), \n            legend.position = c(0.03,0.03), legend.width=0.55)\n\nWe are now ready to plot the three maps we created side to side and compare the different neighbourhoods:\n\ntmap_arrange(final_map_q, final_map_knn, final_map_d)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDoes the look of neighbourhoods plotted above match your expectations?\n\n\n\nInverse distance weights\n\nAn extension of the weights above is to introduce further detail by assigning different weights to different neighbours within the radius based on how far they are from the observation of interest. For example, we could think of assigning the inverse of the distance between the centroid of a polygon \\(i\\) and the centroid of a neighouburing polygon \\(j\\) as \\(w_{ij} = \\dfrac{1}{d_{ij}}\\), where \\(d_{ij}\\) is the distance in meters between the centroids. This way, polygons that are closer to \\(i\\) “weight” more.\nThis can be computed by firstly, computing the list of neighbours under a distance of 2,000 meters as we did before:\n\n# Create an inverse distance-based neighbors list with a minimum distance of 0 and maximum distance of 2000\nnb_d_inverse &lt;- dnearneigh(st_coordinates(st_centroid(df)), d1=0, d2=2000)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nThen, we can obtain the distance between the centroid of each polygon and its neighbours. Note that we set longlat to FALSE since the coordinate reference system (CRS) for the geometry field in df is not expressed as longitude-latitude decimal degrees:\n\n# Calculate distances between neighbors using the inverse distance-based neighbors list\ndist &lt;- nbdists(nb_d_inverse, st_coordinates(st_centroid(df)), longlat = FALSE)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# Create a list of weights by taking the reciprocal of distances\nw_inverse &lt;- lapply(dist, function(x) 1/(x))\n\nLet’s inspect the weights given to the neighbours of the first polygon in df:\n\nw_inverse[[1]]\n\n [1] 0.0014293845 0.0014775670 0.0008560902 0.0008248890 0.0006169082\n [6] 0.0007411043 0.0007861690 0.0005497876 0.0005350619 0.0005334485\n[11] 0.0005077662 0.0012913246 0.0009829388 0.0006646896 0.0008091102\n[16] 0.0008041621 0.0005807921 0.0015580848 0.0005792682 0.0008094105\n[21] 0.0005404384 0.0006692724 0.0005636064 0.0007081589 0.0006001997\n[26] 0.0005066073 0.0005804556 0.0005049042 0.0007100073 0.0010060556\n[31] 0.0010939774 0.0021358621 0.0010380355 0.0005412010 0.0006706428\n[36] 0.0005877428 0.0041059582 0.0007474042 0.0006616669 0.0008186524\n[41] 0.0005138538 0.0010182534 0.0006099168 0.0023982797\n\n\nNote that none of them should be smaller than 1/2,000 = 0.0005 since all the neighbours are within a 2,000 meter radius from the centroid of the first polygon.\nFollowing this logic of more detailed weights through distance, there is a temptation to take it further and consider everyone else in the dataset as a neighbor whose weight will then get modulated by the distance effect shown above. However, although conceptually correct, this approach is not always the most computationally or practical one. Because of the nature of spatial weights matrices, particularly because of the fact their size is \\(N\\) by \\(N\\) if all the neighbours are present, they can grow substantially large. A way to cope with this problem is by making sure they remain fairly sparse (with many zeros). Sparsity is typically ensured in the case of contiguity or KNN by construction but, with inverse distance, it needs to be imposed as, otherwise, the matrix could be potentially entirely dense (no zero values other than the diagonal). In practical terms, what is usually done is to impose a distance threshold beyond which no weight is assigned and interaction is assumed to be non-existent, as we did here by setting this threshold to 2,000 meters. Beyond being computationally feasible and scalable, results from this approach usually do not differ much from a fully “dense” one as the additional information that is included from further observations is almost ignored due to the small weight they receive.\n\n\nBlock weights\nBlock weights connect every observation in a dataset that belongs to the same category in a list provided ex-ante. Usually, this list will have some relation to geography an the location of the observations but, technically speaking, all one needs to create block weights is a list of memberships. In this class of weights, neighboring observations, those in the same group, are assigned a weight of one, and the rest receive a weight of zero.\nIn this example, we will build a spatial weights matrix that connects every LSOA with all the other ones in the same MSOA. See how the MSOA code is expressed for every LSOA:\n\nhead(df)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 334715 ymin: 385417 xmax: 339020.8 ymax: 390548\nProjected CRS: Transverse_Mercator\n# A tibble: 6 × 3\n  LSOA11CD  MSOA11CD                                                        geom\n  &lt;chr&gt;     &lt;chr&gt;                                             &lt;MULTIPOLYGON [m]&gt;\n1 E01006512 E02001377 (((336103.4 389628.6, 336103.4 389629.2, 336103.4 389629.…\n2 E01006513 E02006932 (((335173.8 389691.5, 335169.8 389693.8, 335160.4 389699.…\n3 E01006514 E02001383 (((335495.7 389697.3, 335495.4 389699.3, 335486.8 389699.…\n4 E01006515 E02001383 (((334953 389029, 334951 389035, 334950 389040, 334949 38…\n5 E01006518 E02001390 (((335354 388601.9, 335354 388602, 335347 388600, 335335.…\n6 E01006519 E02001402 (((338007.9 385540.8, 338000 385547, 337997 385549.3, 337…\n\n\nTo build a block spatial weights matrix that connects as neighbors all the LSOAs in the same MSOA, we only require the MSOA codes. Using nb2blocknb(), this is very straighforward!\n\n# Create a block weights matrix using MSOA11CD as block IDs and LSOA11CD as unit IDs\nw_block &lt;- nb2blocknb(nb=NULL, df$MSOA11CD, row.names = df$LSOA11CD)\n\nWe can visualise this by creating a map, using the same procedure as before:\n\n# Extract the first row of the dataframe as 'obs1'\nobs1 &lt;- df[1,]\n\n# Extract the rows corresponding to the block neighbors of the first observation\nobs1_nb_block &lt;- df[c(w_block[[1]]),]\n\n\n# Create a map for the rest of the units in mistyrose3\nrest_map &lt;- tm_shape(df) +  \n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"mistyrose3\") \n\n# Create a map for block neighbors in steelblue4\nneighbors_map &lt;- tm_shape(obs1_nb_block) +\n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"steelblue4\")\n\n# Create a map for observation 1 in red2\nobs1_map &lt;- tm_shape(obs1) +\n  tm_borders(col = \"black\", lwd = 0.5) +  \n  tm_fill(col = \"red2\")  \n\n# Combine all the maps, add compass, scale bar, and legend\nfinal_map_block &lt;- rest_map + neighbors_map + obs1_map +\n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_scale_bar(position = c(\"right\", \"bottom\")) + \n  tm_add_legend(type = \"fill\", col = c(\"red2\", \"steelblue4\",\"mistyrose3\"), \n                labels = c(\"Observation 1\", \"Block neighbourhood (same MSOA)\", \"Rest of LSOAs\"), title = \"\") + \n  tm_layout(legend.text.size = 0.65, inner.margins = c(0.1, 0.1, 0.02, 0.05), \n            legend.position = c(0.03,0.03), legend.width=0.55)\n\n\nfinal_map_block\n\n\n\n\nTo check that the highlighted polygons in the map above are actually the ones belonging to the same MSOA as observation 1, we can do the following. First, output the rows of df where the MSOA code, given by column MSOA11CD is the as for the first polygon (observation 1):\n\n# Subset the dataframe to get rows with matching MSOA11CD as observation 1\ndf[df$MSOA11CD == obs1$MSOA11CD, ]\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 335310.8 ymin: 389306.6 xmax: 337349.4 ymax: 391245.9\nProjected CRS: Transverse_Mercator\n# A tibble: 5 × 3\n  LSOA11CD  MSOA11CD                                                        geom\n  &lt;chr&gt;     &lt;chr&gt;                                             &lt;MULTIPOLYGON [m]&gt;\n1 E01006512 E02001377 (((336103.4 389628.6, 336103.4 389629.2, 336103.4 389629.…\n2 E01006747 E02001377 (((335371.7 390556.5, 335367.6 390568.1, 335366.6 390570.…\n3 E01006748 E02001377 (((336367.8 390638.9, 336387 390629.9, 336387.5 390629.6,…\n4 E01006751 E02001377 (((336357.9 389851.3, 336354.6 389857.8, 336354.4 389858.…\n5 E01033763 E02001377 (((336853.1 390479.4, 336858.5 390480.7, 336862.5 390481.…\n\n\nThese observations should be the same as the rows corresponding to the neighbours of the first polygon as given by the nb2blocknb fucntion:\n\n# Extract the rows corresponding to the block neighbors of the first observation\ndf[c(w_block[[1]]),]\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 335310.8 ymin: 389409.1 xmax: 337349.4 ymax: 391245.9\nProjected CRS: Transverse_Mercator\n# A tibble: 4 × 3\n  LSOA11CD  MSOA11CD                                                        geom\n  &lt;chr&gt;     &lt;chr&gt;                                             &lt;MULTIPOLYGON [m]&gt;\n1 E01006747 E02001377 (((335371.7 390556.5, 335367.6 390568.1, 335366.6 390570.…\n2 E01006748 E02001377 (((336367.8 390638.9, 336387 390629.9, 336387.5 390629.6,…\n3 E01006751 E02001377 (((336357.9 389851.3, 336354.6 389857.8, 336354.4 389858.…\n4 E01033763 E02001377 (((336853.1 390479.4, 336858.5 390480.7, 336862.5 390481.…\n\n\nNote that the first output is a dataframe with one more row than the second. This is because the first output includes all the observations in MSOA with code E02001377, whereas the second output only includes the block neighbours that share MSOA with LSOA with code E91006512."
  },
  {
    "objectID": "spatialw_code.html#standardising-spatial-weights-matrices",
    "href": "spatialw_code.html#standardising-spatial-weights-matrices",
    "title": "Lab **Under construction**",
    "section": "Standardising spatial weights matrices",
    "text": "Standardising spatial weights matrices\nIn the context of many spatial analysis techniques, a spatial weights matrix with raw values (e.g. ones and zeros for the binary case) is not always the best suiting one for analysis and some sort of transformation is required. This implies modifying each weight so they conform to certain rules. A common one is being row-normalised. This simply means, that for each observation, the weights corresponding to the neighbours must add to 1. We will look into how we can do this in the case of the queen neighbourhood.\nWe define the neighbour list once again according to the queen criterion:\n\n# Construct neighbors list using queen contiguity\nnb_q &lt;- poly2nb(df, queen = TRUE)\n\nBefore, we constructed the spatial weights matrix using style = \"B\" for binary values. To have row-normalised weights, we set style = \"W\".\n\n# Create a binary spatial weights matrix using queen contiguity\nw_queen &lt;- nb2listw(nb_q, style = \"B\")\n\n# Create a row-standardized spatial weights matrix using queen contiguity\nw_queen_std &lt;- nb2listw(nb_q, style = \"W\")\n\nWe can now inspect the values of the weights of the neighbours of the first polygon in our data set:\n\n# Display the binary weights for the first observation\nw_queen$weights[[1]]  \n\n[1] 1 1 1 1 1 1\n\n# Display the row-standardized weights for the first observation\nw_queen_std$weights[[1]] \n\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\n\n\nThe sum of row-standarised weights should add up to one:\n\n# Calculate the sum of row-standardized weights for the first observation\nsum(w_queen_std$weights[[1]])\n\n[1] 1\n\n\nYES!!!"
  },
  {
    "objectID": "spatialw_code.html#spatial-lag",
    "href": "spatialw_code.html#spatial-lag",
    "title": "Lab **Under construction**",
    "section": "Spatial lag",
    "text": "Spatial lag\nOne of the most direct applications of spatial weight matrices is the so-called spatial lag. The spatial lag of a given variable observed at several locations is the product of a spatial weight matrix and the variable itself:\n\\[\nY_{sl} = WY\n\\] where \\(Y\\) is a \\(Nx1\\) vector with the \\(N\\) observations of the variable. Recall that the product of a matrix and a vector equals the sum of a row by column element multiplication for the resulting value of a given row. In terms of the spatial lag:\n\\[\ny_{sl-i}= \\sum_{j=1}^Nw_{ij}y_j\n\\] If we use row-standardized weights, \\(w_{ij}\\) becomes a proportion between zero and one, and \\(y_{sl-i}\\) can be seen as a weighted average of the variable \\(Y\\) in the neighborhood of \\(i\\).\nTo illustrate this here, we will use the area of each polygon as the variable \\(Y\\) of interest. And to make things a bit nicer later on, we will keep the log of the area instead of the raw measurement. Hence, let’s create a column for it:\n\n# Calculate the logarithm of the area of each polygon and add it as a new column named 'area'\ndf$area &lt;- log(as.vector(st_area(df)))\n\nThe spatial lag of a given variable, we use the fucntion lag.listw():\n\n# Calculate the spatial lag of the (log of) 'area' variable using the row-standardized spatial weights matrix\narea.lag &lt;- lag.listw(w_queen_std, df$area)\n\nBelow, we output the IDs of the neighbouring polygons accroding to the queen criterion as well as the spatial lag value for observation 1 (i.e. the average value of the log of the area of the neighbours, weighted according to the row-standardised weights in the spatial weights matrix):\n\n# Display the neighbors of the first observation in the spatial weights matrix\nw_queen_std$neighbours[[1]]\n\n[1]   2 149 155 221 287 298\n\n # Display the spatial lag of the 'area' variable for the first observation\narea.lag[[1]]\n\n[1] 12.4066\n\n\nWe add the spatial lag of the log of the area as a new column in df called w_area:\n\n# Add the calculated spatial lag of 'area' as a new column named 'w_area'\ndf$w_area &lt;- area.lag\n\nWe can create two choropleth maps, side to side, showing the values of the variable area in each polygon and of the lagged area:\n\n# Create a map displaying the 'area' variable\narea_map &lt;- tm_shape(df) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(\"area\", n=10, style = \"quantile\", title = \"Area\", palette = \"YlGn\") +\n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_scale_bar(position = c(\"right\", \"bottom\")) + \n  tm_layout(legend.text.size = 0.55, inner.margins = c(0.1, 0.1, 0.02, 0.05), legend.position = c(0.03,0.03), legend.width=0.55)\n\n# Create a map displaying the spatially lagged 'area' variable\nw_area_map &lt;- tm_shape(df) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(\"w_area\", n=10, style = \"quantile\", title = \"Lagged area\", palette = \"YlGn\") +\n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_scale_bar(position = c(\"right\", \"bottom\")) + \n  tm_layout(legend.text.size = 0.55, inner.margins = c(0.1, 0.1, 0.02, 0.05), legend.position = c(0.03,0.03), legend.width=0.55)\n\n# Arrange both maps side by side\ntmap_arrange(area_map, w_area_map)"
  },
  {
    "objectID": "spatialw_code.html#moran-plot",
    "href": "spatialw_code.html#moran-plot",
    "title": "Lab **Under construction**",
    "section": "Moran plot",
    "text": "Moran plot\nThe Moran Plot is a graphical way to start exploring the concept of spatial autocorrelation, and it is a good application of spatial weight matrices and the spatial lag. In essence, it is a standard scatter plot in which a given variable (area, for example) is plotted against its own spatial lag. Usually, a fitted line is added to include more information.\nWe create a Moran plot as follows:\n\n# Create a Moran plot using ggplot2, adding a regression line accroding to a linear model\nmoran_plot &lt;- ggplot(df, aes(x=area, y=w_area)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  labs(title=\"Moran plot\", x=\"Area (log)\", y = \"Lagged area (log)\")\n\n# Apply a minimal theme to the Moran plot\nmoran_plot + theme_minimal()  \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIn order to easily compare different scatter plots and spot outlier observations, it is common practice to standardize the values of the variable before computing its spatial lag and plotting it. This can be accomplished by substracting the average value and dividing the result by the standard deviation:\n\\[\nz_i = \\dfrac{y_i - \\bar{y}}{\\sigma_y}\n\\] where \\(z_i\\) is the standardised version of \\(y_i\\), also knwon as \\(z\\)-score, \\(\\bar{y}\\) is the average of the variable, and \\(\\sigma_y\\) its standard deviation.\nWe compute the \\(z\\)-score by running the code below. We store it as a new column in df called area_z:\n\n# Standardize the 'area' variable and add it as a new column named 'area_z'\ndf$area_z &lt;- (df$area - mean(df$area)) / sd(df$area)\n\nCreating a standardized Moran Plot implies that average values are centered in the plot (as they are zero when standardized) and dispersion is expressed in standard deviations, with the rule of thumb of values greater or smaller than two standard deviations being outliers. A standardized Moran Plot also partitions the space into four quadrants that represent different situations:\n\nHigh-High (HH): values above average surrounded by values above average.\nLow-Low (LL): values below average surrounded by values below average.\nHigh-Low (HL): values above average surrounded by values below average.\nLow-High (LH): values below average surrounded by values above average.\n\nThese will be further explored once spatial autocorrelation has been properly introduced in subsequent blocks.\nBelow we create a Moran plot with the \\(z\\)-scores, but first we need to computer the lag of the \\(z\\)-scores:\n\n# Calculate the spatial lag of the standardized 'area' variable\narea_z.lag &lt;- lag.listw(w_queen_std, df$area_z)\n\n# Add the calculated spatial lag of standardized 'area' as a new column named 'w_area_z'\ndf$w_area_z &lt;- area_z.lag\n\nAnd the plot follows as before, but replacing the to the new standardised variables:\n\n# Create a standardized Moran plot using ggplot2\nmoran_plot_z &lt;- ggplot(df, aes(x=area_z, y=w_area_z)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  geom_hline(aes(yintercept = 0)) +\n  geom_vline(aes(xintercept = 0)) +\n  labs(title=\"Standardised Moran plot\", x=\"Area (log) z-score\", y = \"Lagged area (log) z-score\")\n\n# Apply a minimal theme to the standardized Moran plot\nmoran_plot_z + theme_minimal()  \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "spatialw.html#space-formally",
    "href": "spatialw.html#space-formally",
    "title": "5 Spatial Weights **Under construction**",
    "section": "Space, formally",
    "text": "Space, formally\nHow do you express geographical relations between objects (e.g. areas, points) in a way that can be used in statistical analysis? This is exactly the core of what we get into in here. There are several ways but one of the most widespread approaches is what is termed spatial weights matrices. Their role is to:\n\nTake all richness of geographical relationships and express it in a language that is well understood by statistics and computers\nRelate to concepts of spatial ‘smoothing’ and interpolating data, facilitating visualisation and exploratory analysis\nCheck how the characteristics or outcomes of one spatial object might be correlated with those of its neighbours: e.g. education, criminality, etc.\n\nFurthermore, they are a core element in several spatial analysis techniques:\n\nSpatial autocorrelation\nSpatial clustering/geo-demographics\nSpatial regression\n\nWe define spatial weights matrices as structured sets of numbers that formalise geographical relationships between the objects in a dataset. Essentially, a spatial weights matrix of a given geography is a positive definite matrix of dimensions \\(N \\times N\\), where \\(N\\) is the total number of observed objects:\n\\(W = \\begin{pmatrix} 0 & w_{12} & \\dots & w_{1N}\\\\ w_{21} & \\ddots & w_{ij} & \\vdots \\\\ \\vdots & w_{ji} & 0 & \\vdots \\\\ w_{N1} & \\dots & \\dots & 0 \\end{pmatrix}\\)\nwhere each cell contains a value that represents the degree of spatial contact or interaction between observations \\(i\\) and \\(j\\). A fundamental concept in this context is that of neighbor and neighborhood. By convention, elements in the diagonal (\\(w_{ii}\\)) are set to zero. A neighbour of a given observation \\(i\\) is another observation with which \\(i\\) has some degree of spatial connection. In terms of \\(W\\), \\(i\\) and \\(j\\) are neighbours if \\(w_{ij} &gt; 0\\). Following this logic, the neighbourhood of \\(i\\) will be the set of observations in the system with which it has certain connection, or those observations with a weight greater than zero."
  },
  {
    "objectID": "spatialw.html#types-of-weights",
    "href": "spatialw.html#types-of-weights",
    "title": "5 Spatial Weights **Under construction**",
    "section": "Types of Weights",
    "text": "Types of Weights\nThere are specific types of spatial weights that we can define for our particular analyses.\nContiguity-based weights\n\nThe neighbourhood of an observation is defined by those observations which share boundaries, e.g. rook, bishop or queen neighbourhood (can be based on a point, an edge, etc.).\n\nDistance based weights\n\nThe neighbourhood of an observation is defined by those spatial units which are at a certain distance from the central one, with the weights typically decreasing as distance increases, e.g. inverse distance (1/distance or threshold), \\(k\\) nearest-neighbours or KNN (fixed number of closest neighbors)."
  },
  {
    "objectID": "spatialw.html#the-spatial-lag",
    "href": "spatialw.html#the-spatial-lag",
    "title": "5 Spatial Weights **Under construction**",
    "section": "The Spatial Lag",
    "text": "The Spatial Lag\nWe wrap up the the set of concepts in this block with one of the applications that makes spatial weights matrices so important: the spatial lag.\nGiven a set of observations for a specific variable in a geography and a spatial weights matrix \\(W\\) for that geography, the spatial lag is defined as the product of \\(W\\) and the observations.\nThe spatial lag can be interpreted as a measure that captures the behaviour of the variable in the neighbourhood of each observation \\(i\\). If \\(W\\) is standardised, the spatial lag is the average value of the variable in the neighbourhood."
  },
  {
    "objectID": "spatialw.html#more-materials",
    "href": "spatialw.html#more-materials",
    "title": "5 Spatial Weights **Under construction**",
    "section": "More materials",
    "text": "More materials\nIf you want a similar but slightly different take on spatial weights by Luc Anselin, one of the biggest minds in the field of spatial analysis, we strongly recommend you watch the following two clips, part of the course offered by the Spatial Data Center at the University of Chicago:\nADD LINKS!\nLecture on “Spatial Weights”\nLecture on “Spatial Lag”, you can ignore the last five minutes as they are a bit more advanced"
  },
  {
    "objectID": "spatialw.html#further-readings",
    "href": "spatialw.html#further-readings",
    "title": "5 Spatial Weights **Under construction**",
    "section": "Further readings",
    "text": "Further readings\nIf you liked what you saw in this section and would like to dig deeper into spatial weights, the following readings are good next steps:\nThe chapter is available for free here\nADD LINKS!\nSpatial weights chapter on the GDS book (in progress) reyABwolf.\nFor a more advanced and detailed treatment, the chapters on spatial weights in the Anselin & Rey book anselin2014modern are the best source."
  },
  {
    "objectID": "spatialw.html",
    "href": "spatialw.html",
    "title": "**Under construction**",
    "section": "",
    "text": "5 Spatial Weights\nThis block is about how we can turn geography into numbers that statistics can understand. At this point, we dive right into the more methodological part of the course, so you can expect the conceptual sections to be more challenging. At the same time, the coding side of each block will start looking more and more familiar because we are starting to repeat concepts. We will introduce less new building blocks and instead rely more on what we have seen, just adding small bits here and there."
  },
  {
    "objectID": "spatialw_code.html",
    "href": "spatialw_code.html",
    "title": "1  Under construction",
    "section": "",
    "text": "Lab\nThere are several ways to create spatial weigths matrices so they contain an accurate representation that aligns with the way we understand spatial interactions between observations associated with different locations. In this session, we will introduce the most commonly used ones and will show how to compute them with the following libraries\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(spdep)\n\nLoading required package: spData\n\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\n\n\nTo access larger datasets in this package, install the spDataLarge\npackage with: `install.packages('spDataLarge',\nrepos='https://nowosad.github.io/drat/', type='source')`\n\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(patchwork)"
  },
  {
    "objectID": "spatialw_code.html#under-construction",
    "href": "spatialw_code.html#under-construction",
    "title": "Lab",
    "section": "**Under construction**",
    "text": "**Under construction**\nThere are several ways to create spatial weigths matrices so they contain an accurate representation that aligns with the way we understand spatial interactions between observations associated with different locations. In this session, we will introduce the most commonly used ones and will show how to compute them with the following libraries\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(spdep)\n\nLoading required package: spData\n\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\n\n\nTo access larger datasets in this package, install the spDataLarge\npackage with: `install.packages('spDataLarge',\nrepos='https://nowosad.github.io/drat/', type='source')`\n\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(patchwork)"
  },
  {
    "objectID": "spatialw.html#under-construction",
    "href": "spatialw.html#under-construction",
    "title": "5 Spatial Weights",
    "section": "**Under construction**",
    "text": "**Under construction**\nThis block is about how we can turn geography into numbers that statistics can understand. At this point, we dive right into the more methodological part of the course, so you can expect the conceptual sections to be more challenging. At the same time, the coding side of each block will start looking more and more familiar because we are starting to repeat concepts. We will introduce less new building blocks and instead rely more on what we have seen, just adding small bits here and there."
  }
]