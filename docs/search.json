[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A course in Geographic Data Science",
    "section": "",
    "text": "Welcome\nThis is the website for the “Geographic Data Science” module ENVS363/563 at the University of Liverpool. This is course designed and delivered by Dr. Elisabetta Pietrostefani and Dr. Carmen Cabrera-Arnau from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. Much of the course material is inspired by Dani Arribas-Bel’s course on Geographic Data Science.\nThis module will introduce students to the field of Geographic Data Science (GDS), a discipline established at the intersection between Geographic Information Science (GIS) and Data Science. The course covers how the modern GIS toolkit can be integrated with Data Science tools to solve practical real-world problems.\nCore to the set of employable skills to be taught in this course is an introduction to programming tools. Students will be able to whether to develop their skills in either R (or Python) in Lab sessions.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "A course in Geographic Data Science",
    "section": "Contact",
    "text": "Contact\n\nElisabetta Pietrostefani - e.pietrostefani [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 6xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.\n\n\nCarmen Cabrera-Arnau - c.cabrera-arnau [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 6xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Further readings\nWatch: Solving Life’s Everyday Problems with Data"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "openscience.html",
    "href": "openscience.html",
    "title": "1 - Open Science",
    "section": "",
    "text": "Concepts\nThe ideas behind this block are better communicated through narrative than video or lectures. Hence, the concepts section are delivered through a few references you are expected to read. These will total up about one and a half hours of your focused time.\n\n\nOpen Science\nThe first part of this block is about setting the philosophical background. Why do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\nRead the chapter here. Estimated time: 15min.\n\nFirst half of Chapter 1 in \"Geographic Data Science with PySAL and the PyData stack\" reyABwolf.\n\nRead the piece here. Estimated time: 35min.\n\nThe 2018 Atlantic piece \"The scientific paper is obsolete\" on computational notebooks, by James Somers somers2018scientific.\n\n\n\nModern Scientific Tools\nOnce we know a bit more about why we should care about the tools we use, let's dig into those that will underpin much of this course. This part is interesting in itself, but will also valuable to better understand the practical aspects of the course. Again, we have two reads here to set the tone and complement the practical introduction we saw in the Hands-on and DIY parts of the previous block. We are closing the circle here:\nRead the chapter here.\n\nSecond half of Chapter 1 in \"Geographic Data Science with PySAL and the PyData stack\" reyABwolf.\nThe chapter in the GIS&T Book of Knowledge on computational notebooks, by Geoff Boeing and Dani Arribas-Bel."
  },
  {
    "objectID": "overview.html#aims",
    "href": "overview.html#aims",
    "title": "Overview",
    "section": "Aims",
    "text": "Aims\nThe module has three main aims.\n\nProvide students with core competences in Geographic Data Science (GDS). This includes advancing their statistical and numerical literacy and introducing basic principles of programming and state-of-the-art computational tools for GDS;\nPresent a comprehensive overview of the main methodologies available to the Geographic Data Scientist, as well as their intuition as to how and when they can be applied;\nFocus on real world applications of these techniques in a geographical and applied context."
  },
  {
    "objectID": "overview.html#learning-outcomes",
    "href": "overview.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the module, students should be able to:\nFor all\n\nDemonstrate advanced GIS/GDS concepts and be able to use the tools programmatically to import, manipulate and analyse data in different formats.\nUnderstand the motivation and inner workings of the main methodological approaches of GDS, both analytical and visual.\nEvaluate the suitability of a specific technique, what it can offer and how it can help answer questions of interest.\nApply a number of spatial analysis techniques and how to interpret the results, in the process of turning data into information.\nWhen faced with a new data-set, work independently using GIS/GDS tools programmatically.\n\nOnly for MSc students\n\nDemonstrate a sound understanding of how real-world (geo)data are produced, their potential insights and biases, as well as opportunities and limitations."
  },
  {
    "objectID": "overview.html#feedback",
    "href": "overview.html#feedback",
    "title": "Overview",
    "section": "Feedback",
    "text": "Feedback\nFormal assessment of one map, one MCQ test and one computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during computer, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times. Response time will vary depending on the complexity of the question and staff availability."
  },
  {
    "objectID": "overview.html#computational-environment",
    "href": "overview.html#computational-environment",
    "title": "Overview",
    "section": "Computational Environment",
    "text": "Computational Environment\nADD SOMETHING ABOUR R or Python\nEDIT the below\nThis course can be followed by anyone with access to a bit of technical infrastructure. This section details the set of local and online requirements you will need to be able to follow along, as well as instructions or pointers to get set up on your own. This is a centralized section that lists everything you will require, but keep in mind that different blocks do not always require everything all the time.\nTo reproduce the code in the book, you need the most recent version of R and packages. These can be installed following the instructions provided in our R installation guide.\n\nSoftware\nEDIT\nTo run the analysis and reproduce the code, you need the following software:\n\nQGIS- the stable version (3.22 LTR at the time of writing) is OK, any more recent version will also work.\nR-4.2.2\nRStudio 2022.12.0-353\nQuarto 1.2.280\nthe list of libraries in the next section\n\nTo install and update:\n\nQGIS, download the appropriate version from QGIS.org\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\n\n\nR List of libraries\nThe list of libraries used in this book is provided below:\n\n\nPython set-up\n\n\nOnline accounts"
  },
  {
    "objectID": "assess.html#assignment-i",
    "href": "assess.html#assignment-i",
    "title": "Assessments",
    "section": "Assignment I",
    "text": "Assignment I\n\nTitle: Programmed Map\nType: Coursework\nDue date: 30th October\n25% of the final mark\nChance to be reassessed\nElectronic submission only\n\nThis assignment will be evaluated on technical data processing, map design abilities (assemblage), and design overall narrative.\nOnce you have created your map, you will need to present it. Write 250 about the choices you made to create the map.\n\nSubmit\nOnce completed, you will need to submit the following:\nAn html version of an .qmd document with R or Python integrated code.\nThe assignment will be evaluated based on three main pillars, on which you will have to be successful to achieve a good mark:\n\nData processing\nMap assemblage This includes your ability to master technologies that allow you to create a compelling map.\nDesign and narrative"
  },
  {
    "objectID": "assess.html#assignment-ii",
    "href": "assess.html#assignment-ii",
    "title": "Assessments",
    "section": "Assignment II",
    "text": "Assignment II\n\nTitle: MCQ test\nType: 16th November\nDue date: TBC\n25% of the final mark\nChance to be reassessed\nElectronic submission only\n\n\nTo ensure students are engaging with the course content as it progresses and\nTo provide core learning in advance of the third assessment."
  },
  {
    "objectID": "assess.html#marking-criteria",
    "href": "assess.html#marking-criteria",
    "title": "Assessments",
    "section": "Marking Criteria",
    "text": "Marking Criteria\nThis course follows the standard marking criteria (the general ones and those relating to GIS assignments in particular) set by the School of Environmental Sciences. Please make sure to check the student handbook and familiarise with them. In addition to these generic criteria, the following specific criteria will be used in cases where computer code is part of the work being assessed:\n\n0-15: the code does not run and there is no documentation to follow it.\n16-39: the code does not run, or runs but it does not produce the expected outcome. There is some documentation explaining its logic.\n40-49: the code runs and produces the expected output. There is some documentation explaining its logic.\n50-59: the code runs and produces the expected output. There is extensive documentation explaining its logic.\n60-69: the code runs and produces the expected output. There is extensive documentation, properly formatted, explaining its logic.\n70-79: all as above, plus the code design includes clear evidence of skills presented in advanced sections of the course (e.g. custom methods, list comprehensions, etc.).\n80-100: all as above, plus the code contains novel contributions that extend/improve the functionality the student was provided with (e.g. algorithm optimizations, novel methods to perform the task, etc.)."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Introduction and Open Science\n\nLecture: Introduction to the module & Open Science\nLab: Setting up your Computational Environment & Data Wrangling\n\nSpatial Data\n\nLecture:\nLab:\n\nMapping Vector Data\n\nLecture:\nLab:\n\nMapping Raster Data\n\nLecture:\nLab:\n\nAssignment I: Programmed Map\nSpatial Weights\n\nLecture:\nLab:\n\nESDA\n\nLecture:\nLab:\n\nAssignment II: MCQ test\nClustering\n\nLecture:\nLab:\n\nInterpolation, heatmaps and point patterns\n\nLecture:\nLab:\n\nSpatial Network Analysis\n\nLecture:\nLab:\n\nAssignment III: A computational essay"
  },
  {
    "objectID": "concepts_openscience.html",
    "href": "concepts_openscience.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_spatialdata.html",
    "href": "concepts_spatialdata.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_mapvector.html",
    "href": "concepts_mapvector.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "concepts_mapraster.html",
    "href": "concepts_mapraster.html",
    "title": "Concepts",
    "section": "",
    "text": "bla bla bla"
  },
  {
    "objectID": "environR.html#r-list-of-libraries",
    "href": "environR.html#r-list-of-libraries",
    "title": "R",
    "section": "R List of libraries",
    "text": "R List of libraries\nThe list of libraries used in this book is provided below:\n\nsf\ngeojsonsf\nmapview"
  },
  {
    "objectID": "intro.html#what-is-geographic-data-science",
    "href": "intro.html#what-is-geographic-data-science",
    "title": "Introduction",
    "section": "What is Geographic Data Science?",
    "text": "What is Geographic Data Science?\nThe following clip is taken from a keynote response by Dani Arribas-Bel at the first Spatial Data Science Conference, organised by CARTO and held in Brooklyn in 2017. The talk provides a bit of background and context, which will hopefully help you understand a bit better what Geographic Data Science is.\nTOP UP with slide content"
  },
  {
    "objectID": "intro.html#get-ready",
    "href": "intro.html#get-ready",
    "title": "Introduction",
    "section": "Get ready!",
    "text": "Get ready!\nGo the the Computation Environment section"
  },
  {
    "objectID": "intro.html#further-readings",
    "href": "intro.html#further-readings",
    "title": "1  Introduction",
    "section": "2.1 Further readings",
    "text": "2.1 Further readings\nTo get a better picture, the following readings complement the overview provided above very well:\nBonus\nWatch ! All Maps are wrong https://www.youtube.com/watch?v=kIID5FDi2JQ Watch: Solving Life’s Everyday Problems with Data https://www.sciencefriday.com/segments/solving-lifes-everyday-problems-with-data/\nThe chapter is available free online HTML | PDF\n\nThe introductory chapter to “Doing Data Science” schutt2013doing, by Cathy O’Neil and Rachel Schutt is general overview of why we needed Data Science and where if came from.\nA slightly more technical historical perspective on where Data Science came from and where it might go can be found in David Donoho’s recent overview donoho201750.\nA geographic take on Data Science, proposing more interaction between Geography and Data Science singleton2019geographic."
  },
  {
    "objectID": "environ.html#software",
    "href": "environ.html#software",
    "title": "Environment",
    "section": "Software",
    "text": "Software\nTo run the analysis and reproduce the code, you need the following software:\n\nQGIS- the stable version (3.22 LTR at the time of writing) is OK, any more recent version will also work.\nQGIS, download the appropriate version from QGIS.org\nQuarto 1.2.280\nQuarto, download the appropriate version from the Quarto website"
  },
  {
    "objectID": "assess.html#assignment-iii",
    "href": "assess.html#assignment-iii",
    "title": "Assessments",
    "section": "Assignment III",
    "text": "Assignment III\n\nTitle: Computational Essay\nType: Coursework\nDue date: 18th December\n50% of the final mark\nChance to be reassessed\nElectronic submission only\n\nA 2500 word computational essay on a geographic data set which they have explored and analysed using the skills and techniques developed during the course. Students will complete an essay which combines both code, data visualisation and prose supported by references in order to demonstrate sound understanding of all learning outcomes.\nOverview\nHere’s the premise. You will take the role of a real-world geographic sata scientist tasked to explore datasets on New York City and find useful insights for a variety of city decision-makers. It does not matter if you have never been to New York City. In fact, this will help you focus on what you can learn about the city through the data, without the influence of prior knowledge. Furthermore, the assessment will not be marked based on how much you know about New York City but instead about how much you can show you have learned through analysing data. You will need contextualise your project by highlighting the opportunities and limitations of ‘old’ and ‘new’ forms of spatial data and reference relevant literature.\nWhat is a Computational Essay?\nA computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. This piece of assessment is equivalent to 2,500 word. However, this is the overall weight. Since you will need to create not only narrative but also code and figures, here are the requirements:\n\nMaximum of 1,000 words (ordinary text) (references do not contribute to the word count). You should answer the specified questions within the narrative. The questions should be included within a wider analysis.\nUp to four maps or figures (a figure may include more than one map and will only count as one but needs to be integrated in the same overall output)\nUp to one table\n\nThere are three kinds of elements in a computational essay:\n1. Ordinary text (in English)\n2. Computer input (R)\n3. Computer output These three elements all work together to express what’s being communicated.\nSubmission (Coming Soon)\nData (Coming Soon)\nSpecifics (Coming Soon)"
  },
  {
    "objectID": "environ.html#book-software",
    "href": "environ.html#book-software",
    "title": "Environment",
    "section": "Book Software",
    "text": "Book Software\nTo reproduce the code in the book, you need the most recent version of Quarto, R and relevant packages. These can be installed following the instructions provided in our R installation guide. Quarto (1.2.280) can be downloaded from the Quarto website, it may already be installed when you download R and R Studio."
  },
  {
    "objectID": "intro.html#from-geographic-data-science-to-geographic-data-science",
    "href": "intro.html#from-geographic-data-science-to-geographic-data-science",
    "title": "1 Introduction",
    "section": "From Geographic Data Science to Geographic Data Science",
    "text": "From Geographic Data Science to Geographic Data Science\nGeographic Information holds a pivotal position within our modern societies, permeating various aspects of our daily lives. It underpins essential sectors such as housing, transportation, insurance, banking, telecommunications, logistics, energy, retail, agriculture, healthcare, and urban planning. Its significance lies in the capacity to analyze and derive invaluable insights from geo-spatial data, enabling us to make informed decisions and address complex challenges. Proficiency in this field equips individuals with the ability to work with real-world data across multiple domains and tackle diverse problems. Furthermore, it provides the opportunity to acquire essential data science skills and utilize important tools for answering spatial questions. Given its wide-ranging applications and the increasing reliance on location-based information, there is a substantial demand for experts in the geographic information industry, making it a highly sought-after skill set in today’s workforce.\nWhat information does GIS use?\n\nData that defines geographical features like roads, rivers\nSoil types, land use, elevation\nDemographics, socioeconomic attributes\nEnvironmental, climate, air-quality\nAnnotations that label features and places\n\nGeographic Data Science\nA GIS person typically produces cartographic and analytical products using desktop software. A geospatial data scientist creates code and runs pipelines that produce analytical products and cartographic representations.\nThis entails working with real-world data from various domains and tackling a wide range of complex problems. Through this process geospatial data science includes both data science and GIS tools that lead to the analysos of intricate spatial questions effectively. The synergy between CyberGIS and Geographic Data Science is unmistakable, with coding playing a pivotal role in enabling the seamless development of interactive data analysis. By leveraging cutting-edge technologies and innovative methodologies, this symbiotic relationship enhances the accessibility, scalability, and interactivity of geospatial data analysis. Consequently, it opens up new vistas for collaborative research and decision-making processes.\nThis multifaceted approach equips them with the knowledge and expertise to navigate the intricate world of spatial data analysis and contribute meaningfully to diverse fields where location-based insights are invaluable."
  },
  {
    "objectID": "intro.html#a-useful-clip-cannot-find-it",
    "href": "intro.html#a-useful-clip-cannot-find-it",
    "title": "Introduction",
    "section": "A useful clip (cannot find it)",
    "text": "A useful clip (cannot find it)\nThe following clip is taken from a keynote response by Dani Arribas-Bel at the first Spatial Data Science Conference, organised by CARTO and held in Brooklyn in 2017. The talk provides a bit of background and context, which will hopefully help you understand a bit better what Geographic Data Science is."
  },
  {
    "objectID": "intro.html#open-science-1",
    "href": "intro.html#open-science-1",
    "title": "1 Introduction",
    "section": "Open Science",
    "text": "Open Science\nWhy do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\n\nFirst half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.\nThe 2018 Atlantic piece “The scientific paper is obsolete” on computational notebooks, by James Somers."
  },
  {
    "objectID": "intro.html#modern-scientific-tools",
    "href": "intro.html#modern-scientific-tools",
    "title": "1 Introduction",
    "section": "Modern Scientific Tools",
    "text": "Modern Scientific Tools\nOnce we know a bit more about why we should care about the tools we use, let’s dig into those that will underpin much of this course. This part is interesting in itself, but will also valuable to better understand the practical aspects of the course. Again, we have two reads here to set the tone and complement the practical introduction we saw in the Hands-on and DIY parts of the previous block. We are closing the circle here:\n\nSecond half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists."
  },
  {
    "objectID": "spatialdata.html",
    "href": "spatialdata.html",
    "title": "Spatial Data",
    "section": "",
    "text": "Watch ! All Maps are wrong https://www.youtube.com/watch?v=kIID5FDi2JQ"
  },
  {
    "objectID": "environPy.html",
    "href": "environPy.html",
    "title": "Python",
    "section": "",
    "text": "Resources\nSome help along the way with:\n\nGeographic Data Science with Python by Sergio J. Rey, Dani Arribas-Bel, Levi J. Wolf"
  },
  {
    "objectID": "environR.html",
    "href": "environR.html",
    "title": "R",
    "section": "",
    "text": "Resources\nSome help along the way with:"
  },
  {
    "objectID": "openscienceR.html",
    "href": "openscienceR.html",
    "title": "1  | include: false",
    "section": "",
    "text": "OpenScience in R\nOnce we know a bit about what computational notebooks are and why we should care about them, let’s jump to using them! This section introduces you to using R or Python for manipulating tabular data. Please read through it carefully and pay attention to how ideas about manipulating data are translated into code that “does stuff”. For this part, you can read directly from the course website, although it is recommended you follow the section interactively by running code on your own.\nOnce you have read through and have a bit of a sense of how things work, jump on the Do-It-Yourself section, which will provide you with a challenge to complete it on your own, and will allow you to put what you have already learnt to good use."
  },
  {
    "objectID": "openscienceR.html#data-wrangling",
    "href": "openscienceR.html#data-wrangling",
    "title": "OpenScience in R",
    "section": "Data wrangling",
    "text": "Data wrangling\nReal world datasets are messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to munge them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but has to do with much more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nFor how labor intensive and relevant this aspect is, there is surprisingly very little published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation. In this session, you will use a few real world datasets and learn how to process them into Python so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the bread and butter of data analysis and scientific computing in Python. These are fundamental tools that are constantly used in almost any task relating to data analysis.\nThis notebook covers the basic and the content that is expected to be learnt by every student. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at. As a companion to this introduction, there is an additional notebook (see link on the website page for Lab 01) that covers how the dataset used here was prepared from raw data downloaded from the internet, and includes some additional exercises you can do if you want dig deeper into the content of this lab.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need, so we can get that out of the way and focus on the task at hand:"
  },
  {
    "objectID": "openscienceR.html#loading-packages",
    "href": "openscienceR.html#loading-packages",
    "title": "OpenScience in R",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\nPythonR\n\n\n\n\n\n\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tmap)"
  },
  {
    "objectID": "syllabus.html#part-1",
    "href": "syllabus.html#part-1",
    "title": "Syllabus",
    "section": "Part 1",
    "text": "Part 1\nIntroduction and Open Science\n\nLecture: Introduction to the module & Open Science\nLab: Setting up your Computational Environment & Data Wrangling\n\nSpatial Data\n\nLecture: Spatial Data\nLab: London\n\nMapping Vector Data\n\nLecture:\nLab:\n\nMapping Raster Data\n\nLecture:\nLab:\n\nAssignment I: Programmed Map"
  },
  {
    "objectID": "syllabus.html#part-2",
    "href": "syllabus.html#part-2",
    "title": "Syllabus",
    "section": "Part 2",
    "text": "Part 2\nSpatial Weights\n\nLecture:\nLab:\n\nESDA\n\nLecture:\nLab:\n\nAssignment II: MCQ test\nClustering\n\nLecture:\nLab:\n\nInterpolation, heatmaps and point patterns\n\nLecture:\nLab:\n\nSpatial Network Analysis\n\nLecture:\nLab:\n\nAssignment III: A computational essay"
  },
  {
    "objectID": "openscienceDIY.html#import-libraries",
    "href": "openscienceDIY.html#import-libraries",
    "title": "Do-It-Yourself",
    "section": "Import libraries",
    "text": "Import libraries\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "openscienceDIY.html#tasks",
    "href": "openscienceDIY.html#tasks",
    "title": "Do-It-Yourself",
    "section": "Tasks",
    "text": "Tasks\nNow, the challenge is to put to work what we have learnt in this block. For that, the suggestion is that you carry out an analysis of the Afghan Logs in a similar way as how we looked at population composition in Liverpool. These are of course very different datasets reflecting immensely different realities. Their structure, however, is relatively parallel: both capture counts aggregated by a spatial (neighbourhood) or temporal unit (month), and each count is split by a few categories.\nTry to answer the following questions:\n\nObtain the minimum number of civilian casualties (in what month was that?)\nHow many NATO casualties were registered in August 2008?\nWhat is the month with the most total number of casualties?\n\nTip: You will need to first create a column with total counts"
  },
  {
    "objectID": "openscience.html#data-wrangling",
    "href": "openscience.html#data-wrangling",
    "title": "Lab",
    "section": "Data wrangling",
    "text": "Data wrangling\nReal world datasets tend to be messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to wrangle (manipulate, transform and structure) them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but to more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nIn this session, you will use a few real world datasets and learn how to process them in R so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the fundamental tools of data analysis and scientific computing. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need to run the code:"
  },
  {
    "objectID": "openscience.html#loading-packages",
    "href": "openscience.html#loading-packages",
    "title": "Lab",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\nlibrary(tidyverse) # a structure of data manipulation including several packages \nlibrary(data.table)"
  },
  {
    "objectID": "openscience.html#datasets",
    "href": "openscience.html#datasets",
    "title": "Lab",
    "section": "Datasets",
    "text": "Datasets\nWe will be exploring some demographic characteristics in Liverpool. To do that, we will use a dataset that contains population counts, split by ethnic origin. These counts are aggregated at the Lower Layer Super Output Area (LSOA from now on). LSOAs are an official Census geography defined by the Office of National Statistics. You can think of them, more or less, as neighbourhoods. Many data products (Census, deprivation indices, etc.) use LSOAs as one of their main geographies.\nTo do this, we will download a data folder from github called census2021_ethn. You should place this in a data folder you will use throughout the course.\nImport housesales data from csv\n\ncensus2021 <- read.csv(\"data/census2021_ethn/liv_pop.csv\", row.names = \"GeographyCode\")\n\nLet us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n\nWe are using the method read.csv from base R, you could also use read_csv from library(\"readr\").\nHere the csv is based on a data file but it could also be a web address or sometimes you find data in packages.\nThe argument row.names is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\nWe are using read.csv because the file we want to read is in the csv format. However, many more formats can be read into an R environment. A full list of formats supported may be found here.\nTo ensure we can access the data we have read, we store it in an object that we call census2021. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read.csv.\n\nImportant\nYou need to store the data file on your computer, and read it locally. To do that, you can follow these steps: 1. Download the census2021_ethn file by right-clicking on this link and saving the file 2. Place the file in a data folder you have created where you intend to read it. 3. Your folder should have the following structure a. a gds folder (where you will save your quarto documents) b. a data folder c. the census2021_ethn folder inside your data folder."
  },
  {
    "objectID": "openscience.html#data-sliced-and-diced",
    "href": "openscience.html#data-sliced-and-diced",
    "title": "Lab",
    "section": "Data, sliced and diced",
    "text": "Data, sliced and diced\nNow we are ready to start playing with and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the LSOAs in Liverpool, how many people live in each, by the region of the world where they were born. We call these tables DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain.\nStructure\nLet’s start by exploring the structure of a DataFrame. We can print it by simply typing its name:\n\nview(census2021)\n\nSince they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what we will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. In the example above, we can see how the column index is automatically picked up from the .csv file’s column names. For rows, we have specified when reading the file we wanted the column GeographyCode, so that is used. If we hadn’t specified any, tidyverse in R will automatically generate a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so we will come to it over and over. Importantly, even when we move to spatial data, our datasets will have a similar structure.\nOne further feature of these tables is that they can hold columns with different types of data. In our example, this is not used as we have counts (or int, for integer, types) for each column. But it is useful to keep in mind we can combine this with columns that hold other type of data such as categories, text (str, for string), dates or, as we will see later in the course, geographic features.\nInspecting\nWe can check the top (bottom) X lines of the table by passing X to the method head (tail). For example, for the top/bottom five lines:\n\nhead(census2021) # read first 5 rows\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania\nE01006512                      0\nE01006513                      7\nE01006514                      5\nE01006515                      2\nE01006518                      4\nE01006519                      3\n\ntail(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033763   1302     68                  142                             11\nE01033764   2106     32                   49                             15\nE01033765   1277     21                   33                             17\nE01033766   1028     12                   20                              8\nE01033767   1003     29                   29                              5\nE01033768   1016     69                  111                             21\n          Antarctica.and.Oceania\nE01033763                      4\nE01033764                      0\nE01033765                      3\nE01033766                      7\nE01033767                      1\nE01033768                      6\n\n\nSummarise\nWe can get an overview of the values of the table:\n\nsummary(census2021)\n\n     Europe         Africa       Middle.East.and.Asia\n Min.   : 731   Min.   :  0.00   Min.   :  1.00      \n 1st Qu.:1331   1st Qu.:  7.00   1st Qu.: 16.00      \n Median :1446   Median : 14.00   Median : 33.50      \n Mean   :1462   Mean   : 29.82   Mean   : 62.91      \n 3rd Qu.:1580   3rd Qu.: 30.00   3rd Qu.: 62.75      \n Max.   :2551   Max.   :484.00   Max.   :840.00      \n The.Americas.and.the.Caribbean Antarctica.and.Oceania\n Min.   : 0.000                 Min.   : 0.00         \n 1st Qu.: 2.000                 1st Qu.: 0.00         \n Median : 5.000                 Median : 1.00         \n Mean   : 8.087                 Mean   : 1.95         \n 3rd Qu.:10.000                 3rd Qu.: 3.00         \n Max.   :61.000                 Max.   :11.00         \n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00"
  },
  {
    "objectID": "openscience.html#summarise",
    "href": "openscience.html#summarise",
    "title": "OpenScience",
    "section": "Summarise",
    "text": "Summarise\nOr of the values of the table:\n\nRPython\n\n\n\nsummary(census2021)\n\n     Europe         Africa       Middle.East.and.Asia\n Min.   : 731   Min.   :  0.00   Min.   :  1.00      \n 1st Qu.:1331   1st Qu.:  7.00   1st Qu.: 16.00      \n Median :1446   Median : 14.00   Median : 33.50      \n Mean   :1462   Mean   : 29.82   Mean   : 62.91      \n 3rd Qu.:1580   3rd Qu.: 30.00   3rd Qu.: 62.75      \n Max.   :2551   Max.   :484.00   Max.   :840.00      \n The.Americas.and.the.Caribbean Antarctica.and.Oceania\n Min.   : 0.000                 Min.   : 0.00         \n 1st Qu.: 2.000                 1st Qu.: 0.00         \n Median : 5.000                 Median : 1.00         \n Mean   : 8.087                 Mean   : 1.95         \n 3rd Qu.:10.000                 3rd Qu.: 3.00         \n Max.   :61.000                 Max.   :11.00         \n\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nRPython\n\n\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00"
  },
  {
    "objectID": "openscience.html#queries",
    "href": "openscience.html#queries",
    "title": "Lab",
    "section": "Queries",
    "text": "Queries\nIndex-based queries\nHere we explore how we can subset parts of a DataFrame if we know exactly which bits we want. For example, if we want to extract the total and European population of the first four areas in the table:\nWe can select with c(). If this structure is new to you have a look here.\n\neu_tot_first4 <- census2021[c('E01006512', 'E01006513', 'E01006514', 'E01006515'), c('Total_Population', 'Europe')]\n\neu_tot_first4\n\n          Total_Population Europe\nE01006512             1880    910\nE01006513             2941   2225\nE01006514             2108   1786\nE01006515             1208    974\n\n\nCondition-based queries\nHowever, sometimes, we do not know exactly which observations we want, but we do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose we want to select…\nAreas with more than 900 people in Total:\n\npop900 <- census2021 %>%\n  filter(Total_Population > 900)\n\nAreas where there are no more than 750 Europeans:\n\neuro750 <- census2021 %>%\n  filter(Europe < 750)\n\nAreas with exactly ten person from Antarctica and Oceania:\n\noneOA <- census2021 %>%\n  filter(`Antarctica.and.Oceania` == 10)\n\nPro-tip: These queries can grow in sophistication with almost no limits.\nCombining queries\nNow all of these queries can be combined with each other, for further flexibility. For example, imagine we want areas with more than 25 people from the Americas and Caribbean, but less than 1,500 in total:\n\nac25_l500 <- census2021 %>%\n  filter(The.Americas.and.the.Caribbean > 25, Total_Population < 1500)\nac25_l500\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033750   1235     53                  129                             26\nE01033752   1024     19                  114                             33\nE01033754   1262     37                  112                             32\nE01033756    886     31                  221                             42\nE01033757    731     39                  223                             29\nE01033761   1138     52                  138                             33\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01033750                      5             1448      1448          1\nE01033752                      6             1196      1196          1\nE01033754                      9             1452      1452          1\nE01033756                      5             1185      1185          1\nE01033757                      3             1025      1025          1\nE01033761                     11             1372      1372          1"
  },
  {
    "objectID": "openscience.html#sorting",
    "href": "openscience.html#sorting",
    "title": "Lab",
    "section": "Sorting",
    "text": "Sorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine we want to sort the table by total population:\n\ndb_pop_sorted <- census2021 %>%\n  arrange(desc(Total_Pop)) #sorts the dataframe by the \"Total_Pop\" column in descending order \n\nhead(db_pop_sorted)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006747   2551    163                  812                             24\nE01006513   2225     61                  595                             53\nE01006751   1843    139                  568                             21\nE01006524   2235     36                  125                             24\nE01006787   2187     53                   75                             13\nE01006537   2180     23                   46                              6\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01006747                      2             3552      3552          1\nE01006513                      7             2941      2941          1\nE01006751                      1             2572      2572          1\nE01006524                     11             2431      2431          1\nE01006787                      2             2330      2330          1\nE01006537                      2             2257      2257          1"
  },
  {
    "objectID": "openscience.html#visual-exploration",
    "href": "openscience.html#visual-exploration",
    "title": "OpenScience",
    "section": "Visual Exploration",
    "text": "Visual Exploration"
  },
  {
    "objectID": "openscience.html#python-4",
    "href": "openscience.html#python-4",
    "title": "OpenScience",
    "section": "Python",
    "text": "Python\n:::\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nRPython\n\n\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00   \n\n\n\n\n\n\n\n\nCreate new columns Delete columns"
  },
  {
    "objectID": "openscienceDIY.html#data-preparation",
    "href": "openscienceDIY.html#data-preparation",
    "title": "Do-It-Yourself",
    "section": "Data preparation",
    "text": "Data preparation\nBefore you can set off on your data journey, the dataset needs to be read, and there’s a couple of details we will get out of the way so it is then easier for you to start working.\nThe data are published on a Google Sheet.\nAs you will see, each row includes casualties recorded month by month, split by Taliban, Civilians, Afghan forces, and NATO.\nLet’s read it into an R session:\n\n# Specify the URL of the CSV file\nurl &lt;- \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRa7OIBiz7-yqmgwUEn4V5Wm1TO8rGow_wQVS1PWp--UTCAKqNUhtifECO5ZR9XrMd6Ddq9NxQwf1ll/pub?gid=0&single=true&output=csv\"\n\n# Read the data from the URL into a DataFrame\ndata &lt;- read.csv(url)\n\n# see the data\nhead(data)\n\n  Year    Month Taliban Civilians Afghan.forces Nato..detailed.in.spreadsheet.\n1 2004  January      15        51            23                               \n2 2004 February                 7             4                              5\n3 2004    March      19         2                                            2\n4 2004    April       5         3            19                               \n5 2004      May      18        29            56                              6\n6 2004     June     163        32            14                              2\n  Nato...official.figures\n1                      11\n2                       2\n3                       3\n4                       3\n5                       9\n6                       5\n\n\nThis allows us to read the data straight into a data frame, as we have done in the previous session.\nNow we are good to go!"
  },
  {
    "objectID": "environ.html#website-software",
    "href": "environ.html#website-software",
    "title": "Environment",
    "section": "Website Software",
    "text": "Website Software\nTo reproduce the code in the book, you need the most recent version of Quarto, R and relevant packages. These can be installed following the instructions provided in our R installation guide. Quarto (1.2.280) can be downloaded from the Quarto website, it may already be installed when you download R and R Studio."
  },
  {
    "objectID": "environR.html#r-basics-and-making-a-simple-map-of-london",
    "href": "environR.html#r-basics-and-making-a-simple-map-of-london",
    "title": "R",
    "section": "R Basics and Making a simple map of London",
    "text": "R Basics and Making a simple map of London\n\nStarting a session\nUpon startup, RStudio will look something like this. Note: the Pane Layout and Appearance settings can be altered e.g. on Mac OS by clicking RStudio>Preferences>Appearance and RStudio>Preferences>Pane Layout. I personally like to have my Console in the top right corner and Environment in the bottom left and keep the Source and Environment panes wider than Console and Files for easier readability. Default settings will probably have the Console in the bottom left and Environment in the top right. You will also have a standard white background; I personally use the Cobalt theme.\n\n\n\n\n\nAt the start of a session, it’s good practice clearing your R environment:\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set. You should have set this following the pre-recorded video.\n\ngetwd() \n\nIf the directory is not set yet, type in setwd(\"~/pathtodirectory\") to set it. It is crucial to perform this step at the beginning of your R script, so that relative paths can be used in the subsequent parts.\n\nsetwd(\"~/Dropbox/Github/gds\")\n\nIf you have set your directory correctly, it will show up at the top of the console pane:\n\n\n\n\n\n\n\nUsing the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\nR Objects\nEverything in R is an object. R possesses a simple generic function mechanism which can be used for an object-oriented style of programming. Indeed, everything that happens in R is the result of a function call (John M. Chambers). Method dispatch takes place based on the class of the first argument to the generic function.\nAll R statements where you create objects – “assignments” – have this form: object_name <- value. Assignment can also be performed using = instead of <-, but the standard advice is to use the latter syntax (see e.g. The R Inferno, ch. 8.2.26). In RStudio, the standard shortcut for the assignment operator <- is Alt + - (in Windows) or option + - (in Mac OS).\nA mock assignment of the value 30 to the name age is reported below. In order to inspect the content of the newly created variable, it is sufficient to type the name into the console. Within R, the hash symbol # is used to write comments and create collapsible code sections.\n\nage <- 30 # Assign the number 30 to the name \"age\"\nage # print the variable \"age\" to the console\n\n[1] 30\n\n\n\n\nA small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing <- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA\n\n\n\n\nLogical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n> greater than\n>= greater or equal to\n<= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not\n\n\n\nExamples\nLet’s create some random R objects:\n\n## Entering random \nLondon  <- 8982000 # population\nBristol <- 467099 # population\nLondon_area <-1572 # area km2\nBristol_area <-110 # area km2\n\nLondon\n\n[1] 8982000\n\n\nCalculate Population Density in London:\n\nLondon_pop_dens <- London/London_area\nBristol_pop_dens <- Bristol/Bristol_area\n\nLondon_pop_dens\n\n[1] 5713.74\n\n\nThe function c(), which you will use extensively if you keep coding in R, means “concatenate”. In this case, we use it to create a vector of population densities for London and Bristol:\n\nc(London_pop_dens, Bristol_pop_dens)\n\n[1] 5713.740 4246.355\n\npop_density <- c(London_pop_dens, Bristol_pop_dens) # In order to create a vector in R we make use of c() (which stands for concatenate)\n\nCreate a character variable:\n\nx <- \"a city\"\nclass(x)\n\n[1] \"character\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1\n\n\n\n\nData Structures\nObjects in R are typically stored in data structures. There are multiple types of data structures:\n\n\nVectors\nIn R, a vector is a sequence of elements which share the same data type. A vector supports logical, integer, double, character, complex, or raw data types.\n\n# first vector y\ny <- 1:10\nas.numeric(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\n# another vector z\nz <- c(2, 4, 56, 4)\nz\n\n[1]  2  4 56  4\n\n# and another one called cities\ncities <- c(\"London\", \"Bristol\", \"Bath\")\ncities\n\n[1] \"London\"  \"Bristol\" \"Bath\"   \n\n\n\n\nMatrices\nTwo-dimensional, rectangular, and homogeneous data structures. They are similar to vectors, with the additional attribute of having two dimensions: the number of rows and columns.\n\nm <- matrix(nrow = 2, ncol = 2)\nm\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\nn <- matrix(c(4, 5, 78, 56), nrow = 2, ncol = 2 )\nn\n\n     [,1] [,2]\n[1,]    4   78\n[2,]    5   56\n\n\n\n\nLists\nLists are containers which can store elements of different types and sizes. A list can contain vectors, matrices, dataframes, another list, functions which can be accessed, unlisted, and assigned to other objects.\n\nlist_data <- list(\"Red\", \"Green\", c(21,32,11), TRUE, 51.23, 119.1)\nprint(list_data)\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] 51.23\n\n[[6]]\n[1] 119.1\n\n\n\n\nData frames\nThey are the most common way of storing data in R and are the most used data structure for statistical analysis. Data frames are “rectangular lists”, i.e. tabular structures in which every element has the same length, and can also be thought of as lists of equal length vectors.\n\n## Here is a data frame of 3 columns named id, x, y and 10 rows\ndat <- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nhead(dat) # read first 5 rows\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ntail(dat)\n\n   id  x  y\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n\nDataframes in R are indexed by rows and columns numbers using the [rows,cols] syntax. The $ operator allows you to access columns in the dataframe, or to create new columns in the dataframe.\n\ndat[1,] # read first row and all colum ns\n\n  id x  y\n1  a 1 11\n\ndat[,1] # read all rows and the first column\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\ndat[6,3] # read 6th row, third column\n\n[1] 16\n\ndat[c(2:4),] # read rows 2 to 4 and all columns\n\n  id x  y\n2  b 2 12\n3  c 3 13\n4  d 4 14\n\ndat$y # read column y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat[dat$x<7,] # read rows that have a x value less than 7\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndat$new_column <- runif(10, 0, 1) # create a new variable called \"new_column\"\n\ndat\n\n   id  x  y new_column\n1   a  1 11  0.3230397\n2   b  2 12  0.2004565\n3   c  3 13  0.3654617\n4   d  4 14  0.6679464\n5   e  5 15  0.4820938\n6   f  6 16  0.8266886\n7   g  7 17  0.1367763\n8   h  8 18  0.1307404\n9   i  9 19  0.1557868\n10  j 10 20  0.8541773\n\n\n\n\nExercises 1\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector <- c(\"Elisabetta\", \"Capucine\", \"Lorenzo\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform <- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 2.17794816 0.05844188 1.55794470 0.55761688 0.20836326 2.56954717\n [7] 2.44263921 2.99906779 0.33792777 1.35694690\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector <- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix <-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df <-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\nlibrary(tidyverse)\n\n\n\nShow the code\nnew_df <-  new_df %>%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %>%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714"
  },
  {
    "objectID": "intro.html#open-science",
    "href": "intro.html#open-science",
    "title": "1 Introduction",
    "section": "Open Science",
    "text": "Open Science\nWhy do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\n\nFirst half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.\nThe 2018 Atlantic piece “The scientific paper is obsolete” on computational notebooks, by James Somers."
  },
  {
    "objectID": "openscience.html#columns",
    "href": "openscience.html#columns",
    "title": "Lab",
    "section": "Columns",
    "text": "Columns\nCreate new columns\nWe can generate new variables by applying operations on existing ones. For example, we can calculate the total population by area. Here is a couple of ways to do it:\nOn base R\n\ncensus2021$Total_Population <- rowSums(census2021[, c(\"Africa\", \"Middle.East.and.Asia\", \"Europe\", \"The.Americas.and.the.Caribbean\", \"Antarctica.and.Oceania\")])\n\nUsing the package dplyr\n\ncensus2021 <- census2021 %>%\n  mutate(Total_Pop = rowSums(select(., Africa, Middle.East.and.Asia, Europe, The.Americas.and.the.Caribbean, Antarctica.and.Oceania)))\n\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Population Total_Pop\nE01006512                      0             1880      1880\nE01006513                      7             2941      2941\nE01006514                      5             2108      2108\nE01006515                      2             1208      1208\nE01006518                      4             1696      1696\nE01006519                      3             1286      1286\n\n\nA different spin on this is assigning new values: we can generate new variables with scalars, and modify those:\n\ncensus2021$new_column <- 1\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Population Total_Pop new_column\nE01006512                      0             1880      1880          1\nE01006513                      7             2941      2941          1\nE01006514                      5             2108      2108          1\nE01006515                      2             1208      1208          1\nE01006518                      4             1696      1696          1\nE01006519                      3             1286      1286          1\n\n\ndplyr is an immensely useful package in R because it streamlines and simplifies the process of data manipulation and transformation. With its intuitive and consistent syntax, dplyr provides a set of powerful and efficient functions that make tasks like filtering, summarizing, grouping, and joining datasets much more straightforward. Whether you’re working with small or large datasets, dplyr’s optimized code execution ensures fast and efficient operations. Its ability to chain functions together using the pipe operator (%>%) allows for a clean and readable code structure, enhancing code reproducibility and collaboration. Overall, dplyr is an indispensable tool for data analysts and scientists working in R, enabling them to focus on their data insights rather than wrestling with complex data manipulation code.\nDelete columns\nPermanently deleting variables is also within reach of one command:\nBase R\n\ncensus2021 <- subset(census2021, select = -new_column)\n\ndplyr\n\ncensus2021 <- census2021 %>%\n  mutate(new_column = 1)"
  },
  {
    "objectID": "environR.html#installing-packages",
    "href": "environR.html#installing-packages",
    "title": "R",
    "section": "Installing packages",
    "text": "Installing packages\nIn R, packages are collections of functions, compiled code and sample data. They functionally act as “extensions” to the base R language, and can help you accomplish all operations you might want to perform in R (if no package serves your purpose, you may want to write an entirely new one!). Now, we will install the R package tidyverse. Look at the link to see what tidyverse includes, and directly load a .csv file (comma-separated values) into R from your computer.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "environR.html#resources",
    "href": "environR.html#resources",
    "title": "R",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nR for Data Science. R4DS teaches you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it.\nSpatial Data Science by Edzer Pebesma and Roger Bivand introduces and explains the concepts underlying spatial data.\nGeo-computation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow."
  },
  {
    "objectID": "environR.html#r-basics",
    "href": "environR.html#r-basics",
    "title": "R",
    "section": "R Basics",
    "text": "R Basics\n\nStarting a session\nUpon startup, RStudio will look something like this. Note: the Pane Layout and Appearance settings can be altered e.g. on Mac OS by clicking RStudio>Preferences>Appearance and RStudio>Preferences>Pane Layout. I personally like to have my Console in the top right corner and Environment in the bottom left and keep the Source and Environment panes wider than Console and Files for easier readability. Default settings will probably have the Console in the bottom left and Environment in the top right. You will also have a standard white background; but you can chose specific themes.\n\n\n\n\n\nAt the start of a session, it’s good practice clearing your R environment:\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set. You should have set this following the pre-recorded video.\n\ngetwd() \n\nIf the directory is not set yet, type in setwd(\"~/pathtodirectory\") to set it. It is crucial to perform this step at the beginning of your R script, so that relative paths can be used in the subsequent parts.\n\nsetwd(\"~/Dropbox/Github/gds\")\n\nIf you have set your directory correctly, it will show up at the top of the console pane:\n\n\n\n\n\nImportant: You do not need to set your working directory if you are using an R-markdown or Quarto document and you have it saved in the right location. The pathway will start from where your document is saved.\n\n\nUsing the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\nR Objects\nEverything in R is an object. R possesses a simple generic function mechanism which can be used for an object-oriented style of programming. Indeed, everything that happens in R is the result of a function call (John M. Chambers). Method dispatch takes place based on the class of the first argument to the generic function.\nAll R statements where you create objects – “assignments” – have this form: object_name <- value. Assignment can also be performed using = instead of <-, but the standard advice is to use the latter syntax (see e.g. The R Inferno, ch. 8.2.26). In RStudio, the standard shortcut for the assignment operator <- is Alt + - (in Windows) or option + - (in Mac OS).\nA mock assignment of the value 30 to the name age is reported below. In order to inspect the content of the newly created variable, it is sufficient to type the name into the console. Within R, the hash symbol # is used to write comments and create collapsible code sections.\n\nage <- 30 # Assign the number 30 to the name \"age\"\nage # print the variable \"age\" to the console\n\n[1] 30\n\n\n\n\nA small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing <- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA\n\n\n\n\nLogical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n> greater than\n>= greater or equal to\n<= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not"
  },
  {
    "objectID": "environR.html#a-small-note-on-variable-types",
    "href": "environR.html#a-small-note-on-variable-types",
    "title": "R",
    "section": "A small note on variable types",
    "text": "A small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing <- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA"
  },
  {
    "objectID": "environR.html#logical-operators-and-expressions",
    "href": "environR.html#logical-operators-and-expressions",
    "title": "R",
    "section": "Logical operators and expressions",
    "text": "Logical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n> greater than\n>= greater or equal to\n<= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not"
  },
  {
    "objectID": "environR.html#examples",
    "href": "environR.html#examples",
    "title": "R",
    "section": "Examples",
    "text": "Examples\nLet’s create some random R objects:\n\n## Entering random \nLondon  <- 8982000 # population\nBristol <- 467099 # population\nLondon_area <-1572 # area km2\nBristol_area <-110 # area km2\n\nLondon\n\n[1] 8982000\n\n\nCalculate Population Density in London:\n\nLondon_pop_dens <- London/London_area\nBristol_pop_dens <- Bristol/Bristol_area\n\nLondon_pop_dens\n\n[1] 5713.74\n\n\nThe function c(), which you will use extensively if you keep coding in R, means “concatenate”. In this case, we use it to create a vector of population densities for London and Bristol:\n\nc(London_pop_dens, Bristol_pop_dens)\n\n[1] 5713.740 4246.355\n\npop_density <- c(London_pop_dens, Bristol_pop_dens) # In order to create a vector in R we make use of c() (which stands for concatenate)\n\nCreate a character variable:\n\nx <- \"a city\"\nclass(x)\n\n[1] \"character\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1"
  },
  {
    "objectID": "environR.html#data-structures",
    "href": "environR.html#data-structures",
    "title": "R",
    "section": "Data Structures",
    "text": "Data Structures\nObjects in R are typically stored in data structures. There are multiple types of data structures:\n\nVectors\nIn R, a vector is a sequence of elements which share the same data type. A vector supports logical, integer, double, character, complex, or raw data types.\n\n# first vector y\ny <- 1:10\nas.numeric(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\n# another vector z\nz <- c(2, 4, 56, 4)\nz\n\n[1]  2  4 56  4\n\n# and another one called cities\ncities <- c(\"London\", \"Bristol\", \"Bath\")\ncities\n\n[1] \"London\"  \"Bristol\" \"Bath\"   \n\n\n\n\nMatrices\nTwo-dimensional, rectangular, and homogeneous data structures. They are similar to vectors, with the additional attribute of having two dimensions: the number of rows and columns.\n\nm <- matrix(nrow = 2, ncol = 2)\nm\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\nn <- matrix(c(4, 5, 78, 56), nrow = 2, ncol = 2 )\nn\n\n     [,1] [,2]\n[1,]    4   78\n[2,]    5   56\n\n\n\n\nLists\nLists are containers which can store elements of different types and sizes. A list can contain vectors, matrices, dataframes, another list, functions which can be accessed, unlisted, and assigned to other objects.\n\nlist_data <- list(\"Red\", \"Green\", c(21,32,11), TRUE, 51.23, 119.1)\nprint(list_data)\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] 51.23\n\n[[6]]\n[1] 119.1\n\n\n\n\nData frames\nThey are the most common way of storing data in R and are the most used data structure for statistical analysis. Data frames are “rectangular lists”, i.e. tabular structures in which every element has the same length, and can also be thought of as lists of equal length vectors.\n\n## Here is a data frame of 3 columns named id, x, y and 10 rows\ndat <- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nhead(dat) # read first 5 rows\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ntail(dat)\n\n   id  x  y\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n\nDataframes in R are indexed by rows and columns numbers using the [rows,cols] syntax. The $ operator allows you to access columns in the dataframe, or to create new columns in the dataframe.\n\ndat[1,] # read first row and all colum ns\n\n  id x  y\n1  a 1 11\n\ndat[,1] # read all rows and the first column\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\ndat[6,3] # read 6th row, third column\n\n[1] 16\n\ndat[c(2:4),] # read rows 2 to 4 and all columns\n\n  id x  y\n2  b 2 12\n3  c 3 13\n4  d 4 14\n\ndat$y # read column y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat[dat$x<7,] # read rows that have a x value less than 7\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndat$new_column <- runif(10, 0, 1) # create a new variable called \"new_column\"\n\ndat\n\n   id  x  y new_column\n1   a  1 11 0.24409650\n2   b  2 12 0.03560325\n3   c  3 13 0.82042254\n4   d  4 14 0.80079558\n5   e  5 15 0.25702070\n6   f  6 16 0.94972479\n7   g  7 17 0.15214774\n8   h  8 18 0.98108283\n9   i  9 19 0.48167385\n10  j 10 20 0.22331529"
  },
  {
    "objectID": "environR.html#exercises-1",
    "href": "environR.html#exercises-1",
    "title": "R",
    "section": "Exercises 1",
    "text": "Exercises 1\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector <- c(\"Elisabetta\", \"Capucine\", \"Lorenzo\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform <- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 1.36448262 0.94936261 1.13249667 0.01386841 1.14539223 1.34002270\n [7] 2.38620325 0.13660425 0.56899676 0.37095379\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector <- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix <-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df <-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\nlibrary(tidyverse)\n\n\n\nShow the code\nnew_df <-  new_df %>%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %>%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714"
  },
  {
    "objectID": "spatialdata_code.html#visual-exploration",
    "href": "spatialdata_code.html#visual-exploration",
    "title": "Spatial Data",
    "section": "Visual exploration",
    "text": "Visual exploration"
  },
  {
    "objectID": "openscience.html#additional-lab-materials",
    "href": "openscience.html#additional-lab-materials",
    "title": "OpenScience",
    "section": "Additional lab materials",
    "text": "Additional lab materials\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis”\nA good introduction to data manipulation in R is the “Data wrangling” chapter in R for Data Science.\nA good extension is Hadley Wickham’ “Tidy data” paper which presents a very popular way of organising tabular data for efficient manipulation."
  },
  {
    "objectID": "openscience.html#additional-resources",
    "href": "openscience.html#additional-resources",
    "title": "Lab",
    "section": "Additional resources",
    "text": "Additional resources\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis”\nA good introduction to data manipulation in R is the “Data wrangling” chapter in R for Data Science.\nA good extension is Hadley Wickham’ “Tidy data” paper which presents a very popular way of organising tabular data for efficient manipulation."
  },
  {
    "objectID": "environR.html#exercises",
    "href": "environR.html#exercises",
    "title": "R",
    "section": "Exercises",
    "text": "Exercises\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector <- c(\"Elisabetta\", \"Carmen\", \"Habib\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform <- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 1.1175108 2.4480687 2.4927297 2.7243597 2.1288363 0.8949736 1.7117790\n [8] 1.9318872 2.1184755 2.8638416\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector <- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix <-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df <-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\n\nShow the code\nnew_df <-  new_df %>%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %>%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714"
  },
  {
    "objectID": "environR.html#import-data-from-csv",
    "href": "environR.html#import-data-from-csv",
    "title": "R",
    "section": "Import data from csv",
    "text": "Import data from csv\n\nDensities_UK_cities <- read_csv(\"data/London/Tables/Densities_UK_cities.csv\")\n\nRows: 76 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): city, pop\ndbl (1): n\nnum (2): area, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nDensities_UK_cities\n\n# A tibble: 76 × 5\n       n city               pop        area density\n   <dbl> <chr>              <chr>     <dbl>   <dbl>\n 1     1 Greater London     9,787,426 1738.    5630\n 2     2 Greater Manchester 2,553,379  630.    4051\n 3     3 West Midlands      2,440,986  599.    4076\n 4     4 West Yorkshire     1,777,934  488.    3645\n 5     5 Greater Glasgow    957,620    368.    3390\n 6     6 Liverpool          864,122    200.    4329\n 7     7 South Hampshire    855,569    192     4455\n 8     8 Tyneside           774,891    180.    4292\n 9     9 Nottingham         729,977    176.    4139\n10    10 Sheffield          685,368    168.    4092\n# … with 66 more rows\n\n\nYou can also view the data set with:\n\nglimpse(Densities_UK_cities)\n\nRows: 76\nColumns: 5\n$ n       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ city    <chr> \"Greater London\", \"Greater Manchester\", \"West Midlands\", \"West…\n$ pop     <chr> \"9,787,426\", \"2,553,379\", \"2,440,986\", \"1,777,934\", \"957,620\",…\n$ area    <dbl> 1737.9, 630.3, 598.9, 487.8, 368.5, 199.6, 192.0, 180.5, 176.4…\n$ density <dbl> 5630, 4051, 4076, 3645, 3390, 4329, 4455, 4292, 4139, 4092, 42…\n\ntable(Densities_UK_cities$city)\n\n\n               Aberdeen  Accrington/ Rossendale Barnsley/ Dearne Valley \n                      1                       1                       1 \n               Basildon             Basingstoke                 Bedford \n                      1                       1                       1 \n                Belfast              Birkenhead               Blackburn \n                      1                       1                       1 \n              Blackpool      Bournemouth/ Poole       Brighton and Hove \n                      1                       1                       1 \n                Bristol                 Burnley       Burton-upon-Trent \n                      1                       1                       1 \n              Cambridge                 Cardiff              Chelmsford \n                      1                       1                       1 \n             Cheltenham            Chesterfield              Colchester \n                      1                       1                       1 \n               Coventry                 Crawley                   Derby \n                      1                       1                       1 \n              Doncaster                  Dundee              Eastbourne \n                      1                       1                       1 \n              Edinburgh                  Exeter  Farnborough/ Aldershot \n                      1                       1                       1 \n             Gloucester         Greater Glasgow          Greater London \n                      1                       1                       1 \n     Greater Manchester                 Grimsby                Hastings \n                      1                       1                       1 \n           High Wycombe                 Ipswich      Kingston upon Hull \n                      1                       1                       1 \n              Leicester                 Lincoln               Liverpool \n                      1                       1                       1 \n                  Luton               Maidstone               Mansfield \n                      1                       1                       1 \n           Medway Towns           Milton Keynes              Motherwell \n                      1                       1                       1 \n                Newport             Northampton                 Norwich \n                      1                       1                       1 \n             Nottingham                  Oxford       Paignton/ Torquay \n                      1                       1                       1 \n           Peterborough                Plymouth                 Preston \n                      1                       1                       1 \n                Reading               Sheffield                  Slough \n                      1                       1                       1 \n        South Hampshire         Southend-on-Sea          Stoke-on-Trent \n                      1                       1                       1 \n             Sunderland                 Swansea                 Swindon \n                      1                       1                       1 \n               Teesside                 Telford                  Thanet \n                      1                       1                       1 \n               Tyneside              Warrington           West Midlands \n                      1                       1                       1 \n         West Yorkshire                   Wigan               Worcester \n                      1                       1                       1 \n                   York \n                      1"
  },
  {
    "objectID": "mapvector.html#visual-exploration",
    "href": "mapvector.html#visual-exploration",
    "title": "3 Mapping Vector Data",
    "section": "Visual exploration",
    "text": "Visual exploration"
  },
  {
    "objectID": "spatialdata_code.html",
    "href": "spatialdata_code.html",
    "title": "Lab",
    "section": "",
    "text": "Python\nAlthough not too complicated, the way to access borders in geopandas is not as straightforward as it is the case for other aspects of the map, such as size or frame."
  },
  {
    "objectID": "spatialdata_code.html#installing-packages",
    "href": "spatialdata_code.html#installing-packages",
    "title": "Lab",
    "section": "Installing packages",
    "text": "Installing packages\nWe will start by loading core packages for working with spatial data. See detailed description of R and Python.\n\nRPython\n\n\n\n# Load the 'sf' library, which stands for Simple Features, used for working with spatial data.\nlibrary(sf)\n# Load the 'tidyverse' library, a collection of packages for data manipulation and visualization.\nlibrary(tidyverse)\n# Load the 'tmap' library, which is used for creating thematic maps and visualizing spatial data.\nlibrary(tmap)\n# The 'readr' library provides a fast and user-friendly way to read data from common formats like CSV.\nlibrary(readr)\n# Converts Between GeoJSON and simple feature objects\nlibrary(geojsonsf) \n# Using data from OpenStreetMap (OSM)\nlibrary(osmdata)\n\n\n\n\n#import pandas as pd\n#from shapely import Point\n#import geopandas as gpd"
  },
  {
    "objectID": "spatialdata_code.html#datasets",
    "href": "spatialdata_code.html#datasets",
    "title": "Lab",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\nRPython\n\n\n\npoi_df = tribble(\n  ~name, ~lon, ~lat,\n  \"The British Museum\",        -0.1459604, 51.5045975,\n  \"Big Ben\",    -0.1272057, 51.5007325,\n  \"King's Cross\", -0.1319481, 51.5301701,\n  \"The Natural History Museum\",     -0.173734, 51.4938451\n)\npoi_sf = sf::st_as_sf(poi_df, coords = c(\"lon\", \"lat\"), crs = \"EPSG:4326\")\n\n\n\n\n#poi = gpd.GeoDataFrame([\n#    {\"name\": \"The British Museum\",        \"geometry\": Point(-0.1459604, 51.5045975)},\n#    {\"name\": \"Big Ben\",    \"geometry\": Point(-0.1272057, 51.5007325)},\n#    {\"name\": \"King's Cross\", \"geometry\": Point(-0.1319481, 51.5301701)},\n#    {\"name\": \"The Natural History Museum\",     \"geometry\": Point(-0.173734, 51.4938451)},\n#], crs=4326)\n\n\n\n\n\n\nPolygons\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPython\n\n\n\n\n\n\n\n\nLines\nWe them import a geojson file of roads in London and plot it.\n\nRPython\n\n\n\na_roads <- read_sf(\"data/London/Lines/a_roads.shp\")\n#a_roads <- geojson_sf(\"data/London/Lines/a_roads.geojson\")\n\nplot(a_roads$geometry)\n\n\n\n\n\n\n\n\n\n\n\n\nPoints\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science lab or other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\n\nRPython\n\n\n\nosm_q_sf <- opq(\"Greater London, U.K.\") %>% # searching only in Greater London\n    add_osm_feature(key = \"building\", value = \"museum\") %>% #adding osm data that is tagged as a museum\n  osmdata_sf () # transforming to sf object\n\nThe structure of osmdata objects are clear from their default print method, illustrated using the museum example. We will use them shortly.\n\nosm_q_sf  \n\nObject of class 'osmdata' with:\n                 $bbox : 51.2867601,-0.5103751,51.6918741,0.3340155\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 206 points\n            $osm_lines : NULL\n         $osm_polygons : 'sf' Simple Features Collection with 8 polygons\n       $osm_multilines : NULL\n    $osm_multipolygons : 'sf' Simple Features Collection with 1 multipolygons\n\n\n\n\nIn Python we will use osmnx. Note that we use the method pois_from_place, which queries for points of interest (POIs, or pois) in a particular place (London in this case). In addition, we can specify a set of tags to delimit the query. We use this to ask only for amenities of the type “…”\n\n\n\n\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “museums in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote the code cell above requires internet connectivity.\nImportant: Be careful, if you query too much data, your environment is likely to get stuck."
  },
  {
    "objectID": "spatialdata_code.html#inspecting-spatial-data",
    "href": "spatialdata_code.html#inspecting-spatial-data",
    "title": "Lab",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting Dataframes\nJust like a Dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within this spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non sptaial dataframes.\nWe can see our data is very similar to a traditional, non-spatial DataFrame, but with an additional column called geometry.\n\nRPython\n\n\n\nhead(districts) # the command \"head\" reads the first 5 rows of the data\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 515484.9 ymin: 156480.8 xmax: 554503.8 ymax: 198355.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  <chr>     <chr>                                                  <POLYGON [m]>\n1 00AA      City of London       ((531028.5 181611.2, 531036.1 181611.5, 531074…\n2 00AB      Barking and Dagenham ((550817 184196, 550814 184189.1, 550799 18416…\n3 00AC      Barnet               ((526830.3 187535.5, 526830.3 187535.4, 526829…\n4 00AD      Bexley               ((552373.5 174606.9, 552372.9 174603.9, 552371…\n5 00AE      Brent                ((524661.7 184631, 524665.3 184626.4, 524667.9…\n6 00AF      Bromley              ((533852.2 170129, 533850.4 170128.5, 533844.9…\n\n\nWe can inspect the object in different ways :\n\ndistricts[1,] # read first row\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  <chr>     <chr>                                                  <POLYGON [m]>\n1 00AA      City of London ((531028.5 181611.2, 531036.1 181611.5, 531074 18161…\n\ndistricts[,1] # read first column\n\nSimple feature collection with 33 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.1 ymin: 155850.8 xmax: 561957.4 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 33 × 2\n   DIST_CODE                                                            geometry\n   <chr>                                                           <POLYGON [m]>\n 1 00AA      ((531028.5 181611.2, 531036.1 181611.5, 531074 181610.3, 531107 18…\n 2 00AB      ((550817 184196, 550814 184189.1, 550799 184162.6, 550797.2 184159…\n 3 00AC      ((526830.3 187535.5, 526830.3 187535.4, 526829 187534.7, 526825.3 …\n 4 00AD      ((552373.5 174606.9, 552372.9 174603.9, 552371 174595.3, 552367.9 …\n 5 00AE      ((524661.7 184631, 524665.3 184626.4, 524667.9 184623.1, 524673.5 …\n 6 00AF      ((533852.2 170129, 533850.4 170128.5, 533844.9 170127.9, 533842.6 …\n 7 00AG      ((531410.7 181576.1, 531409.4 181573.1, 531409.4 181573.1, 531405 …\n 8 00AH      ((532745.1 157404.6, 532756.3 157394.6, 532768.1 157384, 532777.3 …\n 9 00AJ      ((512740.6 182181.6, 512740.1 182185.9, 512739.5 182190.5, 512739.…\n10 00AK      ((530417 191627.4, 530416.8 191627.6, 530410 191631.2, 530396.3 19…\n# … with 23 more rows\n\ndistricts[1,1] #read first row, first column: 00AA\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 2\n  DIST_CODE                                                             geometry\n  <chr>                                                            <POLYGON [m]>\n1 00AA      ((531028.5 181611.2, 531036.1 181611.5, 531074 181610.3, 531107 181…\n\n# variable can be called using the operator $\ndistricts$DIST_NAME #read the column \"DIST_NAME\"\n\n [1] \"City of London\"         \"Barking and Dagenham\"   \"Barnet\"                \n [4] \"Bexley\"                 \"Brent\"                  \"Bromley\"               \n [7] \"Camden\"                 \"Croydon\"                \"Ealing\"                \n[10] \"Enfield\"                \"Greenwich\"              \"Hackney\"               \n[13] \"Hammersmith and Fulham\" \"Haringey\"               \"Harrow\"                \n[16] \"Havering\"               \"Hillingdon\"             \"Hounslow\"              \n[19] \"Islington\"              \"Kensington and Chelsea\" \"Kingston upon Thames\"  \n[22] \"Lambeth\"                \"Lewisham\"               \"Merton\"                \n[25] \"Newham\"                 \"Redbridge\"              \"Richmond upon Thames\"  \n[28] \"Southwark\"              \"Sutton\"                 \"Tower Hamlets\"         \n[31] \"Waltham Forest\"         \"Wandsworth\"             \"Westminster\"           \n\n\nWe can read or create subsets :\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\ndistricts[districts$DIST_NAME== \"City of London\",] \n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  <chr>     <chr>                                                  <POLYGON [m]>\n1 00AA      City of London ((531028.5 181611.2, 531036.1 181611.5, 531074 18161…\n\n\n\n\n\n\n\n\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\nRPython\n\n\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# plot london in grey\nplot(districts$geometry, col = \"lightgrey\")\n\n# Add city of London in turquoise to the map\nplot(districts[districts$DIST_NAME == \"Hackney\", ]$geometry, # select city of london\n     col = \"turquoise\",\n     add = T) # add to the existing map\n\n\n\n\nSome guidance on colours in R can be found here.\nHow to reset a plot:\n\nplot(districts$geometry, reset = T) # reset"
  },
  {
    "objectID": "spatialdata_code.html#styling-plots",
    "href": "spatialdata_code.html#styling-plots",
    "title": "Lab",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\nRPython\n\n\n\nplot(districts$geometry)\nplot(a_roads$geometry, add=T) # note the `add=T` is adding the second layer.\n\n\n\n\nOr use the ggplot package for something a bit fancier\n\nggplot() +\n geom_sf(data = districts, color = \"black\") +  # Plot districts with black outline\n  geom_sf(data = a_roads, color = \"brown\") +  # Plot roads with brown color and 50% transparency\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\nRPython\n\n\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\nRPython\n\n\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme(line = element_blank(), # remove tick marks\n        rect = element_blank(), # remove background\n        axis.text=element_blank()) # remove x and y axis\n\n\n\n  # theme_void() # could also be used instead of the 3 above lines \n\nFor more on themes in ggplot see here\n\n\n\n\n\n\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\nRPython\n\n\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme_void() + # \n  ggtitle(\"Some London Roads\") #add ggtitle\n\n\n\n\n\n\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\nR\n\n\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  \n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) + \n  geom_sf(data = poi_sf, color = \"blue\", size = 3) + # size adjusts size of visualization\n  theme_void() +\n  ggtitle(\"Some London Roads\") #add ggtitle\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\n\nRPython\n\n\nHere we are using geom_sf_text to add data, specifically the distrct name, to the centre of each District in a specific size.\n\nggplot() +\n  geom_sf(data = districts,\n          fill = \"gray95\") +\n  geom_sf_text(data = districts,\n               aes(label = DIST_NAME),\n               fun.geometry = sf::st_centroid, size=2) +\n  theme_void()\n\n\n\n\ngeom_sf_text() and geom_sf_label() can also be used to achieve similar effects."
  },
  {
    "objectID": "spatialdata_code.html#coordinate-reference-systems",
    "href": "spatialdata_code.html#coordinate-reference-systems",
    "title": "Lab",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\nRPython\n\n\n\nst_crs(districts) # retrieve coordinate reference system from object\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nThe st_crs function also has one helpful feature – we can retrieve some additional information about the used CRS. For example, try to run:\n\nst_crs(districts)$IsGeographic # to check is the CRS is geographic or not\n\n[1] FALSE\n\nst_crs(districts)$units_gdal # to find out the CRS units\n\n[1] \"metre\"\n\nst_crs(districts)$srid # extracts its SRID (when available)\n\n[1] \"EPSG:27700\"\n\nst_crs(districts)$proj4string # extracts the proj4string representation\n\n[1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n\n\n\n\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\n\n\n\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\n\nRPython\n\n\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the st_set_crs() function can be used:\n\ndistricts_4326 = st_transform(districts, \"EPSG:4326\") # set CRS\n# districts_4326 <- st_transform(districts_4326, crs = 4326)\n\n\n\n\n\n\n\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a spatial datafrane. For example we have some London housing transactions we want to import and use.\n\nRPython\n\n\nWe want to transform the .csv in a sf object with the st_as_sf function using the coordinates stored in columns 17 and 18, and then we set the dataframe CRS to the British National Grid (EPSG:27700) using the st_set_crs function.\n\nhousesales <- read.csv(\"data/London/Tables/housesales.csv\") # import housesales data from csv\n\n# 3 commands: \nhousesales_filtered = filter(housesales,price < 500000)\nhousesales_sf <- st_as_sf(housesales_filtered, coords = c(17,18)) # denote columns which have the coordinates\nhousesales_clean <- st_set_crs(housesales_sf, 27700)# set crs to British National Grid \n\nAs we’ve seen in open science, we can do consecutive operations using dplyr pipes %>%, they are used to simplify syntax. Pipes allow to perform successive operations on dataframes in one command! More info here.\n\n# all one in go and one output\nhousesales_clean = housesales %>% # select the main object\n  filter(price < 500000) %>% # remove values above 500,000\n  st_as_sf(coords = c(17,18)) %>% # # denote columns which have the coordinates\n  st_set_crs(27700) # set crs to British National Grid\n\n\n\n\n\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\nRPython\n\n\n\nggplot() + \n geom_sf(data = districts_4326$geometry) + \n  geom_sf(data = poi_sf$geometry, fill = 'red') +\n  coord_sf(xlim = c(-0.180723,-0.014212), ylim = c(51.476668,51.532337)) +\n   theme_void()"
  },
  {
    "objectID": "spatialdata_code.html#manipulating-spatial-tables",
    "href": "spatialdata_code.html#manipulating-spatial-tables",
    "title": "Lab",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a geographical dataframes contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial dataframes (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, geo dataframes also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeo dataframes come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nArea\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the dataframe you are working with is projected. If that is the case, you can calculate areas as follows:\n\nRPython\n\n\nWe had already checked that district was projected to the British National Grid\n\ndistricts <- districts %>%\n  mutate(area = st_area(.)/1000000) # calculate area and make it km2\n\n\n\n\n\n\n\n\n\nLength\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\nRPython\n\n\n\na_roads <- a_roads %>%\n  mutate(street_length = st_length(geometry)) # calculate street length in metres\n\nIf you check the dataframe you will see the lengths.\n\n\n\n\n\n\n\n\nCentroids\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\nRPython\n\n\n\n# Create a dataframe with centroids\ncentroids_df <- districts %>%\n  st_centroid()\n\nPlot the centroids\n\nggplot() +\n  geom_sf(data = districts) +  # Plot the districts segments\n  geom_sf(data = centroids_df, color = \"red\", size = 2) +  # Plot the centroids in red\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nBuffers and selecting by location\n\nRPython\n\n\nHere, we first select by expression the Hackney district and then we create a 1km buffer around it with the st_buffer() function from the sf package.\n\n# Select a district of London and make new object\nhackney = districts[districts$DIST_NAME== \"Hackney\",] \nplot(hackney$geometry)\n\n\n\n# buffer\nhackney_buffer <- st_buffer(hackney, 1000)\n\nplot(hackney_buffer$geometry)\nplot(hackney$geometry, add=T)\n\n\n\n\n\n\n\n\n\n\n\n\nJoin districts with educational level data\n\nRPython\n\n\n\n# import qualifications data from csv\nqualifications2001_df <- read.csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# take a quick look at the table by reading the first 5 lines\nhead(qualifications2001_df)\n\n  Zone_Code            Zone_Name Population1674 Noquals Level1 Level2 Level3\n1      00AA       City of London           6067     607    359    634    665\n2      00AB Barking and Dagenham         113579   44873  21654  20564   6626\n3      00AC               Barnet         228123   44806  25558  41118  24695\n4      00AD               Bexley         156172   44887  32110  35312  10759\n5      00AE                Brent         198712   48915  23913  33280  21121\n6      00AF              Bromley         212368   47093  34879  48012  19550\n  Level4\n1   3647\n2  11615\n3  80907\n4  20704\n5  60432\n6  49598\n\n\n\nInstall the dplyr package, which is a must have package for data cleaning. More info can be found here. dplyr is a part of the tidyverse!\nJoin merge two datasets join(x, y).\n\nleft_join returns all rows from x (districts), and all columns from x (districts) and y (qualifications2001)\ninner join returns all rows from x where there are matching values in y, and all columns from x and y)\nright join returns all rows from x, and all columns from x and y)\nfull_join returns all rows and all columns from both x and y)\n\nMerge the data from the districts shapefile and the qualifications from the csv file\nJoin districts data to qualifications2001 using district identifiers called DIST_CODE in districts and Zone_Code in qualifications2001_df\n\n\n#join\ndistricts <- left_join(districts, \n                       qualifications2001_df, \n                       by=c(\"DIST_CODE\"=\"Zone_Code\"))\n\n# tidyverse alternative with pipe operator %>%\n\ndistricts_tidy <- districts %>%\n  left_join(qualifications2001_df, by=c(\"DIST_CODE\"=\"Zone_Code\"))\n\n# check the first rows of the merged data table\nhead(districts)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 515484.9 ymin: 156480.8 xmax: 554503.8 ymax: 198355.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 11\n  DIST_CODE DIST_NAME                    geometry   area Zone_…¹ Popul…² Noquals\n  <chr>     <chr>                   <POLYGON [m]>  [m^2] <chr>     <int>   <int>\n1 00AA      City of Lo… ((531028.5 181611.2, 531…   3.15 City o…    6067     607\n2 00AB      Barking an… ((550817 184196, 550814 …  37.8  Barkin…  113579   44873\n3 00AC      Barnet      ((526830.3 187535.5, 526…  86.7  Barnet   228123   44806\n4 00AD      Bexley      ((552373.5 174606.9, 552…  64.3  Bexley   156172   44887\n5 00AE      Brent       ((524661.7 184631, 52466…  43.2  Brent    198712   48915\n6 00AF      Bromley     ((533852.2 170129, 53385… 150.   Bromley  212368   47093\n# … with 4 more variables: Level1 <int>, Level2 <int>, Level3 <int>,\n#   Level4 <int>, and abbreviated variable names ¹​Zone_Name, ²​Population1674\n\n\nNow, as in Workshop 2, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\ndistricts <- districts %>%\n  mutate(Level4p = Level4/Population1674)"
  },
  {
    "objectID": "spatialdata_code.html#saving-maps-to-figures",
    "href": "spatialdata_code.html#saving-maps-to-figures",
    "title": "Lab",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\n\nRPython\n\n\nhttps://intro2r.com/export-plots.html\n\ndir.create(\"maps\") # create a file to put your maps\n\nWarning in dir.create(\"maps\"): 'maps' already exists\n\n\n\npdf(\"maps/london_test.pdf\") # Opening the graphical device\nplot(districts$geometry)\nplot(housesales_clean$geometry, add=TRUE) \ndev.off() #Closing the graphical device\n\nquartz_off_screen \n                2 \n\n\n\nggsave(\"maps/map3.pdf\")\n\nSaving 7 x 5 in image\n\nggsave(\"maps/map3_1.png\", width = 4, height = 4)\nggsave(\"maps/map3_2.png\", width = 20, height = 20, units = \"cm\")\n#https://ggplot2.tidyverse.org/reference/ggsave.html"
  },
  {
    "objectID": "spatialdata_code.html#adding-base-layers-from-web-sources",
    "href": "spatialdata_code.html#adding-base-layers-from-web-sources",
    "title": "Lab",
    "section": "Adding base layers from web sources",
    "text": "Adding base layers from web sources\nAdd in"
  },
  {
    "objectID": "spatialdata_code.html#interactive-maps",
    "href": "spatialdata_code.html#interactive-maps",
    "title": "Lab",
    "section": "Interactive maps",
    "text": "Interactive maps\n\nRPython\n\n\n\nlibrary(leaflet)\npopup = c(\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\")\nleaflet() %>%\n  addProviderTiles(\"CartoDB.Positron\") %>%\n  addMarkers(lng = c(-0.1459604, -0.1272057, -0.1319481, -0.173734),\n             lat = c(51.5045975, 51.5007325, 51.5301701, 51.4938451), \n             popup = popup)"
  },
  {
    "objectID": "spatialdata_code.html#additional-resources",
    "href": "spatialdata_code.html#additional-resources",
    "title": "Lab",
    "section": "Additional resources",
    "text": "Additional resources"
  },
  {
    "objectID": "intro.html#open-source-gis",
    "href": "intro.html#open-source-gis",
    "title": "1 Introduction",
    "section": "Open Source GIS",
    "text": "Open Source GIS\nOpen source Geographic Information Systems (GIS), such as QGIS, have made geographic analysis accessible worldwide. GIS programs tend to emphasize graphical user interfaces (GUIs), with the unintended consequence of discouraging reproducibility (although many can be used from the command line Python + QGIS). R and Python by contrast, emphasizes the command line interface (CLI).\nThe ‘geodata revolution’ drives demand for high performance computer hardware and efficient, scalable software to handle and extract signal from the noise, to understand and perhaps change the world. Spatial databases enable storage and generation of manageable subsets from the vast geographic data stores, making interfaces for gaining knowledge from them vital tools for the future.\nR and Python are both tools with advanced modeling and visualization capabilities."
  },
  {
    "objectID": "mapraster.html",
    "href": "mapraster.html",
    "title": "4 Mapping Raster Data",
    "section": "",
    "text": "Raster data is more and more popular…"
  },
  {
    "objectID": "mapvector_code.html",
    "href": "mapvector_code.html",
    "title": "Lab",
    "section": "",
    "text": "# Load the 'sf' library, which stands for Simple Features, used for working with spatial data.\nlibrary(sf)\n# Load the 'tidyverse' library, a collection of packages for data manipulation and visualization.\nlibrary(tidyverse)\n# Load the 'tmap' library, which is used for creating thematic maps and visualizing spatial data.\nlibrary(tmap)\n# The 'readr' library provides a fast and user-friendly way to read data from common formats like CSV.\nlibrary(readr)\n# Converts Between GeoJSON and simple feature objects\nlibrary(geojsonsf) \n# Using data from OpenStreetMap (OSM)\nlibrary(osmdata)\n\n\ndistricts &lt;- read_sf(\"data/London/Polygons/districts.shp\")\n\n# import qualifications data from csv\nqualifications2001_df &lt;- read.csv(\"data/London/Tables/qualifications2001_2.csv\")\n\ndistricts_tidy &lt;- districts %&gt;%\n  left_join(qualifications2001_df, by=c(\"DIST_CODE\"=\"Zone_Code\")) %&gt;%\n  mutate(Level4p = Level4/Population1674)\n\nAnd finally, we can map the results! We will be using both the tmap (specific for maps, see here), the ggplot2 (for general data visualisations, see here and the mapsf (for thematic cartography, see here) packages:\n\n#3a. plot the new variable using the tmap package\nif(!require(\"tmap\")) install.packages(\"tmap\")\nlibrary(\"tmap\")\nqtm(districts_tidy, \"Level4p\") # Quick thematic map plot\n\n\n\n#3b. Use ggplot to create the same map of London\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(\"ggplot2\")\n\nggplot() +\n  geom_sf(data = districts_tidy, aes(fill = Level4p)) +\n  theme_void() \n\n\n\n#3c. Use mapsf to create the same map of London\nif(!require(\"mapsf\")) install.packages(\"mapsf\")\n\nLoading required package: mapsf\n\nlibrary(\"mapsf\") \n\n# Plot the base map\nmf_map(x = districts_tidy)\n# Plot with the data\nmf_map(x = districts_tidy, var = \"Level4p\", type = \"choro\",\n       pal = \"Dark Mint\", \n       breaks = \"quantile\", \n       nbreaks = 6, \n       leg_title = \"% of highest edu\", \n       add = TRUE)"
  }
]