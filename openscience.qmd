# OpenScience {#sec-open-science .unnumbered}

Now that we know a bit about what computational notebooks are and why we should care about them, let's jump to using them! This section introduces you to using R or Python for manipulating tabular data. Please read through it carefully and pay attention to how ideas about manipulating data are translated into code. For this part, you can read directly from the course website, although it is recommended you follow the section interactively by running the code on your own.

Once you have read through and have a bit of a sense of how things work, jump on the [Do-It-Yourself](https://pietrostefani.github.io/gds/openscienceDIY.html) section, which will provide you with a challenge to complete it on your own, and will allow you to put what you have already learnt to good use.

## Data wrangling

Real world datasets are messy. There is no way around it: datasets have "holes" (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to wrangle (manipulating, transforming & structuring) them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but has to do with much more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.

In this session, you will use a few real world datasets and learn how to process them in R or Python so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the bread and butter of data analysis and scientific computing. These are fundamental tools that are constantly used in almost any task relating to data analysis.

This notebook covers the basic and the content that is expected to be learnt by every student. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at.

In this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.

Before we get our hands data-dirty, let us import all the additional libraries we will need, so we can get that out of the way and focus on the task at hand:

## Loading packages

We will start by loading core packages for working with geographic vector and attribute data.

::: {.panel-tabset group="language"}
## R

```{r message=FALSE}
library(sf)
library(tidyverse)
library(data.table)
```

## Python
:::

## Datasets

We will be exploring some demographic characteristics in Liverpool. To do that, we will use a dataset that contains population counts, split by ethnic origin. These counts are aggregated at the Lower Layer Super Output Area (LSOA from now on). LSOAs are an official Census geography defined by the Office of National Statistics. You can think of them, more or less, as neighbourhoods. Many data products (Census, deprivation indices, etc.) use LSOAs as one of their main geographies.

To make things easier, we will read data from a file posted online so, for now, you do not need to download any dataset:

**Import housesales data from csv**

::: {.panel-tabset group="language"}
## R

```{r}
census2021 <- read.csv("data/census2021_ethn/liv_pop.csv", row.names = "GeographyCode")
```

Let us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:

- We are using the method `read.csv` from base R, you could also use `read_csv` from `library("readr")`

- Here the csv is based in the file data but it could also be a web address or sometimes you find data in packages

- The argument row.names is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.

- We are using read.csv because the file we want to read is in the csv format. However, many more formats can be read into an R environment. A full list of formats supported may be found [here](https://www.datacamp.com/tutorial/r-data-import-tutorial).

## Python
```{python}

```

Let us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:

- We are using the method read_csv from the pandas library, which we have imported with the alias pd.

- In this form, all that is required is to pass the path to the file we want to read, which in this case is a web address.

- The argument index_col is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.

- We are using read_csv because the file we want to read is in the csv format. However, pandas allows for many more formats to be read and write. A full list of formats supported may be found here.

To ensure we can access the data we have read, we store it in an object that we call db. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read_csv.

:::

To ensure we can access the data we have read, we store it in an object that we call census2021. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of `read.csv`.

**Important**
You need to store the data file on your computer, and read it locally. To do that, you can follow these steps:
1. Download the file by right-clicking on this link and saving the file (add link)
2. Place the file on the same folder as the notebook where you intend to read it

## Data, sliced and diced

Now we are ready to start playing and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the LSOAs in Liverpool, how many people live in each, by the region of the world where they were born. We call these tables `DataFrame` objects, and they have a lot of functionality built-in to explore and manipulate the data they contain. Let’s explore a few of those cool tricks!

**Structure**

The first aspect worth spending a bit of time is the structure of a DataFrame. We can print it by simply typing its name:

::: {.panel-tabset group="language"}
## R
```{r}
census2021
```
## Python

:::

Note the printing is cut to keep a nice and compact view, but enough to see its structure. Since they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what we will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. In the example above, we can see how the column index is automatically picked up from the .csv file’s column names. For rows, we have specified when reading the file we wanted the column GeographyCode, so that is used. If we hadn’t specified any, pandas will automatically generate a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so we will come to it over and over. Importantly, even when we move to spatial data, our datasets will have a similar structure.

One final feature that is worth mentioning about these tables is that they can hold columns with different types of data. In our example, this is not used as we have counts (or int, for integer, types) for each column. But it is useful to keep in mind we can combine this with columns that hold other type of data such as categories, text (str, for string), dates or, as we will see later in the course, geographic features.

Inspecting what it looks like. We can check the top (bottom) X lines of the table by passing X to the method head (tail). For example, for the top/bottom five lines:
Or getting an overview of the table:

::: {.panel-tabset group="language"}
## R
```{r}
head(census2021) # read first 5 rows
tail(census2021)
```
## Python

:::


## Summarise

Or of the values of the table:
::: {.panel-tabset group="language"}
## R
```{r}

summary(census2021)
```
## Python

:::

Note how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).

In this case, the summary might be better presented if the table is “transposed”:

::: {.panel-tabset group="language"}
## R
```{r}
t(summary(census2021))
```
## Python

:::

Create new columns
Delete columns

## Queries

Index-based queries
Condition-based queries
Combining queries

## Sorting


## Visual Exploration
