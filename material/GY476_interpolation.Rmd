---
title: "GY476_Smoothing_Interpolation"
author: "Elisabetta Pietrostefani & Louise Bernard" 
date: "updated 21/10/2022"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
---

# **Introduction** 

In this tutorial, we are going to learn how to interpolate across space and create spatial weights. 

Spatial interpolation is useful to visualize patterns and fill missing data. 
It is commonly used to create maps of housing prices, pollution, weather and climate data. 

Spatial weights can be used to see how one's characteristics or outcomes is correlated 
with their neighbours: e.g. education, criminality, ...

## A bit about code chunks

A Rmarkdown document has three types of content:

* An (optional) YAML header surrounded by ---s
* R code chunks surrounded by ```s
* text mixed with simple text formatting # heading, _italics_ or **bold**

When you open a Rmarkdown file in the RStudio, it becomes a notebook interface for R.
You can run each code chunk by clicking the green icon "play". RStudio executes
the code and display the results inline with your file. T

You can add options to your chunks using different arguments in {} of a chunk header. 

https://rmarkdown.rstudio.com/lesson-3.html

To generate a report, you use the “Knit” button in the RStudio to render the file. 
The default is an HTML report. Get more info about the different report styles in the Rmarkdown documentation: 

https://rmarkdown.rstudio.com/lesson-2.html


## Install all relevant packages

* gstat: Spatial and Spatio-Temporal Geostatistical Modelling, Prediction and Simulation
* spdep: Spatial Dependence: Weighting Schemes, Statistics. 
* spatstat: Spatial Point Pattern Analysis 

```{r setup, include=FALSE, echo=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("gstat")) install.packages("gstat") #https://cran.r-project.org/web/packages/gstat/index.html
if(!require("raster")) install.packages("raster")
if(!require("sf")) install.packages("sf")
if(!require("dplyr")) install.packages("dplyr")
#if(!require("spdep")) install.packages("spdep")
if(!require("tmap")) install.packages("tmap")
if(!require("spatstat")) install.packages("spatstat")
if(!require("mapview")) install.packages("mapview")


```

## Relative and absolute path

``` {r setdatafolder}

getwd() # check working directory
datafolder <- file.path("../GY476_data_2022_23") # define the location of the data folder

```

# **Interpolation**
Spatial interpolation is the activity of estimating values spatially continuous 
variables for spatial locations where they have not been observed, based on observations. 
The statistical methodology for spatial interpolation, called geostatistics, 
is concerned with the modelling, prediction and simulation of spatially continuous phenomena.

The typical problem is a missing value problem: we observe a property of a phenomenon $Z(s)$
at a limited number of sample locations, and are interested in the property value at all 
locations covering an area of interest, so we have to predict it for unobserved locations. 
This is also called kriging, or Gaussian Process prediction.

In case $Z(s)$ contains a white noise component $\epsilon$, possibly reflecting measurement error,
an alternative but similar goal is to predict, which may be called spatial filtering or smoothing.

Data analysis generally involves extracting a ‘signal’ which you are interested in, from the ‘noise’.
When trying to see spatial patterns in a variable x, distributed over space, think of some part following a general smooth trend, and another part as more locally random:
data = smooth + rough
xi = (large scale variation)+(small scale)

We will use package gstat (E. Pebesma and Graeler 2021; Pebesma 2004), 
which offers a fairly wide palette of models and options for non-Bayesian geostatistical analysis.

## Interpolation

The first task is going to create a map of housing price in London. 

First, we are going to load the London districts: 

```{r}
districts <- read_sf(file.path(datafolder,"London/Polygons/districts.shp"))
```

** Task 1: ** Check CRS: is it a projected system?

```{r}
st_crs(districts)
```

Then, we are going to use the housesales again: 

```{r}
housesales <- read.csv(file.path(datafolder, "London/Tables/housesales.csv"))
summary(housesales)

```

and create spatial points as in the first R tutorial: 

``` {r createspatialpts}
housesales_sf <- st_as_sf(housesales, coords = c(17,18)) # create pts from coordinates
housesales_sf <-  st_set_crs(housesales_sf, 27700) # set crs
plot(housesales_sf$geometry)
```

## Inverse Distance Weighted (IDW)

IDW is an interpolation technique where we assume that an outcome varies smoothly across space
- the closer points are - the more likely they are to have the same outcome. 
To predict values across space, IDW uses neighbours values.
There are two main variables: the number of neighbours to consider and the speed of the spatial decay. 
For example, if we were to model the likelihood of a household to shop at the neighbouring local groceries store,
we would need to set a decay such as the probability would be close to 0 as we reach 15 minutes walking distance from home. 

In this example, we are going to predict housesalues in London. We use a simple kriging method: $$Price_i = \sum^N_{j=1} w_j * Price_j + \epsilon_i$$ with $w_j = (\frac{1}{d_{ij}})^2$ for all $i$ and $j \neq i$, $d$ the distance between $i$ and $j$. 

```{r}
library(gstat)
# Create geostatistical prediction using a ordinary Kriging
gs <- gstat(formula = price~1, # formula that defines the dependent variable as a linear model of independent variables
            data = housesales_sf, # housesales data 
            nmax = 50, # the number of nearest observations that should be used
            set=list(idp = 2)) # set inverse distance power to 2

# Create a blank raster using district extent and crs
r <- raster(districts, resolution=500) # output cell size 500

# Create the interpolated raster using the prediction from the model and the blank raster
idw <- interpolate(r, gs)

# Mask values outside London
idwr <- mask(idw, districts)

# Plot result
plot(idwr, col = rev(heat.colors(100)))
plot(districts$geometry, fill = NA, add=T)

```

**Task 2:** Change the IDW parameters (# neighbours, power) and raster (cell size) to see how it changes the map
This can be useful: https://gisgeography.com/inverse-distance-weighting-idw-interpolation/ 

## Saving a plot

You can save a plot in R as jpp, pdf or svg by opening a graphical device, plotting your objects and then closing the device. 

See Section 4.2 of https://intro2r.com/export-plots.html

```{r saving}
# Create a folder to save our maps
dir.create("maps")

# Create a pdf and save the output map
pdf("maps/my_plot.pdf") #Opening the graphical device
plot(idwr, col = rev(heat.colors(100))) #Creating a plot
plot(districts$geometry, fill = NA, add=T) 
dev.off() #Closing the graphical device
```

## Airbnb data

**Task 3:** Repeat **Interpolation** exercise above with Buenos Aires data

Hints: 

* Load the neighbourhood shapefile and check the CRS
* Load the clean listings (without outliers), find the coordinates, set the default CRS and re-project it to be able to calculate the distance
* Adjust the number of neighbours and power to get a nice smooth predicted for BA

# **Point patterns**

Point pattern analysis is concerned with describing patterns of points over space,
and making inference about the process that could have generated an observed pattern.
The main focus here lies on the information carried in the locations of the points, such as:
and typically these locations are not controlled by sampling but a result of a process we’re interested in:

* animal sightings
* accidents
* disease cases, 
* tree locations.

This is opposed to geostatistical processes, where we have values of some phenomenon
everywhere but observations limited to a set of locations that we can control, at least in principle.
Hence, in geostatistical problems the prime interest is not in the observation locations but in estimating the value of the observed phenomenon at unobserved locations.

Point pattern analysis typically assumes that for an observed area, all points are available, 
meaning that locations without a point are not unobserved as in a geostatistical process, but are observed and contain no point.
In terms of random processes, in point processes locations are random variables, 
where in geostatistical processes the measured variable is a random field with locations fixed.

Point patterns have an observation window. Consider the points generated randomly by
```{r window}
n = 30
xy = data.frame(x = runif(n), y = runif(n)) %>% st_as_sf(coords = c("x", "y"))
```

then these points are (by construction) uniformly distributed, or completely spatially random, over the domain  
[0,1]×[0,1]. For a larger domain, they are not uniform, for the two square windows w1 and w2 created by
```{r view}
w1 = st_bbox(c(xmin = 0, ymin = 0, xmax = 1, ymax = 1)) %>%
        st_as_sfc() 
w2 = st_sfc(st_point(c(1, 0.5))) %>% st_buffer(1.2)
```

This is shown here
```{r view2}
par(mfrow = c(1, 2), mar = c(2.1, 2.1, 0.1, 0.5), xaxs = "i", yaxs = "i")
plot(w1, axes = TRUE, col = 'grey')
plot(xy, add = TRUE)
plot(w2, axes = TRUE, col = 'grey')
plot(xy, add = TRUE, cex = .5)
```

Point patterns in spatstat are objects of class ppp that contain points 
and an observation window (an object of class owin). We can create a ppp from points by
where we see that the bounding box of the points is used as observation window when no window is specified. 
If we add a polygonal geometry as the first feature of the dataset, then this is used as observation window:

```{r spatstat}
as.ppp(xy)
(pp1 = c(w1, st_geometry(xy)) %>% as.ppp())
c1 = st_buffer(st_centroid(w2), 1.2)
(pp2 = c(c1, st_geometry(xy)) %>% as.ppp())
```

To test for homogeneity, one could carry out a quadrat count, using an appropriate 
quadrat layout (a 3 x 3 layout)
```{r homogeneity}
par(mfrow = c(1, 2), mar = rep(0, 4))
q1 = quadratcount(pp1, nx=3, ny=3)
q2 = quadratcount(pp2, nx=3, ny=3)
plot(q1, main = "")
plot(xy, add = TRUE)
plot(q2, main = "")
plot(xy, add = TRUE)
```

Kernel densities can be computed using density, where kernel shape and bandwidth can be controlled. 
Here, cross validation is used by function bw.diggle to specify the bandwidth parameter sigma; plots are shown in figure 11.3.
```{r k-density}
density1 <- density(pp1, sigma = bw.diggle)
density2 <- density(pp2, sigma = bw.diggle)
par(mfrow = c(1, 2), mar = c(0,0,1.1,2))
plot(density1)
plot(pp1, add=TRUE)
plot(density2)
plot(pp1, add=TRUE)
```

**Task 6:** Write a short script to conduct point pattern analysis with the 
Buenos Aires airbnb data. Try out different bandwidths to create more
rough or smooth versions.

# References and useful links

https://rpubs.com/erikaaldisa/spatialweights

https://rspatial.org/raster/analysis/4-interpolation.html

https://rstudio-pubs-static.s3.amazonaws.com/556418_da9c54a4e65942d79cdb1a7c3fe252c6.html

Write equation expressions in R: https://rpruim.github.io/s341/S19/from-class/MathinRmd.html










