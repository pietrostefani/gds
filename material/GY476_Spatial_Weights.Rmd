---
title: "Spatial Weights"
author: "Elisabetta Pietrostefani"
date: "2022-11-3"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
---

```{r packages, include=FALSE, echo=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("sf")) install.packages("sf")
if(!require("dplyr")) install.packages("dplyr")
if(!require("spdep")) install.packages("spdep")
if(!require("tmap")) install.packages("tmap")
```

# Relative and absolute path

``` {r setdatafolder, echo=FALSE}
getwd() # check working directory
datafolder <- file.path("../GY476_data_2022_23") # define the location of the data folder

```

# **Spatial Weights**

**How do you express geographical relations between objects (areas, points) in 
a way that can be used in statistical analysis?**

For a statistical method to be explicitly spatial, it needs to contain some 
representation of the geography, or spatial context. One of the most common ways 
is through **Spatial Weights Matrices**

* (Geo)Visualization: translating numbers into a (visual) language (colors) that the human brain _“speaks better”_ 
* Spatial Weights Matrices: translating geography into a (numerical) language that a computer _“speaks better”_

Spatial Weights Matrices are building block for spatial analysis and statistics.

They are used to assign a weighted average or sum of neighbouring data values to 
an observation, or other point in space. 

* Takes all richness of geographical relationships and translates this in a way 
that is well understood by statistics and computers
* Relates to concepts of spatial ‘smoothing’ and interpolating data
* Understand relevance to visualisation and exploratory analysis
* They can be used to see how one's characteristics or outcomes is correlated with 
their neighbours: e.g. education, criminality, ...

Core element in several spatial analysis techniques

* Spatial autocorrelation
* Spatial clustering/geo-demographics
* Spatial regression

Spatial Weights represented by 
$$W$$
N x N positive matrix that contains spatial relations that are translated into values

* If you are not a neighbour, $value = 0$
* If you are a neighbour, $value<0$

We will be looking at practical ways to create aggregate/average variables across 
neighbouring points and districts. This is we are looking at ways to implement this
$$\hat{m_{i}}=\sum_jw_{ij}x_{j}=w_{i}x$$
Or in matrix form, And/or we will generate the matrix W
This is spatially smoothed version of x, a local mean of x, or a ‘spatial lag’
$$\hat{m}=Wx$$

## What is a neighbour?

A neighbour is "somebody who" is

* Next door --> **Contiguity**-based Ws
* Close --> **Distance**-based Ws
* In the same "place" as us --> Block weights

**Contiguity-based** Weights

* Sharing boundaries to any extend: Rook, Queen, (can be based on a point, an edge, ...)

You need to change the pathway for the png to appear
![an image caption Source: From Sedaghat, L., Hersey, J., & McGuire, M.P. (2013). Detecting spatio-temporal outliers in crowdsourced bathymetry data. GEOCROWD '13.](../Week 5-7 - Geospatial Tools/PNGs/Figure1.png)

**Distance based** Weights

Weight is inversely proportional to distance between observations: Inverse distance (1/distance or threshold), KNN (fixed number of neighbors)

In the IDW interpolation method, the sample points are weighted during interpolation such that the influence of one point relative to another declines with distance from the unknown point you want to create

You need to change the pathway for the png to appear
![an image caption Source: https://rafatieppo.github.io/post/2018_07_27_idw2pyr/](../Week 5-7 - Geospatial Tools/PNGs/Figure2.png)
**Task 1:** with your class-mates (groups) take the Buenos Aires Neighbourhood shapefile and 
discuss how even though geography is the same, the way you encapsulate a geography
into a spatial weight matrix varies can differ. Draw a quick sketch (by hand is fine!).

This is crucial because when you will use this in more advanced stats, 
everything the stats will know about the geography is whatever is incapsulated in the matrix.

# **Spatial Lag**

* Product of a spatial weights matrix W and a given variable
* Measure that captures the behaviour of a variable not at it's location but in 
the neighbourhood of a given observation i 
* If W is **standardized**, the spatial lag is the average value of the variable in the neighbourhood

## Contiguity Neighbours

**Join data**
Read qualifications data from CSV & join to district file, and then create the 
share of people with Level 4 education by simply adding a new column to the attribute table. 

```{r read_qual}
#load
districts <- read_sf(file.path(datafolder,"London/Polygons/districts.shp"))

# load qualification CSV
qualifications2001_df <- read.csv(file.path(datafolder, "London/Tables/qualifications2001_2.csv"))

# Join by district code
districts <- left_join(districts, 
                       qualifications2001_df, 
                       by=c("DIST_CODE"="Zone_Code"))

#merge district shapefile and the qualifications from the csv file
head(districts)

# create the share of people with level 4 education
districts$Level4p <- districts$Level4/districts$Population1674
head(districts)

```

**Construct weights**

We use the `spdep` package to construct spatial contiguity weights. For each polygon, 
the function `poly2nb` lists all adjacent polygons. `nb2listw` then constructs the weights 
according to the chosen. In our case, we want all neighbours to sum up to 1, 
so the weights are row normalized. For example, if a polygon has two neighbours, 
they will be given a weight of 0.5. If a polygon has three neighbours, each weight is going to be 0.33. ect.

We then can create the share of people with level 4 qualifications in the neighbouring countries.
For that, we apply the weights to the share of level 4 variables. 

We plot the results using `tmap`. 

**Task 2:** Change the options of the `nb2listw` to see how if affects the weights

hint: you can type `help(nb2listw)` to access the command description

```{r construct_neigh}
# list all adjacent districts for each polygon
nb_q <- poly2nb(districts, queen = FALSE) # Construct neighbours list from polygon list

# create weights
W_districts_mat <- nb2listw(nb_q, style="W", zero.policy=TRUE) # Spatial weights for neighbours lists
W_districts_mat
head(W_districts_mat$weight)

# create spatial lag variable
districts$w_Level4p <- lag.listw(W_districts_mat, districts$Level4p, zero.policy = T) # create spatially lag variables 

```

**Compare values with maps**
Finally, we create a map comparing the two variables:
``` {r tmap}
qtm(districts, 
    fill = c("Level4p", "w_Level4p"), 
    fill.breaks=seq(0, 0.8, 0.2), 
    fill.palette = "Blues", 
    ncol = 2) +
  tm_legend(legend.position = c("right", "bottom")) # change the legend position to bottom right
```

## Neighbours by distance

We can also create spatial weights using the distance between points. Here we consider all properties within 2,000m. 

``` {r neigh by distance}

housesales <- read.csv(file.path(datafolder, "London/Tables/housesales.csv"))
housesales_sf <- st_as_sf(housesales, coords = c(17,18)) # create pts from coordinates
housesales_sf <-  st_set_crs(housesales_sf, 27700) # set crs

# Neighbourhood by distance
W_housesales_dist1 <- dnearneigh(housesales_sf, d1=0, d2=2000, row.names=housesales_sf$propid) 
W_housesales_dist1_mat <- nb2listw(W_housesales_dist1, zero.policy = TRUE)
summary(W_housesales_dist1_mat, zero.policy=TRUE)

# plot
plot(W_housesales_dist1_mat, coords = housesales[, 17:18], pch=19, cex=0.1, col="blue")
```
For more on Weights in R see Chapter 14 https://r-spatial.org/book/14-Areal.html#distance-based-neighbours

