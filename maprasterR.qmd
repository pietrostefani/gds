# Lab in R {#sec-map-raster-R .unnumbered}

In this session, we will further explore the world of geographic data visualization by building upon our understanding of both raster data and choropleths. Raster data, consisting of gridded cells, allows us to represent continuous geographic phenomena such as temperature, elevation, or satellite imagery. Choropleths, on the other hand, are an effective way to visualize spatial patterns through the use of color-coded regions, making them invaluable for displaying discrete data like population density or election results. By combining these techniques, we will gain a comprehensive toolkit for conveying complex geographical information in a visually compelling manner.

## Installing Packages

```{r message=FALSE, warning=FALSE}
# Provides various utility functions for R programming.
library(R.utils)
# For data manipulation and transformation.
library(dplyr)
# Spatial data
library(sf)
# Popular data visualization package in R.  
library(ggplot2)
# For creating thematic maps 
library(tmap)
# Color palettes suitable for data visualization, especially for those with color vision deficiencies.
library(viridis)
# A collection of color palettes for data visualization.
library(RColorBrewer)
# For working with raster data, such as gridded spatial data like satellite imagery or elevation data.
library(raster)
# An alternative to the 'raster' package and is used for working with large raster datasets efficiently.
library(terra)
# Tools for extracting data from raster layers at exact locations, often used in spatial analysis.
library(exactextractr)
# Common methods of the tidyverse packages for objects created with the {terra} package: SpatRaster and SpatVector
library(tidyterra)
```

## Terrain data

### **Import raster data**

Raster **terrain** data consists of gridded elevation values that represent the topography of a geographic area. You can download this from the [relevant github folder](https://github.com/pietrostefani/gds/tree/main/data/Lebanon). A good place to download elevation data is [Earth Explorer](https://earthexplorer.usgs.gov/). This [video](https://www.youtube.com/watch?v=NQg0g9ObhXE) takes you through the download process if you want to try this out yourself.

We first import a raster file for elevation.

```{r lebanon-import_a_raster}
elevation <- rast("data/Lebanon/LBN_elevation_w_bathymetry.tif")
```

Plot it.

```{r}
plot(elevation) 
```

Have a look at the CRS.

```{r}
crs(elevation)
```

### **Import the Lebanon shapefile**

Import the Lebanon shapefile, plot it, and verify its Coordinate Reference System (CRS). Is it the same as the raster's CRS?

```{r lebanon-import_shp}
Lebanon_adm1 <- read_sf("data/Lebanon/LBN_adm1.shp")
plot(Lebanon_adm1$geometry)
crs(Lebanon_adm1)
```

### **Reproject the Raster**

As we are using both the `raster` and `terra` packages to handle the raster data it is useful to write `terra::` or `raster::` in front of the function we are using.

We use the terra `project()` function, we need to define two things:

1.  The object we want to reproject and
2.  The CRS that we want to reproject it to.

```{r lebanon-reproject}
elevation <- terra::project(elevation, crs(Lebanon_adm1)) # reporjectig the elevation data to the crs of the Lebanon shapefile
crs(elevation)
```

### **Cropping and Masking**

Cropping and masking are both spatial operations used in raster data analysis.

**Cropping**:

-   Purpose: Cropping a raster involves changing the extent of the raster dataset by specifying a new bounding box or geographic area of interest. The result is a new raster that covers only the specified region.

-   Typical Use: Cropping is commonly used when you want to reduce the size of a raster dataset to focus on a smaller geographic area of interest while retaining all the original data values within that area.

**Masking**:

-   Purpose: Applying a binary mask to the dataset. The mask is typically a separate raster or polygon layer where certain areas are designated as "masked" (1) or "unmasked" (0).

-   Typical Use: Masking is used when you want to extract or isolate specific areas or features within a raster dataset. For example, you might use a mask to extract land cover information within the boundaries of a protected national park.

In many cases, these cropping and masking are executed one after the other because it is computationally easier to crop when dealing with large datasets, and then masking.

```{r lebanon-crop}
elevation_lebanon <- crop(elevation, extent(Lebanon_adm1))
elevation_lebanon_mask <- mask(elevation_lebanon, Lebanon_adm1)
```

### **Plot elevation**

```{r lebanon-plot}
plot(elevation_lebanon_mask)
plot(Lebanon_adm1$geometry, col= NA, add=T)
```

Let's improve this a bit. Remember that there is a lot we can do with [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3).

::: {.panel-tabset group="data"}
# `plot`

```{r lebanon-changecolours}
#| warning = FALSE

pal = rev(brewer.pal(6,"Oranges"))
plot(elevation_lebanon_mask, breaks=c(-100,0,700,1200,1800,3300), col=pal)
plot(Lebanon_adm1$geometry, col= NA, add=T)
```

# `tmap`

```{r}
#| warning = FALSE

# Define the palette
pal <- rev(brewer.pal(6, "Oranges"))

# Create the base map
tm_shape(elevation_lebanon_mask) +
  tm_raster(breaks = c(-100, 0, 700, 1200, 1800, 3300),
            palette = pal) +  # Plot the raster with breaks and palette
  tm_shape(Lebanon_adm1) +
  tm_borders(lwd = 2) +  # Add borders to the administrative boundaries
tm_layout(frame = FALSE, legend.outside = TRUE, legend.outside.position = "right")  # Remove frame
```
:::

Questions to ask yourself about how you can improve these maps, going back to [geo-visualisation and choropleths](https://pietrostefani.github.io/gds/mapvector.html).

-   What are the logical breaks for elevation data?

-   Should the colours be changed to standard elevation pallettes?

-   Have a look at some of the [`tmap` documentation](https://r-tmap.github.io/tmap/reference/tm_layout.html) to improve your map further .

### **Slope**

We are now going to calculate slopes of the `terra` package.

```{r rasters}
# import elevation data
elevation_proj <- rast("data/Lebanon/DEM_Leb_projected.tif")

slope <- terra::terrain(elevation_proj, v="slope", neighbors=8, unit="degrees")
plot(slope)
```

Alternatively you can try out the function `raster::terrain`, details [here](https://www.rdocumentation.org/packages/raster/versions/3.6-23/topics/terrain).

Now let's stylise the contours. The package `tidyterra` provides a great solution by combingn the methods of the `tidyverse` packages for objects SpatRaster objects. See [here](https://dieghernan.github.io/tidyterra/reference/geom_spat_contour.html) for more information.

```{r}
ggplot() +
  geom_spatraster_contour(data = elevation_proj)

# Create a new ggplot2 plot object.
ggplot() +
  # Add a spatial raster contour layer to the plot.
  geom_spatraster_contour(
    data = elevation_proj,               # Use the 'elevation' data for this layer.
    aes(color = after_stat(level)), # Color the contour lines based on 'level'.
    binwidth = 100,                # Define the binwidth for contouring. Every 100 metres
    linewidth = 0.4               # Set the line width for contour lines.
  ) +
  # Customize the color scale for contour lines.
  scale_color_gradientn(
    colours = hcl.colors(20, "Terrain2"), # Specify a color palette.
    guide = guide_coloursteps()           # Use a color step guide for legend.
  ) +
  # Apply a minimalistic theme to the plot.
  theme_minimal()
```

Have a play around with [hcl.color](https://blog.r-project.org/2019/04/01/hcl-based-color-palettes-in-grdevices/).

### **Flood risk area**

We can employ reclassification techniques to delineate flood risk areas. Specifically, we identify and classify areas where the elevation is under 10 meters above sea level as high-risk zones. This critical step in flood risk assessment helps communities and decision-makers pinpoint vulnerable areas, enabling them to implement effective mitigation strategies and disaster preparedness plans to safeguard against potential flooding events.

```{r floodrisk}
flood_risk <- app(elevation_proj, fun= function(x) ifelse(x<10, 1, 0))
# This is an anonymous function defined within the 'app' function.
# It checks each pixel (or cell) value in the 'elevation' raster or matrix.
# If the pixel value is less than 10 meters, it assigns 1 to 'flood_risk'; otherwise, it assigns 0. This effectively creates a binary flood risk map, where 1 represents areas with a flood risk (elevation < 10m), and 0 represents areas with no flood risk (elevation >= 10m).
plot(flood_risk)
```

### **Spatial join with vector data**

You might want to extract values from a raster data set, and then map them eith in a vector `sf` framework or extract them to analyse them statistically. If it therefore very useful to know hoe to extract:

```{r extract}
# Load some geo-localised survey data 
households <- read_sf("data/Lebanon/random_survey_LBN.shp")

# Using the 'raster::extract' function, it calculates the elevation values at the coordinates of the points. 'elevation' is a raster layer, and 'households' is point data representing household locations.
housesales_elevation <- raster::extract(elevation,
                                households)

# Attach elevation at each point to the original housesales dataframe
households <- cbind(households, housesales_elevation)

# Check out the data
head(households)
```

::: callout-important
Make sure all your data is in the same CRS, otherwise the `raster::extract` function will not work properly.

You should not be seeing NAs, if you do you should use the terra `project()` function.
:::

## Night Lights

This section is a bit more advanced, there are hints along the way to make it simpler.

### **Some useful functions**

To start this exercise, let's see some useful functions to manipulate data in R.

```{r output=FALSE}

# Before we get to clean the data, let's go over a few useful function
# set the working directory to where you saved the GY476 data

# list files 
list.files()

# list files in a specific folder
list.files(file.path("data/Lebanon/Polygons"))

# list files corresponding to a specific pattern ("shp" in the filename)
list.files(file.path("data/Lebanon/Polygons"), pattern = "shp")

# list files corresponding to a specific pattern ("shp" at the end of the filename)
shps <- list.files("data/Lebanon/Polygons", pattern = "*.shp")

# we can also select strings following a pattern inside a list or vector using grepl
shps <- shps[grepl("Lebanon", shps)] 

# let's extract the first element of the list "shps"
file1 <- shps[1]
file1

# how many characters in the filename
nchar(file1)

# let's remove the last 4 charcters (the file extension)
file1_short <- substr(file1, 1, nchar(file1)-4)

# let's add something to the name (concatenate strings) - for example, a new extension ".tif"
paste(file1_short, ".tif", sep="")

# finally let's create a function MathOperations that first calculate the square and then add 3
MathOperations <- function(x) {
  sq <- x^2
  z <- sq+3
  return(z)
}

# try the function on 4, 5, 6
MathOperations(4)
MathOperations(5)
MathOperations(6)

# repeat this operation for the vector 4 to 6 (similar to a loop in STATA)
lapply(4:6, function(x) MathOperations(x))


```

### **Download data**

We need to download some raster data. NOAA has made nighttime lights data available for 1992 to 2013. It is called the Version 4 DMSP-OLS Nighttime Lights Time Series. The files are cloud-free composites made using all the available archived DMSP-OLS smooth resolution data for calendar years. In cases where two satellites were collecting data - two composites were produced. The products are 30 arc-second grids, spanning -180 to 180 degrees longitude and -65 to 75 degrees latitude. We can download the [Average, Visible, Stable Lights, & Cloud Free Coverages for 1992 and 2013](https://www.ngdc.noaa.gov/eog/data/web_data/v4composites/) and put them in the `data/Kenya_Tanzania` folder.

::: callout-important
You can also download the data [here](https://theuniversityofliverpool-my.sharepoint.com/:u:/g/personal/pietrost_liverpool_ac_uk/EX85foAh7ZBBtSZ64pxbFmsBStf8UX2uaLXE5MBYJt6PeQ?e=LMYu7d). You will need to be logged into your UoL account.

The downloaded files are going to be in a **"TAR" format**. A TAR file is an archive created by tar, a Unix-based utility used to package files together for backup or distribution purposes. It contains multiple files stored in an uncompressed format along with metadata about the archive. Tars files are also used to reduce files' size. TAR archives compressed with GNU Zip compression may become GZ, .TAR.GZ, or .TGZ files. We need to decompress them before using them.
:::

Before you move forward **download the data for 1992 and 2013.** It is also good practice to create a scratch folder where you do all your unzipping.

In our example, we will only download two years, but generally, you will have to repeat the same cleaning operations many times. Therefore, to speed up the process, we are going to create a new function. The function is going to:

1.  Decompress the files using the `untar` command,

2.  List the decompressed files using `list.files` command (notice there are compressed files inside the TAR archive)

3.  Identify the TIF archive files using `grepl`

4.  Decompress using the `gunzip` command.

We are then going to run the function on all the TAR files.

::: callout-note
You can do these steps manually if you can't get the below chunk to work.
:::

```{r downloadunzip, message=FALSE}
datafolder <- file.path("./data") # define the location of the data folder

# list downloaded files: they are compressed files using the "tar" format
tars <- list.files(file.path("data/Kenya_Tanzania/scratch"), pattern = "*.tar")

# unzip
UnzipSelect <- function(i) {
  untar(file.path(datafolder,"Kenya_Tanzania/scratch",i), exdir = file.path(datafolder, "Kenya_Tanzania/scratch")) # unzip
  all.files <- list.files(file.path(datafolder,"Kenya_Tanzania/scratch"), pattern = paste0(substr(i, 6, 12), "*")) # list extracted files
  gz <- all.files[grepl("web.stable_lights.avg_vis.tif.gz", all.files)] # select the TIF files
  R.utils::gunzip(filename	= file.path(datafolder,"Kenya_Tanzania/scratch", gz),
                  destname = file.path(datafolder,"Kenya_Tanzania", substr(gz, 1, nchar(gz) -3)),
                  overwrite = TRUE) # unzip again
}

# loop over the TAR files
# note that the function returns the last element created - in this example, the TIF files
nl <- lapply(tars, UnzipSelect)

# you can delete the scratch folder with the data we don't need
# unlink(file.path(datafolder,"Kenya_Tanzania/scratch"), recursive = TRUE)
```

We can load and plot the nighttime lights data. When working with many rasters of the same origin, it is usually faster to stack them together for faster processing.

```{r nightlightststats}
# load NL
# we apply the function raster to each tif file to load the raster in the workspace
nl_rasters <- lapply(nl, raster)
# we stack the raster (only possible for rasters of the same extent and definition)
nl_rasters_stack <- stack(nl_rasters)
# plot the result
plot(nl_rasters_stack,  
     main=c("Nightlights 1992", "Nightlights 2013"),
     axes=FALSE)
# change the names 
names(nl_rasters_stack) <- c("NL1992", "NL2013")
```

Why can't you see much? Discuss with the person next to you.

### **Country shapefiles**

The second step is to download the shapefiles for Kenya and Tanzania. GADM has made available national and subnational shapefiles for the world. The zips you download, such as *gadm36_KEN_shp.zip* from GADM should be placed in the **Kenya_Tanzania** folder. This is the link [gadm](https://gadm.org/formats.html).

```{r loadshp, output=FALSE}
# list country shp that we downloaded from the GADM website
files <- list.files(file.path(datafolder,"Kenya_Tanzania"), pattern = "_shp.zip*", recursive = TRUE, full.names = TRUE)
files

# create a scratch folder
# dir.create(file.path(datafolder,"Kenya_Tanzania/scratch"))

# unzip
lapply(files, function(x) unzip(x, exdir = file.path(datafolder,"Kenya_Tanzania/scratch")))

# GADM has shapefiles for different regional levels (e.g. country, region, district, ward) 
gadm_files <- list.files(file.path(datafolder,"Kenya_Tanzania"), pattern = "gadm*", recursive = TRUE, full.names = TRUE)
gadm_files

# let's select regional level 2
gadm_files_level2 <- gadm_files[grepl("2.shp", gadm_files)]
gadm_files_level2

# load the shapefiles
shps <- lapply(gadm_files_level2, read_sf)
shps

# delete the scratch folder with the data we don't need
# unlink(file.path(datafolder,"Kenya_Tanzania/scratch"), recursive = TRUE)
```

### **Zonal statistics**

We use the package `exactextractr` to calculate the sum and average nighttime for each region. The nighttime lights rasters are quite large, but as we do not need to do any operations on them (e.g. calculations using the overlay function, cropping or masking to the shapefiles extent), the process should be relatively fast.

Again, we use the `lapply` function to process the two countries successively.

```{r zonalstats}
# summarize
ex <- lapply(shps, function(x) exact_extract(nl_rasters_stack, x, c("sum", "mean", "count"), progress = FALSE))
# lapply returns a list of two dataframes, we can use "do.call" to return each element of the list and iterate the function rbind
# the results is a dataframe with the merged rows of the dataframes
ex <- do.call("rbind", ex)

# show first files
head(ex)

# summary
summary(ex)
```

### **Merge shapefiles**

Even though it is not necessary here, we can merge the shapefile to visualize all the regions at once.

Usually, it is easier to process data in small chunks using a function like "sapply, lapply, mapply" or a loop before merging. For example, when doing zonal statistics, it is faster and easier to process one country at a time and then combine the resulting tables. If you have access to a computer with multiple cores, it is also possible to do "parallel processing" to process each chunk at the same time in parallel.

```{r merge shapefile}

# merge together
# we select each sf object and merge the rows
# do.call() in R to apply a given function to a list as a whole
# The rbind()  function can be used to bind or combine several vectors, matrices, or data frames by rows
tza_ken <- do.call("rbind", shps)

# inspect
str(tza_ken)

# plot
plot(tza_ken$geometry)

```

### **Visualize**

Let's have a first look at our result.

```{r plotnl}
# merge back with shapefile attribute table
# this time instead of merging the rows, we append the columns using cbind
df <- cbind(tza_ken, ex)

# Create a map object for "mean.NL1992"
map1992 <- tm_shape(df) +
  tm_fill(col = "mean.NL1992", palette = "seq", title = "Mean 1992") +
  tm_layout(aes.palette = list(seq = "-YlGnBu"))

# Create a map object for "mean.NL2013"
map2013 <- tm_shape(df) +
  tm_fill(col = "mean.NL2013", palette = "seq", title = "Mean 2013") +
  tm_layout(aes.palette = list(seq = "-YlGnBu"))

# Plot both maps side by side
tmap_arrange(map1992, map2013)

```

Most of the Kenya and Tanzania have really low values. To make the maps tell a story, we can use fixed breaks and make the maps interactive using the `tmap` package:

```{r prettymap}
tmap_mode("view") # switch to other mode: ttm()

tm_shape(df) +
  tm_fill(c("mean.NL1992", "mean.NL2013"), title=c("Average nightlights"), style="fixed", palette = "seq", breaks=c(0, 0.05, 0.1, 2, 63)) +
  tm_layout(aes.palette = list(seq = "-YlGnBu"), legend.outside = TRUE, legend.outside.position = "right") +
  tm_facets(sync = TRUE, ncol = 2) +
  tm_borders()

```

Have a think about what the data is telling you. What's the story?

# Resources

-   Downloading [night lights] (https://github.com/walshc/nightlights/tree/master/R)

-   The package [`nightlightstats`](https://github.com/JakobMie/nightlightstats)
